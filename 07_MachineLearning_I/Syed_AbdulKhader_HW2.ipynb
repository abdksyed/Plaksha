{"cells":[{"cell_type":"raw","metadata":{},"source":["---\n","title: \"Machine Learning Homework II\"\n","description: \"Linear Regression and Support Vector Machines\"\n","author: Abdul Khader, Syed\n","format:\n","    html:\n","        code-fold: false\n","execute: \n","    enabled: false\n","---"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[],"source":["#| echo: false\n","\n","import numpy as np\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","import matplotlib.pyplot as plt\n","from IPython.display import display, Latex\n","import itertools"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Problem 1\n","\n","In real world applications, there are outliers in data. This can be dealt with using a soft margin, specified in a slightly different optimization problem as below (soft-margin SVM):\n","\n","$$\n","min \\frac{1}{2}w^Tw + C \\sum_{i}^{N} \\xi_i where \\xi_i \\ge 0\n","\\\\\n","s.t. y^{(i)} (w^Tx^{(i)} + b) \\ge 1 - \\xi_i\n","$$\n","\n","$ξi$ represents the slack for each data point $i$, which allows misclassification of datapoints in the event that the data is not linearly seperable. SVM without the addition of slack terms is known as hard-margin SVM."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![](./data/SVM.jpg){height=70%}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1. Intuitively, where do the data points lie relative to where the margin is when $ξi$ = 0 ? Are all training data points classified correctly?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We know that, $ξi$ is the distance between the support vector plane (margins) and the incorrectly classified data point. For example, for a data point which belongs to a positive class, $ξi$ is the distance between the datapoint and $\\pi^+$ margin. If the data point is above the $\\pi^+$ margin, than $ξi = 0$, since the point was correctly classified.\n","\n","Similarly for a data point belonging to negative class, $ξi$ is the distance between that point and $\\pi^-$ margin, if the data point is below the $\\pi^-$ margin, than $ξi = 0$, since the point was correctly classified.\n","\n","The **Data Points lie on or above(for positive class)/below(for negative class) the margin (support vectors) when $ξi = 0$** In the above diagram $x_k$ is such a point and $x_i$.\n","\n","##### Are all training data points classified correctly?\n","\n","**Yes**, all the training data points whose $ξi=0$ are correctly classified."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2.Intuitively, where does each data point lie relative to where the margin is when $0 < ξi ≤ 1$ ? Are all training data point classified correctly?\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As mentioned above, $ξi$ is the distance from the margin (support vector) to the data point, which is $1-Z_i$ for incorrectly classified points and 0 otherwise. When $0 < ξi \\le 1$, the point lies between the margin and hyperplane. Also given $ξi=1$, so the data point can lie on the hyperplane too, when the distance between the margin and data point is 1, which is distance between the margin and hyperplane.\n","\n","\n","##### Are all training data points classified correctly?\n","\n","And, **No**, All the training points are incorrectly classified while **training**. During testing we just check where does the point lie realtive to the hyperplane $f(x) = w^Tx + b$. If  $f(x) > 0$ we say positive class, and if $f(x) < 0$ we say negative class. \n","\n","But while training we want the data point of the class to be above the positive margin $\\pi^+$ ($w^Tx + b \\ge 1$) for positive class and below the negative margin $\\pi^-$($w^Tx + b \\le 1$) for negative class. The soft margin assumes this data points between the margins to be incorrectly classified and add them to the error term during the optimization."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 3. Intuitively, where does each data point lie relative to where the margin is when $ξi > 1$ ? Are all training data points classified correctly?\n","\n","When $ξi > 1$, the data points are in the opposite direction of the hyperplane and are more than 1 distance away from the margin. The point is clearly misclassified.\n","\n","For positive points, the point will lie below the hyperplane which is more than 1 distance below from $\\pi^+$ and for negative points, the point will lie above the hyperplane and more than 1 distance above the $\\pi^-$.\n","\n","##### Are all training data points classified correctly?\n","**No**, All the training points whose $ξi>1$ are incorrectly classified."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Problem 2\n","\n","Support Vector Machines can be used to perform non-linear classification with a kernel trick. Recall the hard-margin SVM from class:\n","\n","$$\n","min \\frac{1}{2}w^Tw\n","\\\\\n","s.t. y^{(i)} (w^Tx^{(i)} + b) \\ge 1\n","$$\n","\n","The dual of this primal problem can be specified as a procedure to learn the following linear classifier:\n","$$\n","f(x) = \\sum_{i}^{N} \\alpha_i y_i (x_i^Tx) + b\n","$$\n","Note that now we can replace $x_i^Tx$ with a kernel $k(x_i,x)$, and have a non-linear decision boundary."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In Figure, there are different SVMs with different shapes/patterns of decision boundaries. The training data is labeled as $y_i ∈ \\{−1, 1\\}$, represented as the shape of circles and squares respectively. Support vectors are drawn in solid circles/squares. Match the scenarios described below to one of the 6 plots (note that one of the plots does not match to anything). Each scenario should be matched to a unique plot. Explain in less than two sentences why it is the case for each scenario."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1. A soft-margin linear SVM with C = 0.02.\n","\n","Linear Soft margin, with a very low value of C indicated that the model can have higher values of $\\xi$, and model is fine with some misclassification but desires higher variance, which is depicted by **Figure 4**.\n","\n","\n","![](./data/4.png){width=30%}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2. A soft-margin linear SVM with C = 20.\n","\n","Linear Soft margin, with a high value of C indicated that we want lower values of $\\xi$, and we are not so hard with misclassification and desired higher bias and closer parallel linear margins, which is depicted by **Figure 3**.\n","\n","![](./data/3.png){height=30%}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 3. A hard-margin kernel SVM with $k(u, v) = u · v + (u · v)^2$\n","\n","Here the hard margin kernel SVM has a kernel which have the function which is hyperbollic, and **Figure 5** has a hyperbolla margins.\n","\n","![](./data/5.png){height=30%}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 4. A hard-margin kernel SVM with $k(u, v) = exp(−5||u − v||^2)$\n","\n","With the high value of $\\gamma$ in RBF kernel $exp(-\\gamma|||u − v||^2)$, the kernel value is small and needs more support vectors for classifiaction which is as depiced in **Figure 6**\n","\n","![](./data/6.png){height=30%}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 5. A hard-margin kernel SVM with $k(u, v) = exp(−\\frac{1}{5}||u − v||^2)$\n","\n","With the lower value of $\\gamma$ in RBF kernel $exp(-\\gamma|||u − v||^2)$, the kernel value is higher and we can classify data points using fewer support vectors, which is depiced in **Figure 1**\n","\n","![](./data/1.png){height=30%}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Problem 3\n","Suppose we have the following data on seven variables $x1, · · · , x7$ and the output $y$, given as fol- lows:"]},{"cell_type":"code","execution_count":224,"metadata":{},"outputs":[],"source":["x = np.array([[0.94,0.37, 0.76, 0.56, 0.77 ,0.51 ,0.80],\n","[-0.88 ,-0.33,-0.68,-0.51 ,-0.75 ,-0.47, -0.76],\n","[1.32 ,-0.31 ,0.78 ,0.23, 0.47 ,0.10 ,0.90],\n","[-0.21 ,0.79, 0.13, 0.46,0.27 ,0.54, 0.05],\n","[ 0.75 ,-0.70 ,0.28 ,-0.22, -0.07 ,-0.34 ,0.39],\n","[ -0.33, -1.30 ,-0.66, -0.98, -0.86 ,-1.06, -0.57],\n","[1.27 ,0.81 ,1.14, 0.96 ,1.08, 0.92, 1.15],\n","[ -0.60, 0.84, -0.13, 0.36, 0.35, 0.48, -0.24],\n","[ 0.15,-1.35 ,-0.35 ,-0.85 ,-0.47 ,-0.98 ,-0.22],\n","[-0.33, 0.86, 0.07, 0.46 ,0.37, 0.56, -0.03]])\n","\n","y = np.array([0.66, -0.60, 0.50, 0.29, 0.03, -0.82, 1.04, 0.12, -0.60, 0.26])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1. Find the linear regression to this data using the closed form expressions (can use calculators). Does the formula work? If not, explain why not."]},{"cell_type":"code","execution_count":225,"metadata":{},"outputs":[{"data":{"text/plain":["3.1679045781163164e-16"]},"execution_count":225,"metadata":{},"output_type":"execute_result"}],"source":["xTx = np.matmul(x.T, x)\n","np.linalg.det(xTx)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The determinant of $X^Tx$ is very small. If this is approximated than the determinant will go to zero, and the inverse won't exist. Since the inverse won't exist, the problem can not be solved in closed form.\n","\n","But if we go ahead with the small value of determinant and find the inverse, and continue to find the weights using the closed form where the weights $\\beta = (x^Tx)^{-1} x^T y$. We get the weights of the model."]},{"cell_type":"code","execution_count":226,"metadata":{},"outputs":[{"data":{"text/plain":["6.496738215544207e-05"]},"execution_count":226,"metadata":{},"output_type":"execute_result"}],"source":["xTx_inv = np.linalg.inv(xTx)\n","weights = np.matmul(np.matmul(xTx_inv, x.T), y)\n","y_hat = np.matmul(x, weights)\n","rss = np.sum((y-y_hat)**2)\n","rss"]},{"cell_type":"code","execution_count":227,"metadata":{},"outputs":[{"data":{"text/latex":["$f(x) = (-0.395)*x_1 +  (-0.865)*x_2 +   (0.501)*x_3 +   (0.623)*x_4 + (0.021)*x_5 +                 (0.982)*x_6 + (0.128)*x_7$"],"text/plain":["<IPython.core.display.Latex object>"]},"metadata":{},"output_type":"display_data"}],"source":["x1,x2,x3,x4,x5,x6,x7 = np.round(weights, 3)\n","display(Latex(f'$f(x) = ({x1})*x_1 +  ({x2})*x_2 +   ({x3})*x_3 +   ({x4})*x_4 + ({x5})*x_5 + \\\n","                ({x6})*x_6 + ({x7})*x_7$'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2. Fit a linear ridge regression model to this data using the closed form expressions "]},{"cell_type":"code","execution_count":244,"metadata":{},"outputs":[{"data":{"text/plain":["9.375815496416118e-05"]},"execution_count":244,"metadata":{},"output_type":"execute_result"}],"source":["LAMBDA = 10e-4\n","xTx_inv = np.linalg.inv(xTx + LAMBDA*np.eye(7))\n","weights = np.matmul(np.matmul(xTx_inv, x.T), y)\n","y_hat = np.matmul(x, weights)\n","rss = np.sum((y-y_hat)**2)\n","rss"]},{"cell_type":"code","execution_count":229,"metadata":{},"outputs":[{"data":{"text/latex":["$f(x) = (-0.374)*x_1 +  (-0.831)*x_2 +   (0.487)*x_3 +   (0.634)*x_4 + (0.021)*x_5 +                 (0.936)*x_6 + (0.124)*x_7$"],"text/plain":["<IPython.core.display.Latex object>"]},"metadata":{},"output_type":"display_data"}],"source":["x1,x2,x3,x4,x5,x6,x7 = np.round(weights, 3)\n","display(Latex(f'$f(x) = ({x1})*x_1 +  ({x2})*x_2 +   ({x3})*x_3 +   ({x4})*x_4 + ({x5})*x_5 + \\\n","                ({x6})*x_6 + ({x7})*x_7$'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 3. Fit a linear lasso regression model to this data using a computer program - can use packages."]},{"cell_type":"code","execution_count":238,"metadata":{},"outputs":[{"data":{"text/plain":["0.00015660574547596038"]},"execution_count":238,"metadata":{},"output_type":"execute_result"}],"source":["lasso_reg = Lasso(alpha=10e-4, fit_intercept=False).fit(x, y)\n","y_hat = lasso_reg.predict(x)\n","rss = np.sum((y-y_hat)**2)\n","rss"]},{"cell_type":"code","execution_count":239,"metadata":{},"outputs":[{"data":{"text/latex":["$f(x) = (0.452)*x_1 +  (0.475)*x_2 +   (0.069)*x_3 +   (0.0)*x_4 + (0.0)*x_5 +                 (0.0)*x_6 + (0.0)*x_7$"],"text/plain":["<IPython.core.display.Latex object>"]},"metadata":{},"output_type":"display_data"}],"source":["x1,x2,x3,x4,x5,x6,x7 = np.round(lasso_reg.coef_, 3)\n","display(Latex(f'$f(x) = ({x1})*x_1 +  ({x2})*x_2 +   ({x3})*x_3 +   ({x4})*x_4 + ({x5})*x_5 + \\\n","                ({x6})*x_6 + ({x7})*x_7$'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 4. Use the following subset selection methods to choose the two features that best explain the data:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### a. Forward Stepwise"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["feature, RSS Value, Weight\n","0 1.7680480306099482 0.5005626828719334\n","1 1.5559485515856177 0.5004526294944694\n","2 0.34805009659969083 0.9066846986089646\n","3 0.3239590689664171 0.885160347571954\n","4 0.10393465264034997 0.9160504114154776\n","5 0.6325514156337191 0.7849334492064217\n","6 0.6943223421821415 0.8177314873618696\n"]}],"source":["print(\"feature, RSS Value, Weight\")\n","for i in range(7):\n","    temp_x = x[:,i].copy()\n","    temp_x = temp_x.T\n","    xTx = np.matmul(temp_x.T, temp_x)\n","    xTx_inv = 1/xTx\n","    weight = xTx_inv*np.matmul(temp_x.T,y)\n","    y_hat = weight*temp_x\n","    print(i, np.sum((y-y_hat)**2), weight)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we can see the feature 5 has the least RSS, so we select feature 5 as the first feature and can go for further selection."]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["feature, RSS Value, Weight Vector\n","0 0.08939491221949343 [0.86424766 0.06320073]\n","1 0.10375136110003312 [ 0.92379547 -0.00764925]\n","2 0.06418266693959254 [0.68199387 0.26275088]\n","3 0.09441954725164416 [0.76785156 0.15649981]\n","5 0.10191595411157069 [0.86769165 0.05015582]\n","6 0.07826337555928795 [0.77953228 0.15717367]\n"]}],"source":["x_5 = x[:,4].copy()\n","print(\"feature, RSS Value, Weight Vector\")\n","for i in range(7):\n","    if i == 4:\n","        continue\n","    temp_x = x[:,i].copy()\n","    temp_x = np.array([x_5,temp_x])\n","    temp_x = temp_x.T\n","    xTx = np.matmul(temp_x.T, temp_x)\n","    xTx_inv = np.linalg.inv(xTx)\n","    weights = np.matmul(np.matmul(xTx_inv, temp_x.T), y)\n","    y_hat = np.matmul(temp_x, weights)\n","    print(i, np.sum((y-y_hat)**2), weights)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here, the **feature 3** in combination with **feature 5** since it has the least RSS value. Also we see that the weight value related to feature 5 has changed from 0.916 to 0.767"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### b. Backward Stepwise"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[],"source":["def get_rss(x):\n","    xTx = np.matmul(x.T, x)\n","    xTx_inv = np.linalg.inv(xTx)\n","    weights = np.matmul(np.matmul(xTx_inv, x.T), y)\n","    y_hat = np.matmul(x, weights)\n","    rss = np.sum((y-y_hat)**2)\n","    return rss"]},{"cell_type":"code","execution_count":219,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6.775622303276243e-05\n","6.85232574647189e-05\n","7.037563225111191e-05\n","7.54452176001615e-05\n","0.000110076377459281\n","Features Index [1, 3]\n"]}],"source":["i = 6\n","features = [0,1,2,3,4,5,6]\n","while i != 1:\n","    comb = list(itertools.combinations(features, i))\n","    rss = []\n","    for j in comb:\n","        temp_x = x.copy()\n","        temp_x = temp_x.T\n","        temp_x = temp_x[list(j)].T\n","        rss.append(get_rss(temp_x))\n","    rss = np.array(rss)\n","    features = list(comb[rss.argmin()])\n","    i -= 1\n","print(\"Features Index\",features)\n","    \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In backward stepwise selection, we got the **feature 2 and 4** as the best selection."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### c. Forward Stagewise"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["From Forward Stepwise we know that the first feature is feature 5, now we need to find 2nd feature using stagewise increment"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["feature, RSS Value, Weight\n","0 1.4321712093629684 0.5005626828719334\n","1 1.8985698176821504 0.5004526294944694\n","2 2.808137102726856 0.9066846986089646\n","3 2.998941025794095 0.885160347571954\n","5 2.7347983916718257 0.7849334492064217\n","6 2.469091091780378 0.8177314873618696\n"]}],"source":["x_5 = x[:,4].copy()\n","weight_5 = 0.9160504114154776\n","print(\"feature, RSS Value, Weight\")\n","for i in range(7):\n","    if i == 4:\n","        continue\n","    temp_x = x[:,i].copy()\n","    temp_x = temp_x.T\n","    xTx = np.matmul(temp_x.T, temp_x)\n","    xTx_inv = 1/xTx\n","    weight = xTx_inv*np.matmul(temp_x.T,y)\n","    temp_x = np.array([x_5,temp_x]).T\n","    weights = np.array([weight_5, weight])\n","    y_hat = np.matmul(temp_x, weights)\n","    print(i, np.sum((y-y_hat)**2), weight)\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we can see the RSS is least for **feature 1**.\n","\n","Hence the two features which get selected are **Festure 1 and 5**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### d. Bestsubset Selection"]},{"cell_type":"code","execution_count":221,"metadata":{},"outputs":[{"data":{"text/plain":["(0, 3)"]},"execution_count":221,"metadata":{},"output_type":"execute_result"}],"source":["features = [0,1,2,3,4,5,6]\n","comb = list(itertools.combinations(features, 2))\n","rss = []\n","for c in comb:\n","    temp_x = x.copy()\n","    temp_x = temp_x.T\n","    temp_x = temp_x[list(c)].T\n","    rss.append(get_rss(temp_x))\n","rss = np.array(rss)\n","comb[rss.argmin()]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Features 1 and 4** are the features having the least RSS value in all possible $7\\choose2$ combinations for features "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 5. Explain all your observations based on the results."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We see that the co-effectients for Linear Regression and Ridge Regression, doesn't change much and all the features are being used as the co-effecients for all the seven features are non-zero values.\n","\n","But when we use Lasso regression, we can see that features $x_1$, $x_2$ are dominant, and there is little influence of feature $x_3$, but features 4,5,6 and 7 are having co-effecients 0. Here this is a feature selection and shrinkage methond, where we only need three features to predict the results\n","\n","Similarly when we performed subset selection different methods yielded different features,\n","\n","Forward Stepwise selection, is you select the first feature with least RSS which gives feature 5, and than pair with all other features. We find the new $\\beta's$ for each pair and see which gives least RSS. We found that **Feature 3 and 5** is selected.\n","\n","In Backward Stepwise selection, we start with all features, and kept on removing feature which harmed the RSS. We got **features 2 and 4**\n","\n","While, in Forward Stagewise, we don't re train the $\\beta$ after getting the $\\beta$ for first selection of feature 5. We got **feature 5 and 1**. We also see that Forward Stepwise selection have features selection which gives lower RSS compared which showed that Forward Stepwise is better than Forward Stagewise, but the stagewise selection is computationally better than stepwise, since we don't train the first selected parameter.\n","\n","Finally, In subset selection we formed all combinations of features, where we had $7\\choose2$ combinations and calculated RSS for each of the pair and selected the combination with the least RSS. This is computational heavy but gives the best two features.\n","**Features 1 amd 4** are selected to best features. This selection method is least computationally effective solution, as it has to find parameters for all combinations."]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5 (main, Jul 16 2022, 07:05:41) [Clang 13.0.0 (clang-1300.0.27.3)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8d519ce160da629244c5fa84cf02a853bdfbd2d6722f9390c675e535dc1f1a96"}}},"nbformat":4,"nbformat_minor":2}
