[
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html",
    "title": "Machine Learning Homework I",
    "section": "",
    "text": "This symmetric matrix is called the Hadamard matrix and it has orthogonal columns:\n\\[\nH =\n\\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n1 & -1 & 1 & -1\\\\\n1 & 1 & -1 & -1\\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n\\]\nalso, \\(H^2 = 4I\\)\n\nWhat is the determinant of \\(H\\)?\n\nWhat are the eigenvalues of \\(H\\)?\n\nWhat are the singular values of \\(H\\)?\n\n\n\n\n\nWe know \\(det(A^2) = det(A)^2\\), Given \\(H^2 = 4I\\),\n\\(det(H) = \\sqrt{det(H^2)} = \\sqrt{det(4I)}\\)\nWe also know that \\(det(kA) = k^n det(A)\\), where \\(n\\) is the dimension of the square matrix \\(A\\)\n\\(det(4I_4) = 4^4 det(I_4)\\) \\[\ndet(I_4) =\n\\begin{vmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{vmatrix} = 1*det(I_3) + 0 + 0 + 0\n\\] \\[\ndet(I_3) =\n\\begin{vmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\end{vmatrix} = 1*(1-0) + 0 + 0 = 1\n\\] \\[\\therefore det(I_4) = 1 * 1 = 1\\]\n\\[det(4I_4) = 4^4 * 1 = 64\\]\n\\[det(H) = \\sqrt{det(H^2)} = \\sqrt{det(4I)} = \\sqrt{64} = 16\\]\n\n\n\nFor Eigen Values of H, we need to solve the Characteristic equation which is \\(|H - \\lambda I| = 0\\)\n\\[\ndet(H - \\lambda I) =\n\\begin{vmatrix}\n\\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n1 & -1 & 1 & -1\\\\\n1 & 1 & -1 & -1\\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n\\lambda & 0 & 0 & 0\\\\\n0 & \\lambda & 0 & 0\\\\\n0 & 0 & \\lambda & 0\\\\\n0 & 0 & 0 & \\lambda\n\\end{bmatrix}\n\\end{vmatrix} =\n\\begin{vmatrix}\n1-\\lambda & 1 & 1 & 1\\\\\n1 & -1-\\lambda & 1 & -1\\\\\n1 & 1 & -1-\\lambda & -1\\\\\n1 & -1 & -1 & 1-\\lambda\n\\end{vmatrix}\n\\]\nWe know that the determinant doesn’t changes when we do row operations of subtracting a multiple of a row with another row.\n\\(R_3 = R_3 - R_2\\)\n\\(R_4 = R_4 - R_2\\)\n\\(\\begin{vmatrix} 1-\\lambda & 1 & 1 & 1\\\\ 1 & -1-\\lambda & 1 & -1\\\\ 0 & 2+\\lambda & -2-\\lambda & 0\\\\ 0 & \\lambda & -2 & 2-\\lambda \\end{vmatrix} = 0\\)\n\\(= (1-\\lambda) \\begin{vmatrix} -1-\\lambda & 1 & -1\\\\ 2+\\lambda & -2-\\lambda & 0\\\\ \\lambda & -2 & 2-\\lambda \\end{vmatrix} - 1 \\begin{vmatrix} 1 & 1 & -1\\\\ 0 & -2-\\lambda & 0\\\\ 0 & -2 & 2-\\lambda \\end{vmatrix} + 1 \\begin{vmatrix} 1 & -1-\\lambda & -1\\\\ 0 & 2+\\lambda & 0\\\\ 0 & \\lambda & 2-\\lambda \\end{vmatrix} - 1 \\begin{vmatrix} 1 & -1-\\lambda & 1 \\\\ 0 & 2+\\lambda & -2-\\lambda\\\\ 0 & \\lambda & -2 \\end{vmatrix} = 0\\)\nFor \\(1^{st}\\) sub determinant\n\\(\\begin{vmatrix} -1-\\lambda & 1 & -1\\\\ 2+\\lambda & -2-\\lambda & 0\\\\ \\lambda & -2 & 2-\\lambda \\end{vmatrix} = -({1+\\lambda})*(\\lambda^2 - 4) - 1*(4 - \\lambda^2) + (-1) * (\\lambda^2 - 4)\\)\n\\(= (\\lambda^2 - 4) * (-1 - \\lambda + 1 - 1) = (\\lambda + 1)*(4 - \\lambda^2)\\)\nFor \\(2^{st}\\) sub determinant\n\\(\\begin{vmatrix} 1 & 1 & -1\\\\ 0 & -2-\\lambda & 0\\\\ 0 & -2 & 2-\\lambda \\end{vmatrix} = 1 * -(4 - \\lambda^2) = -(4-\\lambda^2)\\)\nFor \\(3^{st}\\) sub determinant\n\\(\\begin{vmatrix} 1 & -1-\\lambda & -1\\\\ 0 & 2+\\lambda & 0\\\\ 0 & \\lambda & 2-\\lambda \\end{vmatrix} = 1 * (4 - \\lambda^2) = (4-\\lambda^2)\\)\nFor \\(4^{st}\\) sub determinant\n\\(\\begin{vmatrix} 1 & -1-\\lambda & 1 \\\\ 0 & 2+\\lambda & -2-\\lambda\\\\ 0 & \\lambda & -2 \\end{vmatrix} = 1 * (-4 - 2\\lambda + 2\\lambda + \\lambda^2) = (-4+\\lambda^2) = -(4-\\lambda^2)\\)\n\\(det(H - \\lambda I) = (1 - \\lambda)*(\\lambda + 1)*(4 - \\lambda^2) + (-1) * -(4-\\lambda^2) + 1 * (4-\\lambda^2) + (-1) * -(4-\\lambda^2) = 0\\)\n\\((4-\\lambda^2)(1 - \\lambda^2 + 1 + 1 + 1) = 0\\)\n\\((4-\\lambda^2)(4 - \\lambda^2) = 0\\)\n\\[ \\lambda = \\pm 2, \\pm 2\\]\nTherefore, The Eigen Values for \\(H\\) are \\(2, 2, -2, and -2\\)\n\n\n\nIf A is symmetric then \\(AA^T = A^TA=A^2\\) and \\(U,V,\\sum\\) are square matrices. The eigenvectors of \\(A\\) are also eigenvectors of \\(A^2\\) with squared corresponding eigenvalues and the singular values are the absolute values of the eigenvalues of \\(A\\)\nSince \\(H\\) is a symmetric matrix, the Singular values of \\(H\\) are \\(|2|, |2|, |-2|, |-2|\\),\nSingular Values of \\(H = 2,2,2,2\\)"
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-2",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-2",
    "title": "Machine Learning Homework I",
    "section": "Problem 2",
    "text": "Problem 2\nCalculate the Singular Value Decomposition for the following matrices.\n\\[\nA =\n\\begin{bmatrix}\n1 & -1\\\\\n0 & 1\\\\\n1 & 0\\\\\n\\end{bmatrix}\n\\]\n\\[\nB =\n\\begin{bmatrix}\n1 & 0 & 1\\\\\n-1 & 1 & 0\\\\\n\\end{bmatrix}\n\\]\nIf \\(A=UΣV^T\\) is an \\(SVD\\) for \\(A\\), Whatcan you say about the \\(SVD\\) for \\(A^T\\) ?\n\nSolution\nWe know \\(A=UΣV^T\\)\n\\(A^T = (UΣV^T)^T = (V^T)^TΣ^TU^T = VΣU^T (Σ^T = Σ)\\)\nAnd, \\(AA^T = UΣV^TVΣU^T = UΣ^2U^T (V.V^T = I)\\)\n\\[\nAA^T =\n\\begin{bmatrix}\n2 & -1 & 1\\\\\n-1 & 1 & 0\\\\\n1 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nCharacteristic Equation to Find Eigen value is \\(|AA^T - \\lambda I| = 0\\)\n\\[\n|AA^T - \\lambda I| =\n\\begin{vmatrix}\n2-\\lambda & -1 & 1\\\\\n-1 & 1-\\lambda & 0\\\\\n1 & 0 & 1-\\lambda\\\\\n\\end{vmatrix} = 0\n\\]\n\\(1*(0-(1-\\lambda)) - 0 + (1-\\lambda)*((1-\\lambda)*(2-\\lambda) - 1) = 0\\)\n\\((1-\\lambda)*(-1+\\lambda^2-3\\lambda+1) = 0\\)\n\\((1-\\lambda)*(\\lambda^2-3\\lambda) = 0\\)\n\\((1-\\lambda)* \\lambda *(\\lambda-3) = 0\\)\n\\(\\lambda = 0,1,3\\)\nFor \\(\\lambda = 0\\)\n\\(A.v = \\lambda.v\\)\n\\((A-\\lambda I).v = 0\\)\n\\(\\begin{bmatrix} 2 & -1 & 1\\\\ -1 & 1 & 0\\\\ 1 & 0 & 1\\\\ \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ \\end{bmatrix} = 0\\)\nPerforming Row Operations\n\\(R_1 = R_1/2\\)\n\\(R_2 = R_2+R_1\\)\n\\(R_3 = R_3-R_1\\)\n\\(R_2 = R_2/2\\)\n\\(R_3 = R_3-0.5*R_2\\)\n\\(R_1 = R_1-0.5*R_2\\)\n\\(\\begin{pmatrix} 1 & 0 & 1\\\\ 0 & 1 & 1\\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ \\end{bmatrix} = 0\\)\n\\(x_3 = -x_2\\) and \\(x3 = -x_1\\)\nEigen Vector corresponding to \\(\\lambda = 0\\) is \\(\\begin{bmatrix} -1\\\\ -1\\\\ 1\\\\ \\end{bmatrix}\\)\nSimilarly the Eigen Vectors for \\(\\lambda = 1\\) is \\(\\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ \\end{bmatrix}\\)\nAnd the Eigen Vectors for \\(\\lambda = 3\\) is \\(\\begin{bmatrix} -2\\\\ 1\\\\ -1\\\\ \\end{bmatrix}\\)\n\\(Σ^2 = \\begin{pmatrix} 3 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 0 \\end{pmatrix}\\)\n\\(Σ = \\begin{pmatrix} \\sqrt3 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 0 \\end{pmatrix}\\)\n\\(U = \\begin{pmatrix} -2/\\sqrt6 & 0 & -1/\\sqrt3\\\\ 1/\\sqrt6 & 1/\\sqrt2 & -1/\\sqrt3\\\\ -1/\\sqrt6 & 1/\\sqrt2 & 1/\\sqrt3 \\end{pmatrix}\\)\nSimilarly, \\(A^TA = VΣU^TUΣV^T = VΣ^2V^T (U.U^T = I)\\)\n\\[\nAA^T =\n\\begin{bmatrix}\n2 & -1\\\\\n-1 & 2\\\\\n\\end{bmatrix}\n\\]\nCharacteristic Equation to Find Eigen value is \\(|AA^T - \\lambda I| = 0\\)\n\\[\n|AA^T - \\lambda I| =\n\\begin{bmatrix}\n2-\\lambda & -1\\\\\n-1 & 2-\\lambda\\\\\n\\end{bmatrix} = 0\n\\]\n$ (2-)^2 - 1 = 0$\n$ (2-)^2 = 1$\n$ (2-) = $\n\\(\\lambda = 3,1\\)\nEigen Vector corresponding to \\(\\lambda = 1\\) is \\(\\begin{bmatrix} 1\\\\ 1\\\\ \\end{bmatrix}\\)\nSimilarly the Eigen Vectors for \\(\\lambda = 3\\) is \\(\\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}\\)\n\\(Σ^2 = \\begin{pmatrix} 3 & 0\\\\ 0 & 1\\\\ \\end{pmatrix}\\)\n\\(Σ = \\begin{pmatrix} \\sqrt3 & 0\\\\ 0 & 1\\\\ \\end{pmatrix}\\)\n\\(V = \\begin{pmatrix} -1/\\sqrt2 & 1/\\sqrt2\\\\ 1/\\sqrt2 & 1/\\sqrt2\\\\ \\end{pmatrix}\\)\nHence the \\(SVD\\) of Matrix \\(A\\) is\n\\[\nA = U Σ V^T\n\\]\n\\[\nA =\n\\begin{pmatrix}\n-2/\\sqrt6 & 0 & -1/\\sqrt3\\\\\n1/\\sqrt6 & 1/\\sqrt2 & -1/\\sqrt3\\\\\n-1/\\sqrt6 & 1/\\sqrt2 & 1/\\sqrt3\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sqrt3 & 0\\\\\n0 & 1\\\\\n0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n-1/\\sqrt2 & 1/\\sqrt2\\\\\n1/\\sqrt2 & 1/\\sqrt2\\\\\n\\end{pmatrix}\n\\]\n\nVerification\n\nA = np.array([\n    [1,-1],\n    [0,1],\n    [1,0]\n])\n\nU = np.array([\n    [-2/np.sqrt(6), 0, -1/np.sqrt(3)],\n    [1/np.sqrt(6), 1/np.sqrt(2), -1/np.sqrt(3)],\n    [-1/np.sqrt(6), 1/np.sqrt(2), 1/np.sqrt(3)]\n])\n\nS = np.array([\n    [np.sqrt(3), 0],\n    [0,1],\n    [0,0]\n])\n\nVt = np.array([\n    [-1/np.sqrt(2), 1/np.sqrt(2)],\n    [1/np.sqrt(2), 1/np.sqrt(2)]\n])\n\nnp.allclose(A, np.matmul(U,np.matmul(S,Vt)))\n\nTrue\n\n\nGiven \\[\nB =\n\\begin{bmatrix}\n1 & 0 & 1\\\\\n-1 & 1 & 0\\\\\n\\end{bmatrix} = A^T\n\\]\n\\(SVD(B) = SVD(A^T)\\)\nWe Know, \\(SVD(A) = U Σ V^T\\)\n\\(SVD(A^T) = (U Σ V^T)^T\\)\n\\(SVD(A^T) = (V Σ^T U^T)\\)\nHence the \\(SVD\\) of Matrix \\(B\\) is\n\\[\nB = V Σ^T U^T\n\\]\nWhere \\(U,Σ,V\\) are from Matrix \\(A\\)\n\\[\nA =\n\\begin{pmatrix}\n-1/\\sqrt2 & 1/\\sqrt2\\\\\n1/\\sqrt2 & 1/\\sqrt2\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sqrt3 & 0\\\\\n0 & 1\\\\\n0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n-2/\\sqrt6 & 1/\\sqrt6 & -1/\\sqrt6\\\\\n0 & 1/\\sqrt2 & 1/\\sqrt2\\\\\n-1/\\sqrt3 & -1/\\sqrt3 & 1/\\sqrt3\n\\end{pmatrix}\n\\]\n\n\nVerification\n\nB = np.array([\n    [1,0,1],\n    [-1,1,0]\n])\nnp.allclose(B, np.matmul(Vt.T,np.matmul(S.T,U.T)))\n\nTrue"
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-3",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-3",
    "title": "Machine Learning Homework I",
    "section": "Problem 3",
    "text": "Problem 3\nIn image processing, consider this situation: A satellite takes a picture and wants to send it to Earth. In order to do that, a compression is used. Take an image from the web of Einstein. Apply SVD on the 2D image and compress it to 30% singular values (make the 70% lower singular values as zero). Depict the original image and the compressed image and comment.\n\neinstein = Image.open(\"./data/Albert_Einstein_Head.jpg\")\nprint(einstein.size)\neinstein_arary = np.array(einstein)\nprint(einstein_arary.shape)\n\n(3250, 4333)\n(4333, 3250)\n\n\n\nu, s, vt = np.linalg.svd(einstein_arary, full_matrices=False)\nu.shape, s.shape, vt.shape\n\n((4333, 3250), (3250,), (3250, 3250))\n\n\n\nRETENTION_FACTOR = [0.5,0.3,0.1,0.01]\nreduced_einstein = []\n\nfor r in RETENTION_FACTOR:\n    new_s = s.copy()\n    new_s[int(len(s)*r):] = 0\n    reduced_einstein.append(np.dot(u*new_s, vt))\n\n\nf,ax = plt.subplots(ncols=5)\nf.set_size_inches(15,40)\n\nax[0].set_title('Original image')\nax[0].imshow(einstein_arary, cmap='gray')\n\ntitles = ['50% Retained (50% Compression)', '30% Retained (70% Compression)',\n            '10% Retained (90% Compression)', '1% Retained (99% Compression)']\nfor i in range(4):\n    ax[i+1].set_title(titles[i])\n    ax[i+1].imshow(reduced_einstein[i], cmap='gray')\n\nfor a in ax:\n    a.xaxis.set_visible(False)\n    a.yaxis.set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nAfter performing SVD on the image of size 4333 x 3250, we got 3250 Singular Value in the Diagonal Matrix \\(Σ\\), of which we are using the highest 30%, which is 975 singular values and making all the other as zero, to compress the image by 70%.\nEven after compressing the image to 70% and retaning only 30% of the data, the Einstein is clear in the image and only the very fine details in the image are lost.\nThis was even more evident when we loss 99% of the singular values and only use top 1%, which makes the image blurry due to high loss of data. But even than the person in the image is clearly visible."
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-4",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-4",
    "title": "Machine Learning Homework I",
    "section": "Problem 4",
    "text": "Problem 4\nCalculate the Gradient of the following functions\n\n\\(f(x,y)=x+3y^2\\),\n\\(f(x,y)=\\sqrt{x^2+y^2}\\)\n\n\nSolution\n\n(a)\n\\[\n\\triangledown f(x,y) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x}\\\\\n\\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial (x+3y^2)}{\\partial x}\n= 1\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial (x+3y^2)}{\\partial y}\n= 6y\n\\]\n\\(\\triangledown f(x,y) = \\begin{bmatrix} 1\\\\ 6y \\end{bmatrix}\\)\n\n\n\n(b)\n\\[\n\\triangledown f(x,y) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x}\\\\\n\\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial (\\sqrt{x^2+y^2})}{\\partial x}\n= \\frac{x}{\\sqrt{x^2+y^2}}\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial (\\sqrt{x^2+y^2})}{\\partial y}\n= \\frac{y}{\\sqrt{x^2+y^2}}\n\\]\n\\(\\triangledown f(x,y) = \\begin{bmatrix} \\frac{x}{\\sqrt{x^2+y^2}}\\\\ \\frac{y}{\\sqrt{x^2+y^2}} \\end{bmatrix}\\)"
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-5",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW1.html#problem-5",
    "title": "Machine Learning Homework I",
    "section": "Problem 5",
    "text": "Problem 5\nConsider the optimization problem:\nminimize \\((1/2)x^TPx + q^Tx + r\\)\nsubject to \\(−1≤x_i≤1, i=1,2,3\\)\nwhere,\n\\[\nP = \\begin{bmatrix}\n13 & 12 & -2\\\\\n12 & 17 & 6\\\\\n-2 & 6 & 12\\\\\n\\end{bmatrix},\nq = \\begin{bmatrix}\n-22\\\\\n-14.5\\\\\n13.0\\\\\n\\end{bmatrix},\nr = 1\n\\]\n\nShow that the problem is convex ( Hint : \\(x^TAx\\) is convex if \\(A\\) is positive semi-definite)\nSolve the optimization problem using Projected Gradient Descent method.\nAttempt to solve the problem mathematically and find the optimal solution\n\n\nSolution\n\n(a)\nTo prove the minimization equation is convex, we can prove that each component is convex. The term \\(q^Tx\\) is linear and hence convex, so we only need to prove \\(x^TPx\\) is convex to proof the entire equation is convex\nWe know (from Hint), \\(x^TAx\\) is convex if \\(A\\) is positive semi-definite, which means A should have non-negative eigen values.\nFor Eigen Values of P, we need to solve the Characteristic equation which is \\(|P - \\lambda I| = 0\\)\n\\[\n|P-\\lambda I| =\n\\begin{vmatrix}\n13-\\lambda & 12 & -2\\\\\n12 & 17-\\lambda & 6\\\\\n-2 & 6 & 12-\\lambda\n\\end{vmatrix} = 0\n\\]\n(Using Triangle’s Rule) \\[\n(-λ+13)*(-λ+17)*(-λ+12)+12*6*(-2)+\\\\\n(-2)*12*6-(-2)*(-λ+17)*(-2)-6*\\\\6*(-λ+13)-(-λ+12)*12*12 = 0\n\\]\n\\[-λ^3+42*λ^2-397*λ+100 = 0\\]\nSolving it using a solver, we get\n\\[\\lambda = 0.259, 13.843, 27.898\\]\nSince, all the Eigen Values of P are positive, P is positive semi-definite matrix, hence \\(x^TPx\\) is convex.\nThe entire equation \\((1/2)x^TPx + q^Tx + r\\) is convex\n\n\n(b)\n\nx = np.array([1.9,-1.5])\nnp.sign(x)\n\narray([ 1., -1.])\n\n\n\ndef func(x, P, q, r):\n    return (x.T@P)@x + q.T@x + r\n\ndef func_diff(x, P, q, r):\n    return P.T@x + q\n\ndef update_x(x, grad, alpha=0.0001):\n    x  = x - alpha*grad\n    # Take Minimum Value of Element or 1\n    # Multiply by it's original sign to get back sign\n    x = np.minimum(1,np.abs(x)) * np.sign(x)\n    return x\n\n\nx = np.random.random(3).reshape(3,1)\nP = np.array([\n    [13,12,-2],\n    [12,17,6],\n    [-2,6,12]\n])\nq = np.array([-22,-14.5, 13]).reshape(3,1)\nr = 1\n\nfor _ in range(100_000):\n    x = update_x(x, func_diff(x,P,q,r)) \n\nx\n\narray([[ 1. ],\n       [ 0.5],\n       [-1. ]])\n\n\n\n\n(c)\nConstrained Optimization\n\\[\nf(x) = (1/2)x^TPx + q^Tx + r + \\lambda_1 (x - 1) + \\lambda_2 (x + 1)\n\\]\n\\[\n\\frac{\\partial f}{\\partial \\lambda_1} = x-1 = 0\\\\\nx = 1\\\\\nx =\n\\begin{bmatrix}\n1\\\\\n1\\\\\n1\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\frac{\\partial f}{\\partial \\lambda_2} = x+1 = 0\\\\\nx = -1\\\\x =\n\\begin{bmatrix}\n-1\\\\\n-1\\\\\n-1\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = 1/2(P + P^T)x + q^T + \\lambda_1 + \\lambda_2\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = Px + q^T + \\lambda_1 + \\lambda_2\n\\]\n\\[\\frac{\\partial f}{\\partial x} = 0\\]\n\\[\\lambda_1 = 0, \\lambda_2 = 0\\]\n\\[\n\\frac{\\partial f}{\\partial x} = 0\\\\\nPx + q^T = 0\n\\]\nPutting Values of P and q \\[\n\\begin{bmatrix}\n13 & 12 & -2\\\\\n12 & 17 & 6\\\\\n-2 & 6 & 12\\\\\n\\end{bmatrix}.\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\n-22\\\\\n-14.5\\\\\n13.0\\\\\n\\end{bmatrix} = 0\n\\]\n\\[\n13x_1 + 12x_2 - 2x_3 - 22 = 0\n\\] \\[\n12x_1 + 17x_2 + 6x_3 - 14.5 = 0\n\\] \\[\n-2x_1 + 6x_2 + 12x_3 + 13 = 0\n\\]\nOn solving for \\(x_1, x_2 and x_3\\), we get\n\\[x =\n\\begin{bmatrix}\n0.56\\\\\n0.98\\\\\n-1.48\\\\\n\\end{bmatrix}\n\\]\nFinally \\[\nx =\n\\begin{bmatrix}\n1\\\\\n1\\\\\n1\\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1\\\\\n-1\\\\\n-1\\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n0.56\\\\\n0.98\\\\\n-1.48\\\\\n\\end{bmatrix}\n\\]\nSelecting the minimum values to be under constrain:\n\\[\nx =\n\\begin{bmatrix}\n0.56\\\\\n0.98\\\\\n-1\\\\\n\\end{bmatrix}\n\\]\n\nx = np.random.random(3).reshape(3,1)\nP = np.array([\n    [13,12,-2],\n    [12,17,6],\n    [-2,6,12]\n])\nq = np.array([-22,-14.5, 13])\nr = 1\n\nx = np.array([0.56, 0.98, -1])\n\nP@x.T + q\n\narray([-0.96,  2.88,  5.76])"
  },
  {
    "objectID": "07_MachineLearning_I/third.html",
    "href": "07_MachineLearning_I/third.html",
    "title": "Plaksha",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBClassifier\n\n\ndf = pd.read_csv(\"./project/train.csv\", index_col=0)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      lepton_1_pT\n      lepton_1_eta\n      lepton_1_phi\n      lepton_2_pT\n      lepton_2_eta\n      lepton_2_phi\n      missing_energy_magnitude\n      missing_energy_phi\n      MET_rel\n      axial_MET\n      M_R\n      M_TR_2\n      R\n      MT2\n      S_R\n      M_Delta_R\n      dPhi_r_b\n      cos(theta_r1)\n      class\n    \n  \n  \n    \n      0\n      0.841381\n      1.832647\n      -0.689286\n      0.781839\n      0.572864\n      1.577097\n      0.398978\n      -0.683847\n      0.001826\n      0.651397\n      0.865560\n      0.429017\n      0.439840\n      0.000000\n      0.796105\n      0.342497\n      0.461542\n      0.005710\n      0.0\n    \n    \n      1\n      0.663798\n      2.058290\n      0.681435\n      1.054036\n      0.575352\n      -1.001445\n      0.462154\n      -0.833411\n      0.199734\n      0.215158\n      0.949988\n      0.618046\n      0.577324\n      0.000000\n      0.962927\n      0.333800\n      1.455247\n      0.101246\n      0.0\n    \n    \n      2\n      1.792225\n      -1.099978\n      0.088109\n      0.573157\n      -0.472629\n      1.642084\n      1.203374\n      1.506731\n      0.457695\n      -0.640507\n      1.157024\n      1.585432\n      1.215963\n      0.000000\n      1.113292\n      0.645729\n      0.721326\n      0.613326\n      1.0\n    \n    \n      3\n      0.893018\n      0.297782\n      -1.274870\n      1.316164\n      1.593303\n      0.672115\n      0.307014\n      -1.189868\n      0.064561\n      0.430909\n      1.162625\n      0.548821\n      0.418897\n      0.163908\n      1.157707\n      0.298163\n      0.803802\n      0.038902\n      0.0\n    \n    \n      4\n      1.338997\n      0.350023\n      -1.518510\n      1.482963\n      -0.491807\n      0.340170\n      0.415071\n      -1.292034\n      0.240712\n      0.611775\n      1.307798\n      0.697804\n      0.473487\n      0.429977\n      1.287935\n      0.330327\n      0.717237\n      0.003147\n      1.0\n    \n  \n\n\n\n\n\ndf_test = pd.read_csv(\"./project/test.csv\", index_col=0)\ncolumns = ['class'] + list(df.columns)[:-1]\ndf_full = pd.read_csv(\"./project/SUSY.csv\", names=columns)\ndf_full = df_full[list(df.columns)]\n\ndf_full['JOIN'] = df_full['lepton_1_pT'].astype('str').apply(lambda x: x[:7]) + \\\n                    df_full['lepton_1_eta'].astype('str').apply(lambda x: x[:7]) + \\\n                        df_full['lepton_1_phi'].astype('str').apply(lambda x: x[:7])\n\ndf_test['JOIN'] = df_test['lepton_1_pT'].astype('str').apply(lambda x: x[:7]) + \\\n                    df_test['lepton_1_eta'].astype('str').apply(lambda x: x[:7]) + \\\n                        df_test['lepton_1_phi'].astype('str').apply(lambda x: x[:7])\n\ndf_all = df_full.merge(df_test, on=['JOIN'], how='left', indicator=True)\n\ndf_test = df_full.iloc[(df_all[df_all['_merge'] == 'both']).index].drop('JOIN', axis=1)\n\n\nn_rows = 5\nn_cols = 4\nfig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(25,25))\n\nfor i, column in enumerate(df.columns):\n    sns.histplot(df[column],ax=axes[i//n_cols,i%n_cols])\n\nplt.show()\n\n\n\n\n\nY = df['class']\ndf_train = df.drop('class',axis=1)\n\nYFinal = df_test['class']\nXFinal = df_test.drop('class', axis=1)\n\nfrom sklearn.ensemble import  RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom scipy.stats import uniform\n\nXtrain,Xtest,Ytrain,Ytest=train_test_split(df_train,Y,test_size=0.10,random_state=42,shuffle=True)\n\n\nimport pandas as pd\nwith open(\"/Users/syed/Downloads/Savyasachi Submission_16.csv\", 'r') as f:\n    f.readline()\n    data =f.readlines()\naaysuh = pd.DataFrame([i.strip().split(',')[1] for i in data], dtype=float)\n\nsum(aaysuh[0].values == df_test['class'].values)/1_500_000\n\n0.8044893333333333\n\n\n\nclassifier_XGB = XGBClassifier()\nclassifier_XGB.fit(Xtrain,Ytrain)\n\nprint(\"Training Acc: \",classifier_XGB.score(Xtrain, Ytrain))\n\nprint(\"Val Acc: \",classifier_XGB.score(Xtest, Ytest))\nprint(\"Test Acc: \",classifier_XGB.score(XFinal, YFinal))\n\nTraining Acc:  0.8046225396825397\nVal Acc:  0.8033114285714286\nTest Acc:  0.80312\n\n\n\nclassifier_XGB = XGBClassifier(n_estimators=100)\nclassifier_XGB.fit(Xtrain,Ytrain)\n\nprint(\"Training Acc: \",classifier_XGB.score(Xtrain, Ytrain))\n\nprint(\"Val Acc: \",classifier_XGB.score(Xtest, Ytest))\nprint(\"Test Acc: \",classifier_XGB.score(XFinal, YFinal))\n\nTraining Acc:  0.8046225396825397\nVal Acc:  0.8033114285714286\nTest Acc:  0.80312\n\n\n\nclassifier_XGB = XGBClassifier(n_estimators=1000, n_jobs=-1)\nclassifier_XGB.fit(Xtrain,Ytrain)\n\nprint(\"Training Acc: \",classifier_XGB.score(Xtrain, Ytrain))\n\nprint(\"Val Acc: \",classifier_XGB.score(Xtest, Ytest))\nprint(\"Test Acc: \",classifier_XGB.score(XFinal, YFinal))\n\nTraining Acc:  0.8148161904761905\nVal Acc:  0.8026771428571429\nTest Acc:  0.802146\n\n\n\ndef parameter_tuning(model, parameters, verbose = 2, train_X=Xtrain, train_Y=Ytrain, \n                test_X=Xtest, test_Y=Ytest):\n    \"\"\"\n    This is used for doing the hyper parameter tuning.\n    The model once trained, is than used to \n    \"\"\"\n    global ACCURACIES\n    estimator = model(random_state = 42)\n\n    num_combinations = 1\n    for k,v in parameters.items():\n        num_combinations *= len(v)\n    num_combinations = int(num_combinations*0.2)\n    classifier = RandomizedSearchCV(estimator=estimator, param_distributions= parameters, \n                                    n_iter = num_combinations, cv=2, n_jobs=-1, verbose=verbose)\n    classifier.fit(train_X, train_Y)\n    best = classifier.best_params_\n    print(best)\n\n    classifier = train_model(model, **best)\n    return classifier\n\n\nimport pandas as pd\nwith open(\"/Users/syed/Downloads/best.csv\", 'r') as f:\n    f.readline()\n    data =f.readlines()\naaysuh = pd.DataFrame([i.strip().split(',')[1] for i in data], dtype=float)\n\nsum(aaysuh[0].values == df_test['class'].values)/1_500_000"
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW2.html",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW2.html",
    "title": "Machine Learning Homework II",
    "section": "",
    "text": "In real world applications, there are outliers in data. This can be dealt with using a soft margin, specified in a slightly different optimization problem as below (soft-margin SVM):\n\\[\nmin \\frac{1}{2}w^Tw + C \\sum_{i}^{N} \\xi_i where \\xi_i \\ge 0\n\\\\\ns.t. y^{(i)} (w^Tx^{(i)} + b) \\ge 1 - \\xi_i\n\\]\n\\(ξi\\) represents the slack for each data point \\(i\\), which allows misclassification of datapoints in the event that the data is not linearly seperable. SVM without the addition of slack terms is known as hard-margin SVM.\n\n\n\nWe know that, \\(ξi\\) is the distance between the support vector plane (margins) and the incorrectly classified data point. For example, for a data point which belongs to a positive class, \\(ξi\\) is the distance between the datapoint and \\(\\pi^+\\) margin. If the data point is above the \\(\\pi^+\\) margin, than \\(ξi = 0\\), since the point was correctly classified.\nSimilarly for a data point belonging to negative class, \\(ξi\\) is the distance between that point and \\(\\pi^-\\) margin, if the data point is below the \\(\\pi^-\\) margin, than \\(ξi = 0\\), since the point was correctly classified.\nThe Data Points lie on or above(for positive class)/below(for negative class) the margin (support vectors) when \\(ξi = 0\\) In the above diagram \\(x_k\\) is such a point and \\(x_i\\).\n\n\nYes, all the training data points whose \\(ξi=0\\) are correctly classified.\n\n\n\n\nAs mentioned above, \\(ξi\\) is the distance from the margin (support vector) to the data point, which is \\(1-Z_i\\) for incorrectly classified points and 0 otherwise. When \\(0 < ξi \\le 1\\), the point lies between the margin and hyperplane. Also given \\(ξi=1\\), so the data point can lie on the hyperplane too, when the distance between the margin and data point is 1, which is distance between the margin and hyperplane.\n\n\nAnd, No, All the training points are incorrectly classified while training. During testing we just check where does the point lie realtive to the hyperplane \\(f(x) = w^Tx + b\\). If \\(f(x) > 0\\) we say positive class, and if \\(f(x) < 0\\) we say negative class.\nBut while training we want the data point of the class to be above the positive margin \\(\\pi^+\\) (\\(w^Tx + b \\ge 1\\)) for positive class and below the negative margin \\(\\pi^-\\)(\\(w^Tx + b \\le 1\\)) for negative class. The soft margin assumes this data points between the margins to be incorrectly classified and add them to the error term during the optimization.\n\n\n\n\nWhen \\(ξi > 1\\), the data points are in the opposite direction of the hyperplane and are more than 1 distance away from the margin. The point is clearly misclassified.\nFor positive points, the point will lie below the hyperplane which is more than 1 distance below from \\(\\pi^+\\) and for negative points, the point will lie above the hyperplane and more than 1 distance above the \\(\\pi^-\\).\n\n\nNo, All the training points whose \\(ξi>1\\) are incorrectly classified."
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW2.html#problem-2",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW2.html#problem-2",
    "title": "Machine Learning Homework II",
    "section": "Problem 2",
    "text": "Problem 2\nSupport Vector Machines can be used to perform non-linear classification with a kernel trick. Recall the hard-margin SVM from class:\n\\[\nmin \\frac{1}{2}w^Tw\n\\\\\ns.t. y^{(i)} (w^Tx^{(i)} + b) \\ge 1\n\\]\nThe dual of this primal problem can be specified as a procedure to learn the following linear classifier: \\[\nf(x) = \\sum_{i}^{N} \\alpha_i y_i (x_i^Tx) + b\n\\] Note that now we can replace \\(x_i^Tx\\) with a kernel \\(k(x_i,x)\\), and have a non-linear decision boundary.\nIn Figure, there are different SVMs with different shapes/patterns of decision boundaries. The training data is labeled as \\(y_i ∈ \\{−1, 1\\}\\), represented as the shape of circles and squares respectively. Support vectors are drawn in solid circles/squares. Match the scenarios described below to one of the 6 plots (note that one of the plots does not match to anything). Each scenario should be matched to a unique plot. Explain in less than two sentences why it is the case for each scenario.\n\n1. A soft-margin linear SVM with C = 0.02.\nLinear Soft margin, with a very low value of C indicated that the model can have higher values of \\(\\xi\\), and model is fine with some misclassification but desires higher variance, which is depicted by Figure 4.\n\n\n\n2. A soft-margin linear SVM with C = 20.\nLinear Soft margin, with a high value of C indicated that we want lower values of \\(\\xi\\), and we are not so hard with misclassification and desired higher bias and closer parallel linear margins, which is depicted by Figure 3.\n\n\n\n3. A hard-margin kernel SVM with \\(k(u, v) = u · v + (u · v)^2\\)\nHere the hard margin kernel SVM has a kernel which have the function which is hyperbollic, and Figure 5 has a hyperbolla margins.\n\n\n\n4. A hard-margin kernel SVM with \\(k(u, v) = exp(−5||u − v||^2)\\)\nWith the high value of \\(\\gamma\\) in RBF kernel \\(exp(-\\gamma|||u − v||^2)\\), the kernel value is small and needs more support vectors for classifiaction which is as depiced in Figure 6\n\n\n\n5. A hard-margin kernel SVM with \\(k(u, v) = exp(−\\frac{1}{5}||u − v||^2)\\)\nWith the lower value of \\(\\gamma\\) in RBF kernel \\(exp(-\\gamma|||u − v||^2)\\), the kernel value is higher and we can classify data points using fewer support vectors, which is depiced in Figure 1"
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_HW2.html#problem-3",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_HW2.html#problem-3",
    "title": "Machine Learning Homework II",
    "section": "Problem 3",
    "text": "Problem 3\nSuppose we have the following data on seven variables \\(x1, · · · , x7\\) and the output \\(y\\), given as fol- lows:\n\nx = np.array([[0.94,0.37, 0.76, 0.56, 0.77 ,0.51 ,0.80],\n[-0.88 ,-0.33,-0.68,-0.51 ,-0.75 ,-0.47, -0.76],\n[1.32 ,-0.31 ,0.78 ,0.23, 0.47 ,0.10 ,0.90],\n[-0.21 ,0.79, 0.13, 0.46,0.27 ,0.54, 0.05],\n[ 0.75 ,-0.70 ,0.28 ,-0.22, -0.07 ,-0.34 ,0.39],\n[ -0.33, -1.30 ,-0.66, -0.98, -0.86 ,-1.06, -0.57],\n[1.27 ,0.81 ,1.14, 0.96 ,1.08, 0.92, 1.15],\n[ -0.60, 0.84, -0.13, 0.36, 0.35, 0.48, -0.24],\n[ 0.15,-1.35 ,-0.35 ,-0.85 ,-0.47 ,-0.98 ,-0.22],\n[-0.33, 0.86, 0.07, 0.46 ,0.37, 0.56, -0.03]])\n\ny = np.array([0.66, -0.60, 0.50, 0.29, 0.03, -0.82, 1.04, 0.12, -0.60, 0.26])\n\n\n1. Find the linear regression to this data using the closed form expressions (can use calculators). Does the formula work? If not, explain why not.\n\nxTx = np.matmul(x.T, x)\nnp.linalg.det(xTx)\n\n3.1679045781163164e-16\n\n\nThe determinant of \\(X^Tx\\) is very small. If this is approximated than the determinant will go to zero, and the inverse won’t exist. Since the inverse won’t exist, the problem can not be solved in closed form.\nBut if we go ahead with the small value of determinant and find the inverse, and continue to find the weights using the closed form where the weights \\(\\beta = (x^Tx)^{-1} x^T y\\). We get the weights of the model.\n\nxTx_inv = np.linalg.inv(xTx)\nweights = np.matmul(np.matmul(xTx_inv, x.T), y)\ny_hat = np.matmul(x, weights)\nrss = np.sum((y-y_hat)**2)\nrss\n\n6.496738215544207e-05\n\n\n\nx1,x2,x3,x4,x5,x6,x7 = np.round(weights, 3)\ndisplay(Latex(f'$f(x) = ({x1})*x_1 +  ({x2})*x_2 +   ({x3})*x_3 +   ({x4})*x_4 + ({x5})*x_5 + \\\n                ({x6})*x_6 + ({x7})*x_7$'))\n\n\\(f(x) = (-0.395)*x_1 + (-0.865)*x_2 + (0.501)*x_3 + (0.623)*x_4 + (0.021)*x_5 + (0.982)*x_6 + (0.128)*x_7\\)\n\n\n\n\n2. Fit a linear ridge regression model to this data using the closed form expressions\n\nLAMBDA = 10e-4\nxTx_inv = np.linalg.inv(xTx + LAMBDA*np.eye(7))\nweights = np.matmul(np.matmul(xTx_inv, x.T), y)\ny_hat = np.matmul(x, weights)\nrss = np.sum((y-y_hat)**2)\nrss\n\n9.375815496416118e-05\n\n\n\nx1,x2,x3,x4,x5,x6,x7 = np.round(weights, 3)\ndisplay(Latex(f'$f(x) = ({x1})*x_1 +  ({x2})*x_2 +   ({x3})*x_3 +   ({x4})*x_4 + ({x5})*x_5 + \\\n                ({x6})*x_6 + ({x7})*x_7$'))\n\n\\(f(x) = (-0.374)*x_1 + (-0.831)*x_2 + (0.487)*x_3 + (0.634)*x_4 + (0.021)*x_5 + (0.936)*x_6 + (0.124)*x_7\\)\n\n\n\n\n3. Fit a linear lasso regression model to this data using a computer program - can use packages.\n\nlasso_reg = Lasso(alpha=10e-4, fit_intercept=False).fit(x, y)\ny_hat = lasso_reg.predict(x)\nrss = np.sum((y-y_hat)**2)\nrss\n\n0.00015660574547596038\n\n\n\nx1,x2,x3,x4,x5,x6,x7 = np.round(lasso_reg.coef_, 3)\ndisplay(Latex(f'$f(x) = ({x1})*x_1 +  ({x2})*x_2 +   ({x3})*x_3 +   ({x4})*x_4 + ({x5})*x_5 + \\\n                ({x6})*x_6 + ({x7})*x_7$'))\n\n\\(f(x) = (0.452)*x_1 + (0.475)*x_2 + (0.069)*x_3 + (0.0)*x_4 + (0.0)*x_5 + (0.0)*x_6 + (0.0)*x_7\\)\n\n\n\n\n4. Use the following subset selection methods to choose the two features that best explain the data:\n\na. Forward Stepwise\n\nprint(\"feature, RSS Value, Weight\")\nfor i in range(7):\n    temp_x = x[:,i].copy()\n    temp_x = temp_x.T\n    xTx = np.matmul(temp_x.T, temp_x)\n    xTx_inv = 1/xTx\n    weight = xTx_inv*np.matmul(temp_x.T,y)\n    y_hat = weight*temp_x\n    print(i, np.sum((y-y_hat)**2), weight)\n\nfeature, RSS Value, Weight\n0 1.7680480306099482 0.5005626828719334\n1 1.5559485515856177 0.5004526294944694\n2 0.34805009659969083 0.9066846986089646\n3 0.3239590689664171 0.885160347571954\n4 0.10393465264034997 0.9160504114154776\n5 0.6325514156337191 0.7849334492064217\n6 0.6943223421821415 0.8177314873618696\n\n\nAs we can see the feature 5 has the least RSS, so we select feature 5 as the first feature and can go for further selection.\n\nx_5 = x[:,4].copy()\nprint(\"feature, RSS Value, Weight Vector\")\nfor i in range(7):\n    if i == 4:\n        continue\n    temp_x = x[:,i].copy()\n    temp_x = np.array([x_5,temp_x])\n    temp_x = temp_x.T\n    xTx = np.matmul(temp_x.T, temp_x)\n    xTx_inv = np.linalg.inv(xTx)\n    weights = np.matmul(np.matmul(xTx_inv, temp_x.T), y)\n    y_hat = np.matmul(temp_x, weights)\n    print(i, np.sum((y-y_hat)**2), weights)\n\nfeature, RSS Value, Weight Vector\n0 0.08939491221949343 [0.86424766 0.06320073]\n1 0.10375136110003312 [ 0.92379547 -0.00764925]\n2 0.06418266693959254 [0.68199387 0.26275088]\n3 0.09441954725164416 [0.76785156 0.15649981]\n5 0.10191595411157069 [0.86769165 0.05015582]\n6 0.07826337555928795 [0.77953228 0.15717367]\n\n\nHere, the feature 3 in combination with feature 5 since it has the least RSS value. Also we see that the weight value related to feature 5 has changed from 0.916 to 0.767\n\n\nb. Backward Stepwise\n\ndef get_rss(x):\n    xTx = np.matmul(x.T, x)\n    xTx_inv = np.linalg.inv(xTx)\n    weights = np.matmul(np.matmul(xTx_inv, x.T), y)\n    y_hat = np.matmul(x, weights)\n    rss = np.sum((y-y_hat)**2)\n    return rss\n\n\ni = 6\nfeatures = [0,1,2,3,4,5,6]\nwhile i != 1:\n    comb = list(itertools.combinations(features, i))\n    rss = []\n    for j in comb:\n        temp_x = x.copy()\n        temp_x = temp_x.T\n        temp_x = temp_x[list(j)].T\n        rss.append(get_rss(temp_x))\n    rss = np.array(rss)\n    features = list(comb[rss.argmin()])\n    i -= 1\nprint(\"Features Index\",features)\n    \n\n6.775622303276243e-05\n6.85232574647189e-05\n7.037563225111191e-05\n7.54452176001615e-05\n0.000110076377459281\nFeatures Index [1, 3]\n\n\nIn backward stepwise selection, we got the feature 2 and 4 as the best selection.\n\n\nc. Forward Stagewise\nFrom Forward Stepwise we know that the first feature is feature 5, now we need to find 2nd feature using stagewise increment\n\nx_5 = x[:,4].copy()\nweight_5 = 0.9160504114154776\nprint(\"feature, RSS Value, Weight\")\nfor i in range(7):\n    if i == 4:\n        continue\n    temp_x = x[:,i].copy()\n    temp_x = temp_x.T\n    xTx = np.matmul(temp_x.T, temp_x)\n    xTx_inv = 1/xTx\n    weight = xTx_inv*np.matmul(temp_x.T,y)\n    temp_x = np.array([x_5,temp_x]).T\n    weights = np.array([weight_5, weight])\n    y_hat = np.matmul(temp_x, weights)\n    print(i, np.sum((y-y_hat)**2), weight)\n    \n\nfeature, RSS Value, Weight\n0 1.4321712093629684 0.5005626828719334\n1 1.8985698176821504 0.5004526294944694\n2 2.808137102726856 0.9066846986089646\n3 2.998941025794095 0.885160347571954\n5 2.7347983916718257 0.7849334492064217\n6 2.469091091780378 0.8177314873618696\n\n\nAs we can see the RSS is least for feature 1.\nHence the two features which get selected are Festure 1 and 5\n\n\nd. Bestsubset Selection\n\nfeatures = [0,1,2,3,4,5,6]\ncomb = list(itertools.combinations(features, 2))\nrss = []\nfor c in comb:\n    temp_x = x.copy()\n    temp_x = temp_x.T\n    temp_x = temp_x[list(c)].T\n    rss.append(get_rss(temp_x))\nrss = np.array(rss)\ncomb[rss.argmin()]\n\n(0, 3)\n\n\nFeatures 1 and 4 are the features having the least RSS value in all possible \\(7\\choose2\\) combinations for features\n\n\n\n5. Explain all your observations based on the results.\nWe see that the co-effectients for Linear Regression and Ridge Regression, doesn’t change much and all the features are being used as the co-effecients for all the seven features are non-zero values.\nBut when we use Lasso regression, we can see that features \\(x_1\\), \\(x_2\\) are dominant, and there is little influence of feature \\(x_3\\), but features 4,5,6 and 7 are having co-effecients 0. Here this is a feature selection and shrinkage methond, where we only need three features to predict the results\nSimilarly when we performed subset selection different methods yielded different features,\nForward Stepwise selection, is you select the first feature with least RSS which gives feature 5, and than pair with all other features. We find the new \\(\\beta's\\) for each pair and see which gives least RSS. We found that Feature 3 and 5 is selected.\nIn Backward Stepwise selection, we start with all features, and kept on removing feature which harmed the RSS. We got features 2 and 4\nWhile, in Forward Stagewise, we don’t re train the \\(\\beta\\) after getting the \\(\\beta\\) for first selection of feature 5. We got feature 5 and 1. We also see that Forward Stepwise selection have features selection which gives lower RSS compared which showed that Forward Stepwise is better than Forward Stagewise, but the stagewise selection is computationally better than stepwise, since we don’t train the first selected parameter.\nFinally, In subset selection we formed all combinations of features, where we had \\(7\\choose2\\) combinations and calculated RSS for each of the pair and selected the combination with the least RSS. This is computational heavy but gives the best two features. Features 1 amd 4 are selected to best features. This selection method is least computationally effective solution, as it has to find parameters for all combinations."
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_Lab3-4.html",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_Lab3-4.html",
    "title": "Plaksha",
    "section": "",
    "text": "Think about standardizing the data.\nHow would you replace discrete attributes\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport stat\nimport math\n\n\nurl = \"https://drive.google.com/uc?id=1sEtgeWi0lZHFEBvuTi1DcoeqF7TZ-iE1\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      area\n      land\n      year\n      price\n      bldtype\n    \n  \n  \n    \n      0\n      2607\n      1200\n      2010\n      825000.0\n      0\n    \n    \n      1\n      1950\n      1783\n      1899\n      1685000.0\n      0\n    \n    \n      2\n      2520\n      1875\n      1899\n      1100000.0\n      0\n    \n    \n      3\n      3750\n      3125\n      1931\n      1200000.0\n      1\n    \n    \n      4\n      7812\n      5021\n      1908\n      1900000.0\n      1\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nY = df['bldtype']\nX = df.drop(['bldtype'], axis = 1)\n\n\n\n\n\nplt.figure(figsize=(20,10))\nsns.heatmap(X.corr(), cmap=\"YlGnBu\", annot=True)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7faffafb1ca0>\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\ndf = pd.get_dummies(df, columns=[\"year\"])\nY = df['bldtype'].to_numpy()\nX = df.drop(['bldtype'], axis = 1).to_numpy()\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y,train_size=0.8, random_state=42)\n\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nXtrain[:,:3] = sc.fit_transform(Xtrain[:,:3])\nXtest[:,:3] = sc.transform(Xtest[:,:3])\n\n\n\n\n\nTry with initial value of C=1\n\n\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', C = 1) \nclassifier.fit(Xtrain, Ytrain)\nYpred = classifier.predict(Xtest)\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncm = confusion_matrix(Ytest, Ypred)\nprint(cm)\nprint(accuracy_score(Ytest, Ypred)*100)\nprint(classification_report(Ytest,Ypred))\n\n[[13  0]\n [ 0  6]]\n100.0\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      1.00      1.00         6\n\n    accuracy                           1.00        19\n   macro avg       1.00      1.00      1.00        19\nweighted avg       1.00      1.00      1.00        19\n\n\n\n\n\n\n\nCan do it manually or use GridSearchCV\nDivide the training set into train+validation\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nparam_grid = {'C': list(np.logspace(-5,5, num=100)),'kernel':['linear']}  \n\ngridSVM = GridSearchCV(SVC(), param_grid, return_train_score=False, cv=5) \n   \n# fitting the model for grid search \ngridSVM.fit(Xtrain, Ytrain) \n\n# print best parameter after tuning \nprint(gridSVM.best_params_) \nYpred = gridSVM.predict(Xtest) \nprint(accuracy_score(Ytest,Ypred)*100)\n\n{'C': 2.848035868435799, 'kernel': 'linear'}\n100.0\n\n\n\ndf = pd.DataFrame({\"GroundTruth\": Ytest, \"Predict\": Ypred})\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      GroundTruth\n      Predict\n    \n  \n  \n    \n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n    \n    \n      2\n      0\n      0\n    \n    \n      3\n      0\n      0\n    \n    \n      4\n      0\n      0\n    \n    \n      5\n      0\n      0\n    \n    \n      6\n      0\n      0\n    \n    \n      7\n      1\n      1\n    \n    \n      8\n      0\n      0\n    \n    \n      9\n      1\n      1\n    \n    \n      10\n      0\n      0\n    \n    \n      11\n      1\n      1\n    \n    \n      12\n      1\n      1\n    \n    \n      13\n      1\n      1\n    \n    \n      14\n      0\n      0\n    \n    \n      15\n      1\n      1\n    \n    \n      16\n      0\n      0\n    \n    \n      17\n      0\n      0\n    \n    \n      18\n      0\n      0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\nWe see that we acheived 100% accuracy in the first attempt with C=1 and Linear Kernel. Even using GridSearch CV to find the hyperparameters gave same test accuracy"
  },
  {
    "objectID": "07_MachineLearning_I/Syed_AbdulKhader_Lab3-4.html#q2.-dt-and-rf",
    "href": "07_MachineLearning_I/Syed_AbdulKhader_Lab3-4.html#q2.-dt-and-rf",
    "title": "Plaksha",
    "section": "Q2. DT and RF",
    "text": "Q2. DT and RF\nConsider the Wisconsin Breast Cancer dataset available from http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+ (Diagnostic)\nThe dataset has 32 attributes that predict malignancy. There are a total of 569 data patterns. Use 5-fold cross-validation.\n\nUse Keras or any other framework to construct a decision tree from the training data and obtain the performance on the test data\nConstruct a random forest (of say, 100 trees) from the training data and use the random forest to obtain the performance on the test data\nCompare the performance you obtain in 1 and 2\n\n\nHEADER = ['id','diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean',\n          'concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se','texture_se',\n          'perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se',\n          'fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst',\n          'compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']\n\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', names=HEADER)\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      diagnosis\n      radius_mean\n      texture_mean\n      perimeter_mean\n      area_mean\n      smoothness_mean\n      compactness_mean\n      concavity_mean\n      concave points_mean\n      ...\n      radius_worst\n      texture_worst\n      perimeter_worst\n      area_worst\n      smoothness_worst\n      compactness_worst\n      concavity_worst\n      concave points_worst\n      symmetry_worst\n      fractal_dimension_worst\n    \n  \n  \n    \n      0\n      842302\n      M\n      17.99\n      10.38\n      122.80\n      1001.0\n      0.11840\n      0.27760\n      0.3001\n      0.14710\n      ...\n      25.38\n      17.33\n      184.60\n      2019.0\n      0.1622\n      0.6656\n      0.7119\n      0.2654\n      0.4601\n      0.11890\n    \n    \n      1\n      842517\n      M\n      20.57\n      17.77\n      132.90\n      1326.0\n      0.08474\n      0.07864\n      0.0869\n      0.07017\n      ...\n      24.99\n      23.41\n      158.80\n      1956.0\n      0.1238\n      0.1866\n      0.2416\n      0.1860\n      0.2750\n      0.08902\n    \n    \n      2\n      84300903\n      M\n      19.69\n      21.25\n      130.00\n      1203.0\n      0.10960\n      0.15990\n      0.1974\n      0.12790\n      ...\n      23.57\n      25.53\n      152.50\n      1709.0\n      0.1444\n      0.4245\n      0.4504\n      0.2430\n      0.3613\n      0.08758\n    \n    \n      3\n      84348301\n      M\n      11.42\n      20.38\n      77.58\n      386.1\n      0.14250\n      0.28390\n      0.2414\n      0.10520\n      ...\n      14.91\n      26.50\n      98.87\n      567.7\n      0.2098\n      0.8663\n      0.6869\n      0.2575\n      0.6638\n      0.17300\n    \n    \n      4\n      84358402\n      M\n      20.29\n      14.34\n      135.10\n      1297.0\n      0.10030\n      0.13280\n      0.1980\n      0.10430\n      ...\n      22.54\n      16.67\n      152.20\n      1575.0\n      0.1374\n      0.2050\n      0.4000\n      0.1625\n      0.2364\n      0.07678\n    \n  \n\n5 rows × 32 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nXc = df.drop(labels=['id', 'diagnosis'], axis=1)\nYc = df['diagnosis']\n\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, Ytrain, Ytest = train_test_split(Xc, Yc, test_size = 0.2, random_state = 42)\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n# Practically, we would limit the size of Tree. But the data is small, so going\n# untill we find pure leafs, or leafs with less than minimum number of samples.\n\n# Since we don't have any hyper parameters, no Grid Search is being performed.\n\n# To get better accuracy we will average the accuracy over 5 random states\naccuracy_sum = 0\nfor i in [42,7,99,111,0]:\n    classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = i) \n    classifier.fit(Xtrain, Ytrain)\n    y_pred = classifier.predict(Xtest)\n    accuracy_sum += accuracy_score(Ytest.values,y_pred)\n    print(f\"Accuracy for Random State {i}: {accuracy_score(Ytest.values,y_pred)*100}%\")\nprint(\"\\nAverage Accuracy: \", accuracy_sum/5 * 100)\n\nAccuracy for Random State 42: 94.73684210526315%\nAccuracy for Random State 7: 94.73684210526315%\nAccuracy for Random State 99: 94.73684210526315%\nAccuracy for Random State 111: 94.73684210526315%\nAccuracy for Random State 0: 95.6140350877193%\n\nAverage Accuracy:  94.91228070175438\n\n\nRandom Trees\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifierRF = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42) \nclassifierRF.fit(Xtrain, Ytrain)\ny_pred = classifierRF.predict(Xtest)\n\nprint(\"Accuracy Score: \",accuracy_score(Ytest.values,y_pred)*100)\n\nAccuracy Score:  96.49122807017544\n\n\n\nparam_grid = {'n_estimators': [i for i in range(50,150)]}  \n   \ngridRF = GridSearchCV(RandomForestClassifier(criterion='entropy', random_state= 42), param_grid, return_train_score=False, cv=5) \n   \n# fitting the model for grid search \ngridRF.fit(Xtrain, Ytrain) \n\n\n# print best parameter after tuning \nprint(\"The Best Number of Estimator based on CV Search is: \",gridRF.best_params_['n_estimators']) \nYpred = gridRF.predict(Xtest)\nprint(f\"The Final Test Accuracy with using {gridRF.best_params_['n_estimators']} estimators is {accuracy_score(Ytest.values,Ypred)*100}%\")\n\nThe Best Number of Estimator based on CV Search is:  91\nThe Final Test Accuracy with using 91 estimators is 96.49122807017544%\n\n\n\nRepeat the exercise but add ±10% noise to 25% of the data (Optional)\n\n\nBoosting\nImplement a boosting classifier algorithm for the same dataset as above (sample without noise)\nFeel free to use any boosting algorithm you want\nHowever only run the code for the eventual algorithm you choose and comment out every other algorithm\nBriefly explain why you chose a particular algorithm\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nab_classifier = AdaBoostClassifier(n_estimators=100, learning_rate=1)\nYpred = ab_classifier.fit(Xtrain,Ytrain).predict(Xtest)\n\nprint(\"AdaBoosted Accuracy: \", accuracy_score(Ytest, Ypred)*100)\n\nAdaBoosted Accuracy:  97.36842105263158\n\n\n\n\nUsing different base learners\n\n# Base Learner - Vanilla Decision Tree\nab_classifier = AdaBoostClassifier(n_estimators=100, base_estimator=classifier, learning_rate=1)\nYpred = ab_classifier.fit(Xtrain,Ytrain).predict(Xtest)\n\nprint(\"Using Decision Tree Classifier as Base Learner for Boosting, Accuracy: \", accuracy_score(Ytest, Ypred)*100)\n\nUsing Decision Tree Classifier as Base Learner for Boosting, Accuracy:  94.73684210526315\n\n\n\n# Base Learner - Random Forest\nab_classifier = AdaBoostClassifier(n_estimators=100, base_estimator=classifierRF, learning_rate=1)\nYpred = ab_classifier.fit(Xtrain,Ytrain).predict(Xtest)\n\nprint(\"Using Decision Tree Classifier as Base Learner for Boosting, Accuracy: \", accuracy_score(Ytest, Ypred)*100)\n\nUsing Decision Tree Classifier as Base Learner for Boosting, Accuracy:  96.49122807017544\n\n\n\nAdaBoost gave the better results, due to it’s focus on giving more weights to the data points which are mis-classified in the initial stages and increasing it’s probability. Also in Ada Boost when using trees, we never go for the feature with highest information gain, rather we have a subset of which we select the best, focusing more on generalization rather than just overfitting on training data.\n\n\n\nBagging\nImplement a bagging classifier on the RF you created above\n\nfrom sklearn.ensemble import BaggingClassifier\n\nYou will have to pass the DT into the Bagging Classifier\nOnce you have the y_pred for Bagging and RF, accurately compute the accuracy by computing the numpy sum where pred(bagging) == pred(RF) and divide by len(pred(bagging))\nPlease provide rationale behind why this is done.\n\nfrom sklearn.ensemble import BaggingClassifier\n\nclassifierBag = BaggingClassifier(base_estimator=classifierRF, n_estimators=100)\nclassifierBag.fit(Xtrain,Ytrain)\nbag_pred = classifierBag.predict(Xtest)\nprint(\"Accuracy of Bagging Classifier with Base Estimator as Decision Tree: \", accuracy_score(Ytest,bag_pred)*100)\n\nAccuracy of Bagging Classifier with Base Estimator as Decision Tree:  96.49122807017544\n\n\n\ndt_result = classifier.predict(Xtest)\nbagging_result = classifierBag.predict(Xtest)\nmatching = np.sum(dt_result == bagging_result)\nprint(\"Number of Prediction which both the models predicted same: \", matching)\nprint(\"Matching Percentage: \", matching/len(dt_result)*100)\n\nNumber of Prediction which both the models predicted same:  113\nMatching Percentage:  99.12280701754386\n\n\n\nWe are doing this to see how many data points more did the bagging was able to classify differently from vanilla decision tree. Since Bagging technique involved repeatedly executing the algortihm with different set of the dataset, it has more robustness.\nBonus : While you are looking at ensemble models, explore VotingClassifier\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nestimator = []\nestimator.append(('LR', LogisticRegression(solver ='lbfgs', max_iter = 10_000)))\nestimator.append(('SVC', SVC(gamma ='auto', probability = True)))\nestimator.append(('DTC', DecisionTreeClassifier()))\n\nclassifiedHard = VotingClassifier(estimators = estimator, voting ='hard')\nYpred = classifiedHard.fit(Xtrain, Ytrain).predict(Xtest)\n\nprint(\"Accuracy of Ensemble Model with Hard Voting: \", accuracy_score(Ytest,Ypred)*100)\n\nAccuracy of Ensemble Model with Hard Voting:  96.49122807017544\n\n\nIn hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers.\nIn soft voting, the output class is the prediction based on the average of probability given to that class.\n\nclassifiedSoft = VotingClassifier(estimators = estimator, voting ='soft')\nYpred = classifiedSoft.fit(Xtrain, Ytrain).predict(Xtest)\n\nprint(\"Accuracy of Ensemble Model with Hard Voting: \", accuracy_score(Ytest,Ypred)*100)\n\nAccuracy of Ensemble Model with Hard Voting:  97.36842105263158"
  },
  {
    "objectID": "07_MachineLearning_I/KaggleCompetition.html",
    "href": "07_MachineLearning_I/KaggleCompetition.html",
    "title": "Plaksha",
    "section": "",
    "text": "Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification. The objective of this notebook is one were we try to apply various Machine Learning akgortihms on the Monto-Carlo simualted data, to try to classify the data as either signal or background\nThe Data is named SUSY data, which is available at the UCI Machine Learning repository Link."
  },
  {
    "objectID": "07_MachineLearning_I/KaggleCompetition.html#models",
    "href": "07_MachineLearning_I/KaggleCompetition.html#models",
    "title": "Plaksha",
    "section": "Models",
    "text": "Models\n\nLogistic Regression\n\n# parameters = dict(max_iter = 1000, n_jobs=-1)\n# model = train_model(LogisticRegression, **parameters)\n\nTraining Acc: 0.7884857142857142\nVal Acc: 0.7877085714285714\nWe now have a baseline of 78.77% validation accuracy.\nLogistic Regression is a linear model, which tries to fit the model of seperating the class by a hyper-plane. If the data points is not linearly seprable we have the error, which is evident with error of 0.2123\n\n\nSupport Vector Classifier\nSVMs scale very poorly with data, here we are having 3.5M data points and even after split we have 2.8M data points. In SVM during solving the dual problem the optimizer evaluates 2.8M langrange variables, out of which many will be zero’s and the non-zero are the support vectors. But this will take a lot of time and cross the 1 hour constraint.\n\n\nDecision Tree\nIt was than decided to use a vanilla decision tree, so as to create a simple rule based system derived from the data. This was done primarily to understand the feature dependency on the output. Since the single decision tree is very good to model interpretability.\nA more complex problem will reduce interpretability and explainability of the data, and Decision Tree is a good approach to start with to get the data features influence. This may not be the best solution, but a good start to understand.\n\n# parameters = dict(criterion='entropy', max_depth=8)\n# model = train_model(DecisionTreeClassifier, **parameters)\n\nTraining Acc: 0.7852380952380953\nVal Acc: 0.7816939209726443\n\nHyper Parameter Tuning (using Optuna)\n\nparameters = dict(\n    criterion = ('cat', {'choices':['gini', 'entropy']}),\n    max_depth = ('int', dict(low=2, high=32,log=True)),\n    splitter = ('cat', {'choices':[\"best\", \"random\"]}),\n    max_features = ('cat', {'choices':[None, \"sqrt\", \"log2\"]}),\n    ccp_alpha = ('float', dict(low=0, high=0.02, step=0.005)),\n)\n\n# best_params = parameter_tuning(DecisionTreeClassifier, parameters, trials = 100)\n\nA new study created in memory with name: no-name-ccce6503-93f1-4e69-bcfb-93504aff439\nTrial 0 finished with value: 0.7501951367781156 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 6, ‘splitter’: ‘best’, ‘max_features’: ‘sqrt’, ‘ccp_alpha’: 0.01}. Best is trial 0 with value: 0.7501951367781156\nTrial 1 finished with value: 0.6498364741641337 and parameters: {‘criterion’: ‘entropy’, ‘max_depth’: 2, ‘splitter’: ‘random’, ‘max_features’: ‘log2’, ‘ccp_alpha’: 0.01}. Best is trial 0 with value: 0.7501951367781156\nTrial 7 finished with value: 0.6267954407294832 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 19, ‘splitter’: ‘random’, ‘max_features’: ‘sqrt’, ‘ccp_alpha’: 0.005}. Best is trial 0 with value: 0.7501951367781156\nTrial 8 finished with value: 0.7566920972644376 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 18, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 8 with value: 0.7566920972644376\nTrial 11 finished with value: 0.7481306990881459 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 14, ‘splitter’: ‘best’, ‘max_features’: ‘sqrt’, ‘ccp_alpha’: 0.02}. Best is trial 8 with value: 0.7566920972644376\nTrial 12 finished with value: 0.7584814589665654 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 5, ‘splitter’: ‘best’, ‘max_features’: ‘sqrt’, ‘ccp_alpha’: 0.0}. Best is trial 12 with value: 0.7584814589665654\nTrial 13 finished with value: 0.7581580547112462 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 4, ‘splitter’: ‘best’, ‘max_features’: ‘sqrt’, ‘ccp_alpha’: 0.0}. Best is trial 12 with value: 0.7584814589665654\nTrial 22 finished with value: 0.7831367781155015 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 11, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 22 with value: 0.7831367781155015\nTrial 23 finished with value: 0.7501951367781156 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 11, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.005}. Best is trial 22 with value: 0.7831367781155015\nTrial 42 finished with value: 0.7814951367781156 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 7, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 22 with value: 0.7831367781155015\nTrial 43 finished with value: 0.7850443768996961 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 10, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 43 with value: 0.7850443768996961\nTrial 44 finished with value: 0.7794395136778115 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 12, ‘splitter’: ‘best’, ‘max_features’: ‘log2’, ‘ccp_alpha’: 0.0}. Best is trial 43 with value: 0.7850443768996961\nTrial 49 finished with value: 0.7845990881458966 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 9, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 43 with value: 0.7850443768996961\nTrial 50 finished with value: 0.7850945288753799 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 10, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 50 with value: 0.7850945288753799\nTrial 51 finished with value: 0.7850145896656535 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 10, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 50 with value: 0.7850945288753799\nTrial 52 finished with value: 0.7850823708206687 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 10, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 50 with value: 0.7850945288753799\nTrial 68 finished with value: 0.7501951367781156 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 12, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.005}. Best is trial 50 with value: 0.7850945288753799\nTrial 69 finished with value: 0.7792410334346505 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 9, ‘splitter’: ‘best’, ‘max_features’: ‘sqrt’, ‘ccp_alpha’: 0.0}. Best is trial 50 with value: 0.7850945288753799\nTrial 70 finished with value: 0.7805501519756839 and parameters: {‘criterion’: ‘entropy’, ‘max_depth’: 14, ‘splitter’: ‘random’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 50 with value: 0.7850945288753799\nTrial 71 finished with value: 0.7851279635258359 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 10, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 71 with value: 0.7851279635258359\nTrial 72 finished with value: 0.7850103343465046 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 10, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 71 with value: 0.7851279635258359\nTrial 73 finished with value: 0.7831857142857143 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 11, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 71 with value: 0.7851279635258359\nTrial 98 finished with value: 0.7845826747720365 and parameters: {‘criterion’: ‘gini’, ‘max_depth’: 9, ‘splitter’: ‘best’, ‘max_features’: None, ‘ccp_alpha’: 0.0}. Best is trial 71 with value: 0.7851279635258359\nTrial 99 finished with value: 0.730756838905775 and parameters: {‘criterion’: ‘entropy’, ‘max_depth’: 13, ‘splitter’: ‘random’, ‘max_features’: ‘sqrt’, ‘ccp_alpha’: 0.0}. Best is trial 71 with value: 0.7851279635258359\nBest trial:\n  Value:  0.7851279635258359  \n  Params:    \n    * criterion: gini  \n    * max_depth: 10  \n    * splitter: best  \n    * max_features: None  \n    * ccp_alpha: 0.0  \n\n\nAfter Hyper Parameter Tuning, Training with New Hyperparameters and Evaluating\n\n# model = DecisionTreeClassifier(**best_params)\n# model.fit(Xtrain, Ytrain)\n# print(\"Test Acc: \",model.score(Xtest, YFtest))\n\nTest Acc: 0.7853406666666667\n\n\n\nRandom Forest\nNow once we had a Decision Tree, now we can ensemble multiple trees to form a Random Forest. We do this using bagging technique, where the data points are sampled differently for different trees. At the end we get a collection(forest) of trees.\nThis is done so that we can reduce the variance of the single decisoin tree. This is done using Bagging\n\n# parameters = dict(n_estimators=100, criterion='gini', max_depth = 10, n_jobs=-1)\n# model = train_model(RandomForestClassifier, **parameters)\n\nTraining Acc: 0.8043047619047619\nVal Acc: 0.7947720364741642\n\nparameters = dict(\n    n_estimators = ('int', dict(low=16, high=512, log=True)),\n    max_features = ('float', dict(low=0.15, high=1.0)),\n    min_samples_split = ('int', dict(low=2, high=14)),\n    min_samples_leaf = ('int', dict(low=1, high=14)),\n    max_samples = ('float', dict(low=0.6, high=0.99)),\n)\n\n# best_params = parameter_tuning(RandomForestClassifier, parameters, trials = 30)\n\nA new study created in memory with name: no-name-23d7a54f-1a5b-473e-b1b7-05eaae63c4c\nTrial 7 finished with value: 0.7980237082066869 and parameters: {‘n_estimators’: 47, ‘max_features’: 0.4596051497513195, ‘min_samples_split’: 14, ‘min_samples_leaf’: 2, ‘max_samples’: 0.6523800590566861}. Best is trial 7 with value: 0.7980237082066869\nTrial 3 finished with value: 0.7982547112462006 and parameters: {‘n_estimators’: 87, ‘max_features’: 0.2981610544019258, ‘min_samples_split’: 3, ‘min_samples_leaf’: 3, ‘max_samples’: 0.986226411599963}. Best is trial 3 with value: 0.7982547112462006\nTrial 9 finished with value: 0.7961930091185411 and parameters: {‘n_estimators’: 23, ‘max_features’: 0.5739882381756989, ‘min_samples_split’: 8, ‘min_samples_leaf’: 6, ‘max_samples’: 0.6803133252458505}. Best is trial 3 with value: 0.7982547112462006\nTrial 4 finished with value: 0.7982899696048632 and parameters: {‘n_estimators’: 70, ‘max_features’: 0.9466019600183457, ‘min_samples_split’: 14, ‘min_samples_leaf’: 6, ‘max_samples’: 0.8431696250534466}. Best is trial 4 with value: 0.7982899696048632\nTrial 2 finished with value: 0.7992127659574468 and parameters: {‘n_estimators’: 128, ‘max_features’: 0.6205302240776256, ‘min_samples_split’: 10, ‘min_samples_leaf’: 3, ‘max_samples’: 0.7576130035460769}. Best is trial 2 with value: 0.7992127659574468\nTrial 6 finished with value: 0.7999793313069908 and parameters: {‘n_estimators’: 189, ‘max_features’: 0.48541270809179304, ‘min_samples_split’: 14, ‘min_samples_leaf’: 8, ‘max_samples’: 0.6770325683419379}. Best is trial 6 with value: 0.7999793313069908\nTrial 12 finished with value: 0.797529179331307 and parameters: {‘n_estimators’: 39, ‘max_features’: 0.36246416423360384, ‘min_samples_split’: 13, ‘min_samples_leaf’: 3, ‘max_samples’: 0.8976190443329654}. Best is trial 6 with value: 0.7999793313069908\nTrial 13 finished with value: 0.7924936170212766 and parameters: {‘n_estimators’: 20, ‘max_features’: 0.9820341311824934, ‘min_samples_split’: 5, ‘min_samples_leaf’: 3, ‘max_samples’: 0.7134874049550151}. Best is trial 6 with value: 0.7999793313069908\nTrial 8 finished with value: 0.7999866261398176 and parameters: {‘n_estimators’: 171, ‘max_features’: 0.5787223768791657, ‘min_samples_split’: 9, ‘min_samples_leaf’: 12, ‘max_samples’: 0.6698039063759819}. Best is trial 8 with value: 0.7999866261398176\nTrial 21 finished with value: 0.798937386018237 and parameters: {‘n_estimators’: 478, ‘max_features’: 0.15782017114552138, ‘min_samples_split’: 11, ‘min_samples_leaf’: 12, ‘max_samples’: 0.6056404685945235}. Best is trial 8 with value: 0.7999866261398176\nTrial 10 finished with value: 0.8000282674772037 and parameters: {‘n_estimators’: 333, ‘max_features’: 0.7400302635544946, ‘min_samples_split’: 13, ‘min_samples_leaf’: 5, ‘max_samples’: 0.6199951088017647}. Best is trial 10 with value: 0.8000282674772037\nTrial 17 finished with value: 0.8002747720364741 and parameters: {‘n_estimators’: 399, ‘max_features’: 0.7500853374325903, ‘min_samples_split’: 10, ‘min_samples_leaf’: 13, ‘max_samples’: 0.61439371842897}. Best is trial 17 with value: 0.8002747720364741\nTrial 18 finished with value: 0.8002969604863222 and parameters: {‘n_estimators’: 494, ‘max_features’: 0.7042917092544888, ‘min_samples_split’: 11, ‘min_samples_leaf’: 12, ‘max_samples’: 0.6043806165694288}. Best is trial 18 with value: 0.8002969604863222\nTrial 19 finished with value: 0.8003197568389058 and parameters: {‘n_estimators’: 422, ‘max_features’: 0.7272347622230614, ‘min_samples_split’: 11, ‘min_samples_leaf’: 12, ‘max_samples’: 0.6258827581609802}. Best is trial 19 with value: 0.8003197568389058\nTrial 28 finished with value: 0.8002668693009118 and parameters: {‘n_estimators’: 401, ‘max_features’: 0.8224379443199038, ‘min_samples_split’: 12, ‘min_samples_leaf’: 14, ‘max_samples’: 0.630348068431515}. Best is trial 19 with value: 0.8003197568389058\nTrial 29 finished with value: 0.8001954407294832 and parameters: {‘n_estimators’: 380, ‘max_features’: 0.8484879439317321, ‘min_samples_split’: 11, ‘min_samples_leaf’: 10, ‘max_samples’: 0.6356978721622217}. Best is trial 19 with value: 0.8003197568389058\nBest trial:\n  Value:  0.8003197568389058\n  Params: \n    n_estimators: 422\n    max_features: 0.7272347622230614\n    min_samples_split: 11\n    min_samples_leaf: 12\n    max_samples: 0.6258827581609802\n\n# model = RandomForestClassifier(**best_params)\n# model.fit(Xtrain, Ytrain)\n# print(\"Test Acc: \",model.score(XFinal, YFinal))\n\nTest Acc: 0.8007433333333334\n\n\neXtreme Gradient Boosting\nAt the end, after Random Forest, to have a more robust model which accounts for mis-classification of the points, Boosting was preferred. In boosting, we increaase the probability of the data point being selected if the data point was mis-classified in the previous iteration. We go on perform the iteration repeatedly until we have a robust model.\nThere were various option of AdaBoost, GradientBoost, HistGradientBoost, LightGBM and eXtremeGradientBoost. AdaBoost is limited to only stumps, which is depth of 1 tree with only 2 leafes to a node.\nAll other boosting techniques are realted to Gradient, and the eXtremeGradientBoostin method, uses an Additive Tree learning method ,where we know it is intractable to enumerate all possible tree structures, so we keep on adding one split at a time.\n\n# parameters = dict(n_jobs = -1)\n# model = train_model(XGBClassifier, **parameters)\n\nTraining Acc: 0.8047453571428571\nVal Acc: 0.8017314285714285\nOptuna was used to do hyper parameter testing, but a GPU was used since XGBoost has GPU compatibility. TheGPU helped in running the various combinartions of parameters quickly. The metric for tuning was Area Under Curve rather than error or accuracy. This was done to increase the model robustness. But once we got the hyper parameters the metric was than kept back as error since that is the end objective.\nThe objective parameter is binary-logistic, since it is a bianry problem which is classification. All the other hyper parameters were given a wide range of search space and the best was selected from that space.\n\nHyper Parameter Tuning\n param = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        # sampling ratio for training data.\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        # sampling according to each tree.\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.2, 1.0),\n    }\n\n    param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 15)\n    param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n    param[\"max_delta_step\"] = trial.suggest_int(\"max_delta_step\", 0, 20)\n    param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n    param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-9, 0.5, log=True)\n    param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    param[\"scale_pos_weight\"] = trial.suggest_categorical(\"scale_pos_weight\", [1.17])\n\n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-error\")\n    history = xgb.cv(param, dtrain, num_boost_round=100, nfold=2, callbacks=[pruning_callback],)\n    mean_error = history[\"test-error-mean\"].values[-1]\n\nparameters = {\n 'objective': 'binary:logistic',\n 'eval_metric': 'error',\n 'booster': 'gbtree',\n 'lambda': 0.5,\n 'alpha': 0.5,\n 'gamma': 0.05, #min_split_loss\n 'tree_method': \"hist\",\n 'learning_rate': 0.0271,\n 'max_depth': 10,\n 'min_child_weight': 13,\n 'max_delta_step': 17,\n 'n_estimators': 500,\n 'grow_policy': 'depthwise',\n 'num_parallel_tree': 4,\n 'subsample': 0.8,\n 'colsample_bylevel': 0.9,\n 'colsample_bytree': 0.45,\n#  'scale_pos_weight': 1.17,\n 'n_jobs' : -1}\n \nmodel = train_model(XGBClassifier, **parameters)\n\nTraining Acc:  0.8087035714285714\nVal Acc:  0.8081557142857143\n\n\nTraining Acc: 0.8102378571428571\nVal Acc: Test Acc: 0.8044013333333333\n\n\n\nCreating Files\n\nGenerating CSV\n\ndef gen_csv(model):\n    df_test = pd.read_csv(\"./test.csv\", index_col=0)\n    prediction = model.predict(df_test)\n    with open(\"./prediction.csv\", 'w') as f:\n        f.write(\"id,class\\n\")\n        for idx, i in enumerate(prediction):\n            f.write(f\"{idx},{float(i)}\\n\")\n\ngen_csv(model)\n\n\n\n\nFuture Work\n\nThe main short coming was domain knowledge, and how to come up with different derived features fromthe existing 8/18 features. The paper entails some mass invariance and momentum, which can be calculated with certain equation from our features.\nWeights and Biases was tried to keep the tracking of experiments. It would be helpful to conenct Optuna and Weights and Biases, so that the hyper parameter tuning and all the experiments, artifacts related to the experiments are stored."
  },
  {
    "objectID": "04_DataVisualization/index.html",
    "href": "04_DataVisualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data Visualiation on iHerb Mask Dataset"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html",
    "href": "04_DataVisualization/02_MaskEDA.html",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "",
    "text": "Code\nimport math\nimport random\nimport os\nimport re\nimport string\nfrom collections import Counter\n\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pygal\n\nimport nltk\nnltk.download(\"stopwords\")\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom PIL import Image\nfrom os import path\nfrom wordcloud import WordCloud\n\n\n\n\nCode\nreviews = pd.read_csv(\"./data/reviews.csv\")\nproducts = pd.read_csv(\"./data/products.csv\")"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html#which-are-the-most-popular-face-masks-out-there",
    "href": "04_DataVisualization/02_MaskEDA.html#which-are-the-most-popular-face-masks-out-there",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "Which are the most popular face masks out there?",
    "text": "Which are the most popular face masks out there?\n\nTop 3 Products Overall\n\nLozperi, Copper Mask, Adult, Dot, 1 Count\nHwipure, Disposable KF94 ( N95 / KN95/ FFP2 ) Mask, 25 Masks\nLozperi, Copper Mask, Kids, Gray, 1 Count\n\n\n\nTop Product in each Category\n\nOthers - Lozperi, Copper Mask, Adult, Dot, 1 Count\nDisposable - Hwipure, Disposable KF94 ( N95 / KN95/ FFP2 ) Mask, 25 Masks\nReusable - Kosette, Nano Reusable Face Protection Mask, Medium, 1 Mask"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html#what-do-consumers-like-about-them-why",
    "href": "04_DataVisualization/02_MaskEDA.html#what-do-consumers-like-about-them-why",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "What do consumers like about them? Why?",
    "text": "What do consumers like about them? Why?\n\n\nCode\ndef getFreqDict(df: pd.DataFrame) -> dict:\n    stop_words = set(stopwords.words('english'))\n    stop_words = stop_words.union(set(['masks', 'mask', 'face']))\n\n    d = Counter()\n    for rev in df.review:\n            \n            # Removing punctions from string\n            rev = rev.translate(str.maketrans('', '', string.punctuation))\n            \n            # Removing stop words\n            word_tokens = word_tokenize(rev)\n            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n            \n            d.update(filtered_sentence)\n    \n    return d\n\n\n\n\nCode\ndef makeImage(freq_dict: dict) -> None:\n    \"\"\"\n    Generate Word Cloud of the given frequency dictionary\n    \"\"\"\n    wc = WordCloud(width=1200, height=1200, background_color=\"white\", max_words=20)\n    # generate word cloud\n    wc.generate_from_frequencies(freq_dict)\n\n    # show\n    plt.figure(figsize=(10,10))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\nCode\ndf_good = df[df.ratingValue>30]\ndf_bad = df[df.ratingValue<30]\n\n\n\nGood Rating Word Cloud\n\n\nCode\nfreq_dict = getFreqDict(df_good)\nmakeImage(freq_dict)\n\n\n\n\n\n\n\nBad Rating Word Cloud\n\n\nCode\nfreq_dict = getFreqDict(df_bad)\nmakeImage(freq_dict)\n\n\n\n\n\nSince it’s only one word, some words which may be positive are in bad reviews, like good (where as in negative it may be not good)\n\n\nBiGram Word Cloud\n\nWorld Cloud for Good Ratings with BiGrams\n\n\nCode\ndef getFreqDict(df: pd.DataFrame) -> dict:\n    stop_words = set(stopwords.words('english'))\n    stop_words = stop_words.union(set(['masks', 'mask', 'face']))\n\n    d = Counter()\n    for rev in df.review:\n            \n            # Removing punctions from string\n            rev = rev.translate(str.maketrans('', '', string.punctuation))\n            \n            # Removing stop words\n            word_tokens = word_tokenize(rev)\n            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n            bigram_seq = []\n            for i in range(len(filtered_sentence)-1):\n                bigram_seq.append(filtered_sentence[i]+\" \"+filtered_sentence[i+1])\n            \n            d.update(bigram_seq)\n    \n    return d\n\n\n\n\nWorld Cloud for Bad Ratings with BiGrams\n\n\nCode\nfreq_dict = getFreqDict(df_good)\nmakeImage(freq_dict)\n\n\n\n\n\n\n\nCode\nfreq_dict = getFreqDict(df_bad)\nmakeImage(freq_dict)\n\n\n\n\n\nIn BiGrams it’s more easy to see, which words are occuring more in Negative reviews and depicts negative sentiment"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html#what-different-profiles-of-consumers-buy-masks",
    "href": "04_DataVisualization/02_MaskEDA.html#what-different-profiles-of-consumers-buy-masks",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "What different profiles of consumers buy masks?",
    "text": "What different profiles of consumers buy masks?\n\nCustomer Profile based on Language Code\n\n\nCode\nlang = set()\ncountry = set()\nlang_iso = {'BR':\"BRA\", 'CN':\"CHN\", 'DE':\"DEU\", 'FR':\"FRA\", 'IL':\"ISR\", 'JP':\"JPN\", 'KR':\"KOR\", 'MX':\"MEX\", \n            'RU':\"RUS\", 'SA':\"SAU\", 'TW':\"TWN\", 'US':\"USA\"}\nfor i in df.languageCode.unique():\n    l,c = i.split(\"-\")\n    lang.add(l)\n    country.add(lang_iso[c])\n\n\n\n\nCode\nresult = df.groupby(['languageCode']).agg({'review': 'count'})\nresult.reset_index(inplace=True)\nresult[['lang','country']] = result['languageCode'].str.split('-',expand=True)\nresult['country'] = result['country'].map(lang_iso)\nresult.drop(['languageCode'], inplace=True, axis=1)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld = world.merge(result, how='left', left_on=\"iso_a3\", right_on=\"country\" )\n# world.review.fillna(0, inplace=True)\nworld['coords'] = world['geometry'].apply(lambda x: x.representative_point().coords[:])\nworld['coords'] = [coords[0] for coords in world['coords']]\n\n\n# plot confirmed cases world map \nax = world.plot(column='review', #scheme=\"quantiles\",\n           figsize=(25, 20),\n           legend=True,\n           cmap='coolwarm',\n           missing_kwds={\n        \"color\": \"lightgrey\",\n        \"edgecolor\": \"grey\",\n        # \"hatch\": \"///\",\n        \"label\": \"Missing values\",\n        },\n           legend_kwds={'orientation': \"horizontal\"})\n\n\nax.set_axis_off()\n\nplt.title('Number of Users based on Language',fontsize=25)\n\nfor idx, row in world.iterrows():\n   if not pd.isna(row.review):\n      text = f\"{row.lang}-{row.country} - {row.review}\" \n      plt.annotate(text=text, xy=row['coords'],\n                 horizontalalignment='center',\n                 fontsize=14)\n                \nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Plaksha",
    "section": "",
    "text": "Thie website contains all the content regarding TLP 2022-23\nMathematics I\nIntroduction to Programming\nIntroduction to Artificial Intelligence\nData Visualization\nMathematics II"
  },
  {
    "objectID": "06_Intro_DSA/LE2.html",
    "href": "06_Intro_DSA/LE2.html",
    "title": "Plaksha",
    "section": "",
    "text": "The algo1 implementation has two for loops. The first loop start with 0 index and all the way goes towards the end which is n-1 index, which in total is n iterations.\nThe second loop which is inside the first loop runs from 0 index to n-i-1 index where i is the first loop iteration variable. The if statement inside the second loop is constant.\nWhen i=0, the scond loop run for n-1 times, for i=1, the secnod loops runs for n-1 times and so on, for i=n-1, the second loop runs for 0 times.\nHence the total complexity = $ (n-1) + (n-2) + … + 0$\nThis is the sum of n-1 natural numbers, which is \\(\\frac{(n-1)(n)}{2} = \\frac{n^2 - n}{2}\\)\nIgnoring the constants and dropping the lower terms, asymptotically we have\n\\[\nTime Complexity =\nO(n^2)\n\\]"
  },
  {
    "objectID": "06_Intro_DSA/LE2.html#algorithm-2",
    "href": "06_Intro_DSA/LE2.html#algorithm-2",
    "title": "Plaksha",
    "section": "Algorithm 2",
    "text": "Algorithm 2\nThe algo2 implementation is a recursion, which has two branches for every call. For the input n, the algorithm calls itself with the input n-1 and n-2. This goes on until we reach the base case where n=1 or n=2.\n\\[\nT(n) = T(n-1) + T(n-2) + 1\n\\]\nWe assume \\(T(n-1) ~ T(n-2)\\), we have \\(T(n) = 2T(n-1) + 1\\).\n\\[T(n-1) = T(n-2)+T(n-3)+1 \\approx 2T(n-2) + 1\\] \\[T(n) = 2[2T(n-2) + 1] + 1 = 2^2T(n-2) + 2^2-1\\] \\[...\\] \\[T(n) = 2^kT(n-k) + 2^k-1\\]\nThe Base case occurs when n is either 0 ir 1, which is going down by 1 every oteration. Hence k is equal to n. Also We know at base case \\(T(0) = 1\\)\n\\[\\therefore k=n, T(n) = 2^nT(0) + 2^n-1 = 2^n + 2^n-1\\]\n\\[\nTime Complexity =\nO(2^n)\n\\]"
  },
  {
    "objectID": "06_Intro_DSA/LE2.html#algorithm-3",
    "href": "06_Intro_DSA/LE2.html#algorithm-3",
    "title": "Plaksha",
    "section": "Algorithm 3",
    "text": "Algorithm 3\nThe algo3 is also a recursion, which is dividing the problem into half for every iteration.\n\\[T(n) = T(\\frac{n}{2}) + 1\\] \\[T(n/2) = T(\\frac{n}{4}) + 1\\] \\[T(n) = (T(\\frac{n}{4}) + 1) + 1\\] \\[T(n) = T(\\frac{n}{2^2})+ 2\\] \\[...\\] \\[T(n) = T(\\frac{n}{2^k})+ k\\]\nThe base case, \\(T(1) = 1\\)\n\\[\\frac{n}{2^k} = 1\\] \\[k = \\log_2 n\\]\n\\[T(n) = T(\\frac{n}{2^k})+ k = T(1) + k\\] \\[T(n) = 1 + log_2 n\\]\n\\[\nTime Complexity =\nO(\\log n)\n\\]"
  },
  {
    "objectID": "06_Intro_DSA/LE2.html#algorithm-4",
    "href": "06_Intro_DSA/LE2.html#algorithm-4",
    "title": "Plaksha",
    "section": "Algorithm 4",
    "text": "Algorithm 4\nThe algo4 has a loop which iterate over the enitre array of length n. In the worst case, when the element is not present or at last, the iteration happend for the entire n times. At best when the element is at first position, the algorithm returns out at first step itself. On average when the element is in the middle, the iteration is run for \\(\\frac{n}{2}\\) times. Ignoring the constant term\n\\[\nTime Complexity =\nO(n)\n\\]"
  },
  {
    "objectID": "06_Intro_DSA/LE2.html#algorithm-5",
    "href": "06_Intro_DSA/LE2.html#algorithm-5",
    "title": "Plaksha",
    "section": "Algorithm 5",
    "text": "Algorithm 5\nIn this algorithm, we are diving the problem into 2 sub problems via recursion just like algo3. So the time complexity for the algo5 function will \\(\\log n\\). But we also have another function which is algo5_helper. The aglo5_helper run for the list and compare the elements, and merge two sub-arrays into one, and has the complexity of n. This algo5_helper is being run for log n times.\n\\[\nTime Complexity =\nO(n.\\log n)\n\\]"
  },
  {
    "objectID": "06_Intro_DSA/PA1_Group13.html",
    "href": "06_Intro_DSA/PA1_Group13.html",
    "title": "Programming Assignment 1 - Playing with Stack",
    "section": "",
    "text": "The experiment’s objective is to build the Stack ADT on top of Array and Linked List.\nThe Array implementation was performed using two different methods:\n\nincreasing the array size with a constant size whenever the stack is filled and\nexpanding the array size by doubling it\n\nThe Linked List implementation was performed with the help of a Node class, which gives a node object for every element added to the stack and linked with other nodes via a pointer next\nAfter implementing these Data Structures, we experiment to measure the time taken to push upto 10 Million elements in each implementation and compare them with each other and with the built-in Stack in the collections library named deque"
  },
  {
    "objectID": "06_Intro_DSA/PA1_Group13.html#array-based",
    "href": "06_Intro_DSA/PA1_Group13.html#array-based",
    "title": "Programming Assignment 1 - Playing with Stack",
    "section": "Array Based",
    "text": "Array Based\nWe will be implementing array-based Stack, and the Array is considered dynamic here and can be increased in size in two ways.\n\nConstant Increase\nThe Array-based implementation, which involves increasing the size of the array by the same constant size when the stack is full, is executed by copying the entire array to a new array of the size old_array + constant_factor\n\n\n\nArray with Constant Factor Increase\n\n\n\nTime Complexity for Push Operation\nFor n elements to be pushed in the stack, let us assume we have an initial array of constant size of c, and whenever the stack is filled, we increase the array by the same size c\n\\(T(n) = n + c + 2c + 3c + ... + kc\\)\nHere n is the number of elements being pushed, so for each element accessing and placing the element in the array takes \\(O(1)\\) time. Hence for n elements. it is \\(O(n)\\) time.\nAfter every c element is filled, a new array is created of old_size + c, and all the elements from the old array are copied to this new array, so for every new c addition, we have to copy c, 2c, 3c and so on elements until the final c set elements where we copy kc elements.\n\n\n\nTime Complexity for n pushes in Constant Increase Array\n\n\n\\(T(n) = n + c + 2c + 3c + ... + kc\\)\n\\(T(n) = n + c(1+2+3+...+k)\\)\n\\(T(n) = n + (c*k*(k+1))/2\\)\nc is constant and can be ignored\n\\(T(n) = n + k^2\\)\n\\(k = n/c\\)\nc is constant and can be ignored\n\\(T(n) = n + n^2\\)\n\\(T(n) = O(n^2)\\)\nThe Amortized time for each push operation is \\(T(n)/n = O(n^2)/n = O(n)\\)\n\n\n\nDoubling Size\nWe start with an array of size c(assume c=1) and push elements to the array. When the stack is filled, we create a new array of size old_size * 2. So when for \\(i^{th}\\) increase, the size of the array will be \\(2^i\\)\n\nTime Complexity for Push Operation\nHence for n elements, \\(T(n) = n + (1 + 2 + 2^2 + 2^3 + ... + 2^k)\\)\n\n\n\nArray with Doubling Size\n\n\nSimilar to previous one, the n here is adding n elements to the array, and the second term is for copying from old array to new array.\n\\(T(n) = n + (1 + 2 + 2^2 + 2^3 + ... + 2^k)\\)\n\\(T(n) = n + (2^{2k+1} +1)\\)\n\\(k=\\log_2 n\\)\n\\(T(n) = 3n - 1\\)\n\\(T(n) = O(n)\\)\nThe Amortized time for each push operation is \\(T(n)/n = O(n)/n = O(1)\\)"
  },
  {
    "objectID": "06_Intro_DSA/PA1_Group13.html#linked-list-based",
    "href": "06_Intro_DSA/PA1_Group13.html#linked-list-based",
    "title": "Programming Assignment 1 - Playing with Stack",
    "section": "Linked List Based",
    "text": "Linked List Based\nThe Linked List approach is for every push of the element, we create a node object add the element to data variable of the object and the next variable is the pointer to the next node.\n\n\n\nLinked List\n\n\n\nTime Complexity for Push Operation\nHere, for every push, we create a new node object, and the time to create an object is constant, for n elements the time taken to push these elements is creating n nodes, which takes \\(O(n)\\) time.\n\\(T(n) = n * c\\)\nc is constant and can be ignored\n\\(T(n) = O(n)\\)\nTime for each push is \\(T(n) = O(n)/n = O(1)\\)"
  },
  {
    "objectID": "06_Intro_DSA/PA1_Group13.html#comparision",
    "href": "06_Intro_DSA/PA1_Group13.html#comparision",
    "title": "Programming Assignment 1 - Playing with Stack",
    "section": "Comparision",
    "text": "Comparision\nIncremental Array is having the least optimum time complexity of \\(O(n)\\) for each push, whereas the other two implementations have \\(O(1)\\)\nIn Doubling Array, the worst case time complexity to add one element, when the array size has to be increased is \\(O(n)\\), compared to the best/worst/average time complexity of Linked List is \\(O(1)\\).\nHence, Linked List would be a better Data Structure to implement the Stack ADT."
  },
  {
    "objectID": "06_Intro_DSA/PA1_Group13.html#machine-specification",
    "href": "06_Intro_DSA/PA1_Group13.html#machine-specification",
    "title": "Programming Assignment 1 - Playing with Stack",
    "section": "Machine Specification:",
    "text": "Machine Specification:\n\nCPU: Apple M1 chip\n\n8-core CPU with 4 performance cores and 4 efficiency cores.\n5nm ARM-Based (v8.5+A) Processor\n3.2 GHz CPU Clock Speed for Performance Cores\n192(instruction) + 128(data) KB L1 Cache\n12MB L2 Cache\n\n\n\nMemory\n\n16GB of LPDDR4X-4266 MHz SDRAM"
  },
  {
    "objectID": "06_Intro_DSA/PA1_Group13.html#testing-different-implementations",
    "href": "06_Intro_DSA/PA1_Group13.html#testing-different-implementations",
    "title": "Programming Assignment 1 - Playing with Stack",
    "section": "Testing Different Implementations",
    "text": "Testing Different Implementations\nTo test both the array-based and linked list implementations, we calculate the time taken to push elements in the stack.\nThe setup is such that the time is calculated cumulatively for every 100 pushes for overall n iterations.\nThe n here is 100,000; 500,000; 1,000,000 and 10,000,000 pushes with time calculated cumulatively after every 100 pushes. For each of these experiments, it was repeated 10 times and the average time was for each 100 was calculated at the end."
  },
  {
    "objectID": "06_Intro_DSA/Stack.html",
    "href": "06_Intro_DSA/Stack.html",
    "title": "Plaksha",
    "section": "",
    "text": "import time\nfrom collections import deque\nimport matplotlib.pyplot as plt\n\n\n# Implementing Stacks using Arrays \n\nclass ArrayStack:\n    \n    # Constructor for ArrayStack class\n    def __init__(self):\n        # your code here\n        self.initialSize = 5\n        self.currentPosition = -1\n        self.stack = [None] * self.initialSize\n        \n\n    # Push: Adds a new element at the back of the list\n    def push(self, element):\n        # your code here\n        self.currentPosition += 1\n        if (self.currentPosition >= self.initialSize):\n            self.initialSize += self.initialSize\n            self.newStack = [None] * self.initialSize\n            for elementIndex in range(self.currentPosition):\n                self.newStack[elementIndex] = self.stack[elementIndex]\n            self.stack = self.newStack\n        self.stack[self.currentPosition] =  element\n\n\n    # Pop: Deletes the element at the last and returns the value of it\n    def pop(self):\n        # your code here\n        if self.currentPosition < 0:\n            return -1\n        if self.stack[self.currentPosition] != None:\n            deletedElement = self.stack[self.currentPosition]\n            self.stack[self.currentPosition] = None\n            self.currentPosition -= 1\n            return deletedElement\n        return -1\n\n\n    # Returns the size of the stack\n    def size(self):\n        # your code here\n        return self.currentPosition + 1\n\n\n    # Return the element at the top of the stack without removing it\n    def top(self):\n        if self.stack[self.currentPosition] != None:\n            return self.stack[self.currentPosition]\n        return -1\n\n\n    # Returns true is stack is empty, False if not\n    def isEmpty(self):\n        # your code here\n        if self.currentPosition >= 0:\n            return False\n        return True\n\n\n    def printIsEmpty(self):\n        print(\"\\nStack is Empty\\n\") if self.isEmpty() else print(\"\\nStack is not Empty\\n\")\n\n\n    def printStack(self):\n        print(self.stack)\n\nstk = ArrayStack()\n\nstk.size()\n\nstk.push(2)\n\nstk.push(3)\n\nstk.push(4)\n\nstk.push(5)\n\nstk.push (6)\n\nstk.printStack()\n\nstk.push(7)\n\nstk.push(8)\n\nstk.top()\n\nstk.size()\n\nstk.isEmpty()\n\nstk.printStack()\n\nstk.pop()\n\nstk.printStack()\n\nstk.printIsEmpty()\n\n[2, 3, 4, 5, 6]\n[2, 3, 4, 5, 6, 7, 8, None, None, None]\n[2, 3, 4, 5, 6, 7, None, None, None, None]\n\nStack is not Empty\n\n\n\n\n# Implementing Stacks using Arrays \n\nclass DoublingArrayStack:\n    \n    # Constructor for DoublingArrayStack class\n    def __init__(self):\n        # your code here\n        self.initialSize = 5\n        self.currentPosition = -1\n        self.stack = [None] * self.initialSize\n        self.stackSizeMultiple = 1\n        \n\n    # Push: Adds a new element at the back of the list\n    def push(self, element):\n        # your code here\n        self.currentPosition += 1\n        if (self.currentPosition >= self.initialSize):\n            self.stackSizeMultiple += 1\n            self.initialSize = self.initialSize * self.stackSizeMultiple\n            self.newstack = [None] * self.initialSize\n            for elementIndex in range(self.currentPosition):\n                self.newstack[elementIndex] = self.stack[elementIndex]\n            self.stack = self.newstack\n        self.stack[self.currentPosition] =  element\n\n\n    # Pop: Deletes the element at the last and returns the value of it\n    def pop(self):\n        # your code here\n        if self.currentPosition < 0:\n            return -1\n        if self.stack[self.currentPosition] != None:\n            deletedElement = self.stack[self.currentPosition]\n            self.stack[self.currentPosition] = None\n            self.currentPosition -= 1\n            return deletedElement\n        return -1\n\n\n    # Returns the size of the stack\n    def size(self):\n        # your code here\n        return self.currentPosition + 1\n\n\n    # Return the element at the top of the stack without removing it\n    def top(self):\n        if self.stack[self.currentPosition] != None:\n            return self.stack[self.currentPosition]\n        return -1\n\n\n    # Returns true is stack is empty, False if not\n    def isEmpty(self):\n        # your code here\n        if self.currentPosition >= 0:\n            return False\n        return True\n\n\n    def printIsEmpty(self):\n        print(\"\\nStack is Empty\\n\") if self.isEmpty() else print(\"\\nStack is not Empty\\n\")\n\n\n    def printStack(self):\n        print(self.stack)\n\nstk = DoublingArrayStack()\n\nstk.size()\n\nstk.push(2)\n\nstk.push(3)\n\nstk.push(4)\n\nstk.push(5)\n\nstk.push (6)\n\nstk.push(7)\n\nstk.push(8)\n\nstk.top()\n\nstk.size()\n\nstk.isEmpty()\n\nstk.printStack()\n\nstk.pop()\n\nstk.printStack()\n\n[2, 3, 4, 5, 6, 7, 8, None, None, None]\n[2, 3, 4, 5, 6, 7, None, None, None, None]\n\n\n\n# Implementing Stacks Linked Lists\n\n# Node class for the individual nodes\nclass Node:\n\n    # constructor for Node class\n    def __init__(self, data):\n        # your code here\n        self.data = data\n        self.next = None\n\n# Manager class to link the nodes and manage the overall list\nclass LinkedListStack:\n\n    # constructor for LinkedListStack class\n    def __init__(self):\n        # your code here\n        self.stackSize = 0\n\n    # Push: Adds a new element at the back of the list\n    def push(self, data):\n        # your code here\n        if (self.stackSize == 0):\n            self.headNode = Node(data)\n        else:\n            newNode = Node(data)\n            newNode.next = self.headNode\n            self.headNode = newNode\n        \n        self.stackSize += 1\n\n    # Pop: Deletes the element at the last and returns the value of it\n    def pop(self):\n        # your code here\n        if(self.headNode != None and self.stackSize > 0):\n            currentNode = self.headNode\n            self.headNode = self.headNode.next\n            headData = currentNode.data\n            self.stackSize -= 1\n            del currentNode\n            return headData\n        return None\n\n\n    # Returns the size of the stack\n    def size(self):\n        # your code here\n        return self.stackSize\n\n\n    # Return the element at the top of the stack without removing it\n    def top(self): \n        # your code here\n        return self.headNode.data\n\n\n    # Return true is stack is empty, False if not\n    def isEmpty(self):\n        # your code here\n        if self.stackSize == 0:\n            return True\n        return False\n\n\n    def printIsEmpty(self):\n        print(\"\\nStack is Empty\\n\") if self.isEmpty() else print(\"\\nStack is not Empty\\n\")\n\n    # Reverses the stack\n    def reverseList(self):\n        # your code here\n        if(self.stackSize > 1):\n            previousNode = self.headNode\n            currentNode = previousNode.next\n            previousNode.next = None\n            nextNode = currentNode.next\n            while(currentNode != None):\n                nextNode = currentNode.next\n                currentNode.next = previousNode\n                previousNode = currentNode\n                currentNode = nextNode\n            self.headNode = previousNode\n\n    # Print the linked list in reverse direction\n    def printReverseLinkedList(self, currentNode):\n        if(currentNode != None):\n            self.printReverseLinkedList(currentNode.next)\n            print(currentNode.data)\n\n    # Print data stored in the stack\n    def printStack(self):\n        # your code here\n        currentNode = self.headNode\n        self.printReverseLinkedList(currentNode)\n\nstk = LinkedListStack()\n\nstk.push(2)\n\nstk.push(3)\n\nstk.push(4)\n\nstk.push(5)\n\nstk.printStack()\n# print(stk.pop())\n# print(stk.pop())\n# print(stk.pop())\n# print(stk.pop())\n\n# stk.printIsEmpty()\n\n# stk.push(1)\n\n# stk.printStack()\n\nstk.reverseList()\n\nstk.printStack()\n\n2\n3\n4\n5\n5\n4\n3\n2\n\n\n\nimport random as random\nfrom time import perf_counter\n\n# the program to be timed here\ndef calculateTime(stackObject, iterations):\n    start = perf_counter()\n    timeList  = []\n    for i in range(iterations):\n        number = random.randint(1,100)\n        if (type(stackObject) == deque):\n            stackObject.append(number)\n        else:\n            stackObject.push(number)\n        if (i//100 == 0):\n            end = perf_counter()\n            timeList.append(end-start)\n    return timeList\n\niterations = 10_00_000\narrayStack = ArrayStack()\narrayTimeList = calculateTime(arrayStack, iterations)\ndoublingArrayStack = DoublingArrayStack()\ndoublingArrayStackTimeList = calculateTime(doublingArrayStack, iterations)\nlinkedListStack = LinkedListStack()\nlinkedListTimeList = calculateTime(linkedListStack, iterations) \ninternalStack = deque()\ninternalStackTimeList = calculateTime(internalStack, iterations)\n\nplt.plot(arrayTimeList)\nplt.plot(doublingArrayStackTimeList)\nplt.plot(linkedListTimeList)\nplt.plot(internalStackTimeList)\nplt.legend([\"Constant Increase\", \"Doubling Size\", \"Linked List\", \"Internal Stack\"])\nplt.title(\"Comparison of Different Stack Implementation\")\n\nText(0.5, 1.0, 'Comparison of Different Stack Implementation')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "03_Intro_AI/03_Lab4_Chainning.html",
    "href": "03_Intro_AI/03_Lab4_Chainning.html",
    "title": "Rule Based Knowledge Assignment",
    "section": "",
    "text": "from typing import List, Dict, Set\n\n\nwith open(\"./data/file.txt\") as f:\n    observations = set(f.readline().strip().split(\",\"))\n    # Empty Line\n    f.readline()\n    rules = {}\n    # Loop over the remaining lines\n    for line in f.readlines():\n        # For rules\n        if \"=>\" in line:\n            # Split with \"=>\"\n            rule, goal = line.strip().split(\"=>\")\n            # If the goal is already present in dictionary, append the\n            # new rule_line as a set to the list\n            # If goal not present, create an empty list and than\n            # append the rule set\n            rules.setdefault(goal, []).append(set(rule.split(\"+\")))\n        # For Final Goal\n        elif line.isalpha():\n            finalgoal = line\n\nprint(observations)\nprint(finalgoal)\nprint(rules)\n    \n\n{'e', 'h', 'a', 'k', 'g', 'd', 'c'}\nq\n{'i': [{'m', 'l', 'k'}], 'q': [{'l', 'i', 'j'}, {'b', 'a'}, {'l', 'n', 'p', 'o'}], 'b': [{'e', 'd', 'c'}, {'h', 'f'}], 'r': [{'h', 'c'}], 's': [{'m', 'r', 'j'}], 'f': [{'g'}]}\n\n\n\nBackward Chainning\nGiven a Final Goal and a set of observations, determine if that set of observations leads to the final goal or not.\n\ndef backwardChainning(rules_set: Dict, \n                    observation_set: Set,\n                    final_goal: str,\n                    path: List) -> bool:\n    \"\"\"\n    Predict the given Final Goal is reachable or not from the given observations.\n    \"\"\"\n    # Base Cases\n    # If final_goal in observation, return True\n    if final_goal in observation_set:\n        return True, path\n    # If Goal/Final Goal not in observation\n    # Not even in knowledge base, we can't reach it.\n    # Return False\n    elif final_goal not in rules_set:\n        return False, path\n\n    for rule_set in rules_set[final_goal]:\n        for rule in rule_set:\n            # Recursion with Depth First Search\n            temp_result, path = backwardChainning(rules_set, observation_set, rule, path)\n            if temp_result == False:\n                break # Go to next rule_set\n        # If there was no break, which means all rule in the set\n        # was present, return True.\n        if temp_result: \n            path.append(f\"{rule_set} => {final_goal}\")\n            return temp_result, path\n        \n    return temp_result, path\n\n\nisFinalGoalReached, path = backwardChainning(rules, observations, finalgoal, [])\nprint(f\"Is the expected Final Goal {finalgoal} is reachable: {isFinalGoalReached}\")\nprint(f\"The path to reach the Final Goal is: {'; '.join(path)}\")\n\nIs the expected Final Goal q is reachable: True\nThe path to reach the Final Goal is: {'e', 'd', 'c'} => b; {'b', 'a'} => q\n\n\n\n\nForward Chainning\nGiven a set of observations and knowledge base, identify the most deep goal possible\n\ndef forwardChainning(rules_set: Dict, observation_set: set) -> str|None:\n    reached_goals = []\n    addition = True\n\n    while addition:\n        # Loop until there is no addition to observation\n        addition = False\n        # Go through each goal in the rules\n        for goal, rule_list in rules_set.items():\n            # Loop through all the set of rules(paths) to reach\n            # the goal\n            if goal in observation_set:\n                continue\n            # Check for each observation in the rule list(each line in txt file)\n            for rule_set in rule_list:\n                for rule in rule_set:\n                    # If observation is not present, skip the list\n                    if rule not in observation_set:\n                        break\n                else:\n                    # All rule of rule_set is present in observation\n                    # So goal acheived, add to observation set\n                    observation_set.add(goal)\n                    \n                    # Add the goal to listr of goals reached.\n                    reached_goals.append(goal)\n\n                    # Since there was an addition, we have to do one more run\n                    addition = True\n\n                    # Since goal is reached no need to check for further in that list.\n                    break\n    \n    return reached_goals\n\n\nobs_copy = observations.copy()\nacheivableGoals = forwardChainning(rules, obs_copy)\nprint(f\"The acheivable goals from the given observations are: {', '.join(acheivableGoals)}\")\n\nThe acheivable goals from the given observations are: b, r, f, q"
  },
  {
    "objectID": "03_Intro_AI/index.html",
    "href": "03_Intro_AI/index.html",
    "title": "Introduction to Artificial Intelligence",
    "section": "",
    "text": "AI Today compared to Dartmouth Workshop\nA* Algorithm\nBackward and Forward Chainning\nPOS Tagging using Hidden Markov Model\nLearnig Gates using Single and Multi Layer Perceptron"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html",
    "href": "03_Intro_AI/04_Lab5_HMM.html",
    "title": "Hidden Markov Model",
    "section": "",
    "text": "# Importing libraries\nimport nltk\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n \n# download the treebank corpus from nltk\nnltk.download('treebank')\n \n# download the universal tagset from nltk\nnltk.download('universal_tagset')\n \n# reading the Treebank tagged sentences\nnltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n \n# print the first two sentences along with tags\nprint(nltk_data[:2])\n\n[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html#random-10-sentences-test-accuracy",
    "href": "03_Intro_AI/04_Lab5_HMM.html#random-10-sentences-test-accuracy",
    "title": "Hidden Markov Model",
    "section": "Random 10 sentences Test Accuracy",
    "text": "Random 10 sentences Test Accuracy\n\n# test the Viterbi algorithm on a few sample sentences of test dataset\nrandom.seed(1234)      # define a random seed to get same sentences when run multiple times\nnp.random.seed(1234)\n\n# choose random 10 numbers\nrndom = [random.randint(1, len(test_set)) for x in range(10)]\n \n# list of 10 sentencess on which to test the model\ntest_run = [test_set[i] for i in rndom]\n \n# list of tagged words\ntest_run_base = [tup for sent in test_run for tup in sent]\n \n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_run for tup in sent]\n\n# testing 10 sentences to check the accuracy\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end - start\n \nprint(\"Time taken in seconds:\", difference)\n \n# accuracy should be good enough (> 90%) to be a satisfactory model\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n \naccuracy = len(check) / len(tagged_seq)\nprint('Viterbi Algorithm Accuracy:', accuracy * 100)\n\nTime taken in seconds: 0.06524825096130371\nViterbi Algorithm Accuracy: 92.82296650717703"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html#test-accuracy-for-the-entire-test-set",
    "href": "03_Intro_AI/04_Lab5_HMM.html#test-accuracy-for-the-entire-test-set",
    "title": "Hidden Markov Model",
    "section": "Test Accuracy for the entire Test Set",
    "text": "Test Accuracy for the entire Test Set\n\n# test the Viterbi algorithm on a few sample sentences of test dataset\nrandom.seed(1234)      # define a random seed to get same sentences when run multiple times\nnp.random.seed(1234)\n\n# list of tagged words\ntest_run_base = [tup for sent in test_set for tup in sent]\n \n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_set for tup in sent]\n\n# testing 10 sentences to check the accuracy\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end - start\n \nprint(\"Time taken in seconds:\", difference)\n \n# accuracy should be good enough (> 90%) to be a satisfactory model\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n \naccuracy = len(check) / len(tagged_seq)\nprint('Viterbi Algorithm Accuracy:', round(accuracy * 100))\n\nTime taken in seconds: 3.193380355834961\nViterbi Algorithm Accuracy: 90"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html#hot-cold-state-problem",
    "href": "03_Intro_AI/04_Lab5_HMM.html#hot-cold-state-problem",
    "title": "Hidden Markov Model",
    "section": "Hot Cold State Problem",
    "text": "Hot Cold State Problem\nTwo Hidden States: Hot🥵 and Cold🥶\nThree Observations: 1, 2, and 3\n\nProbability of Sequence 3-1-3 Occuring?\nAns: 2.86%\n\n\nMost Probable States Sequence given the Observation 3-1-3\nAns: Hot-Cold-Hold"
  },
  {
    "objectID": "03_Intro_AI/01_Lab01.html",
    "href": "03_Intro_AI/01_Lab01.html",
    "title": "AI Today Compared to Dartmouth Workshop",
    "section": "",
    "text": "How has the focus on AI shifted from this original proposal to today? Specifically, talk about imbuing AI with human thinking and intelligence.\nThe major change in today’s AI would be the proper definition of tasks and problems being solved, whereas, in the Dartmouth workshop proposal, the problems which were trying to be solved were abstract and high-level.\nToday’s AI is more and more data dependent where with lots of data available today, the models are based on learning patterns in the data rather than learning the general environment. This approach of learning from data which is curated and marked, known as supervised learning, has limitations in scaling.\nWe humans from childhood start learning patterns from what we see even without many labels, and we use the prior information we learn through life to learn new things. The field of AI has been trying to simulate this phenomenon of learning from the environment directly without having labelled data, by exposing the AI to a large magnitude of unlabelled data, which allows the system to learn the representation of the world and objects and form an approximate model of common-sense. This method today was coined as the dark matter of intelligence[1].\nAnother common method today to build AI is how humans build intelligence by playing games and practising. AI today is trained to play games by playing against itself a very large number of times and to self-improve with time, which was also part of the proposal in the workshop.\nCommentary on the long-term implications of the workshop.\nThe workshop was the spark to start a huge field, which went through multiple ups and downs. The idea of computers being able to simulate or solve any real-world problem is still not achieved, but the progress so far has been great with various successes in the field of vision and language. The workshop brainstormed about different problems computers can solve on their own, and made people think about computers as not just tools to perform instructions but as intelligent beings.\nReferences\n[1] LeCun, Yan & Misra, Ishan. Self-supervised learning: The dark matter of intelligence. Meta AI. https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/"
  },
  {
    "objectID": "03_Intro_AI/02_Lab2_Submission_A_Star.html",
    "href": "03_Intro_AI/02_Lab2_Submission_A_Star.html",
    "title": "01 Programming Assignment",
    "section": "",
    "text": "from collections import deque\n\nclass Graph:\n    # example of adjacency list (or rather map) containing node: (neigbhor node, weight of edge)\n    # adjacency_list = {\n    # 'A': [('B', 1), ('C', 3), ('D', 7)],\n    # 'B': [('D', 5)],\n    # 'C': [('D', 12)]\n    # }\n\n    # initialises object with the given node: neighbor list\n    def __init__(self, adjacency_list: dict):\n        self.adjacency_list = adjacency_list\n\n    # gets the neighbor of a given node v from the list\n    def get_neighbors(self, v: str):\n        \"\"\"\n        Returns the List of nodes connected to the node v.\n        \"\"\"\n        try:\n            return self.adjacency_list[v]\n        except KeyError:\n            return []\n\n    # heuristic function with values for all nodes\n    def h(self, n):\n        \"\"\"\n        Heuristic Function, denoting the cost value from n to the goal state.\n        \"\"\"\n        H_dist = {\n            'A': 11,\n            'B': 6,\n            'C': 99,\n            'D': 1,\n            'E': 7,\n            'G': 0,    \n        }\n        return H_dist[n]\n    \n    def back_track(self,start_node, goal_state, back_track_dict):\n        \"\"\"\n        Returns the Path from goal_state to start_node\n        It traces the dictionary, starting from goal_state\n        and check it's parent, than again check parent's parent\n        and so on, till the parent node is initial node while adding\n        each parent node to a string.\n\n        Return the reverse of string to get path.\n        \"\"\"\n        curr_node = goal_state\n        path = goal_state\n        while start_node != curr_node:\n            curr_node = back_track_dict[curr_node]\n            path += curr_node\n\n        return path[::-1]\n\n    def a_star(self, start_node, stop_node):\n        # store visited nodes with uninspected neighbors in open_list, starting with the start node\n        # store visited nodes with inspected neighbors in closed_list\n        open_list = set([start_node])\n        closed_list = set([])\n\n        # g contains current distances from start_node to all other nodes\n        # the default value (if it's not found in the map) is +infinity\n        # It is the priority queue to select the next node for exploration\n        # node: (heuristic_cost to each here(h), cost to go to node(c), total_cost (h+c))\n        g = {}\n\n        g[start_node] = self.h(start_node), 0, self.h(start_node)+0\n\n        # Parents: dictionary where key is a node and it's value \n        # is the parent of that node.\n        parents = {}\n        parents[start_node] = start_node\n        current_node = start_node\n\n        while len(g) > 0:\n\n            # if current node is the goal state, return the path.\n            if current_node == stop_node:\n                return self.back_track(start_node, current_node, parents)\n\n            closed_list.add(current_node)\n            # Traversing the Graph\n            for (child, cost) in self.get_neighbors(current_node):\n                # if node is not in frontier and explored sets\n                if (child not in g) and (child not in closed_list):\n                    # Add the new node to frontier as key, and it's value\n                    # as a tuple of 3 values.\n                    g[child] = (self.h(child) , cost, self.h(child) + cost)\n                    parents[child] = current_node\n                \n                # if the child is present in frontier\n                elif child in g:\n                    # if the new path to the node is cheaper than already present\n                    # in frontier, replace the new cost\n                    if g[child][-1] > g[current_node][1] + cost + self.h(child):\n                        g[child] = (g[current_node][1] + cost, \n                                    self.h(child), \n                                    g[current_node][1] + self.h(child) + cost)\n                        # Also replace the parent of node\n                        parents[child] = current_node\n            \n            # Remove from frontier the explored node.\n            del g[current_node]\n            # Select new node to be explored based on minimum cost value.\n            current_node = min(g.items(), key=lambda item: item[1][-1])[0]\n            \n            \n        return None\n\n\nadjacency_list = {\n    'A': [('B', 1), ('C', 3), ('D', 7)],\n    'B': [('D', 5)],\n    'C': [('D', 12)]\n}\n\n# Driver code for the given graph\ngraph = Graph(adjacency_list)\ngraph.a_star('A', 'D')\n\n'ABD'"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html",
    "href": "03_Intro_AI/05_Lab6_MLP.html",
    "title": "Multi Perceptron Layer",
    "section": "",
    "text": "# Imports\nimport math\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#and-gate",
    "href": "03_Intro_AI/05_Lab6_MLP.html#and-gate",
    "title": "Multi Perceptron Layer",
    "section": "AND Gate",
    "text": "AND Gate\n\nand_data = np.array([[0,0,0],\n                     [0,1,0],\n                     [1,0,0],\n                     [1,1,1]])\n\n# Creating a PyTorch tensor\nand_data = torch.Tensor(and_data)\n\n\n# Same slicing as np arrays\nX = and_data[:,:-1]\ny = and_data[:,-1:]\n\n\nW = torch.randn((1,2), requires_grad=True)\nb = torch.randn((1,1), requires_grad=True)\n\nCreate the training loop (1 point)\n\nn_epochs = 100\nlr = 5e-1\nlosses = []\nfor _ in range(n_epochs):\n    # Define the Training Loop here\n\n    # Get predictions\n    output = sigmoid(perceptron(X, W, b))\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    with torch.no_grad():\n        W = W - lr*W.grad\n        b = b - lr*b.grad\n    W.requires_grad = True\n    b.requires_grad = True\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((perceptron(X, W, b) > 0.5).int())\n\ntensor([[0],\n        [0],\n        [0],\n        [1]], dtype=torch.int32)"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#or-gate",
    "href": "03_Intro_AI/05_Lab6_MLP.html#or-gate",
    "title": "Multi Perceptron Layer",
    "section": "OR Gate",
    "text": "OR Gate\n\nor_data = np.array([[0,0,0],\n                     [0,1,1],\n                     [1,0,1],\n                     [1,1,1]])\n\n# Creating a PyTorch tensor\nor_data = torch.Tensor(or_data)\n\n\n# Same slicing as np arrays\nX = or_data[:,:-1]\ny = or_data[:,-1:]\n\n\nW = torch.randn((1,2), requires_grad=True)\nb = torch.randn((1,1), requires_grad=True)\n\nReuse the training loop\n\nn_epochs = 100\nlr = 5e-1\nlosses = []\n\nfor _ in range(n_epochs):\n    # Get predictions\n    output = sigmoid(perceptron(X, W, b))\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    with torch.no_grad():\n        W = W - lr*W.grad\n        b = b - lr*b.grad\n    W.requires_grad = True\n    b.requires_grad = True\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((perceptron(X, W, b) > 0.5).int())\n\ntensor([[0],\n        [1],\n        [1],\n        [1]], dtype=torch.int32)"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#xor-gate",
    "href": "03_Intro_AI/05_Lab6_MLP.html#xor-gate",
    "title": "Multi Perceptron Layer",
    "section": "XOR Gate",
    "text": "XOR Gate\n\nxor_data = np.array([[0,0,0],\n                     [0,1,1],\n                     [1,0,1],\n                     [1,1,0]])\n\n# Creating a PyTorch tensor\nxor_data = torch.Tensor(xor_data)\n\n\n# Same slicing as np arrays\nX = xor_data[:,:-1]\ny = xor_data[:,-1:]\n\n\nW = torch.randn((1,2), requires_grad=True)\nb = torch.randn((1,1), requires_grad=True)\n\nReuse the training loop\n\nn_epochs = 100\nlr = 5e-1\nlosses = []\n\nfor _ in range(n_epochs):\n    # Get predictions\n    output = sigmoid(perceptron(X, W, b))\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    with torch.no_grad():\n        W = W - lr*W.grad\n        b = b - lr*b.grad\n    W.requires_grad = True\n    b.requires_grad = True\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((perceptron(X, W, b) > 0.5).int())\n\ntensor([[0],\n        [0],\n        [0],\n        [0]], dtype=torch.int32)"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#need-for-mlp",
    "href": "03_Intro_AI/05_Lab6_MLP.html#need-for-mlp",
    "title": "Multi Perceptron Layer",
    "section": "Need for MLP",
    "text": "Need for MLP\nAs seen above, we are unable to model the XOR gate using a single layer perceptron, so we need to add a hidden layer.\n\nW1 = torch.randn((10,2), requires_grad=True)\nW2 = torch.randn((1,10), requires_grad=True)\nb1 = torch.randn((1,10), requires_grad=True)\nb2 = torch.randn((1,1), requires_grad=True)\n\nImplement the mlp function (1 point)\n\ndef mlp(inputs, W1, W2, b1, b2):\n    '''\n    Defines the multi-layer perceptron model\n\n    Note: Only 1 hidden layer\n    '''\n    output = sigmoid(perceptron(inputs, W1, b1))\n    output = sigmoid(perceptron(output, W2, b2))\n\n    return output\n\n\ndef weights_update(weights_list, bias_list):\n    # Update Weights\n    updated_w = []\n    updated_b = []\n    for w,b in zip(weights_list, bias_list):\n        with torch.no_grad():\n                w = w - lr*w.grad\n                b = b - lr*b.grad\n        w.requires_grad = True\n        b.requires_grad = True\n        updated_w.append(w)\n        updated_b.append(b)\n    \n    return updated_w, updated_b\n\nReuse the training loop\nNOTE: It will require slight modification due to the hidden layer\n\nn_epochs = 1000\nlr = 5e-1\nlosses = []\nfor _ in range(n_epochs):\n     # Get predictions\n    output = mlp(X, W1, W2, b1, b2)\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    (W1,W2),(b1,b2) = weights_update([W1,W2], [b1,b2])\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((mlp(X, W1, W2, b1, b2) > 0.5).int())\n\ntensor([[0],\n        [1],\n        [1],\n        [0]], dtype=torch.int32)"
  },
  {
    "objectID": "01_Mathematics_I/06_Maths_PageRank.html",
    "href": "01_Mathematics_I/06_Maths_PageRank.html",
    "title": "Plaksha",
    "section": "",
    "text": "08 Assignment - Page Rank\n\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nQuestion 1\nConsider any \\(2×2\\) matrix \\(A\\) and apply the matrix on a \\(2×1\\) vector \\(v\\). Keep applying this matrix and notice that the direction converges, while the magnitude may get larger. Keep normalizing \\(v\\) and notice the direction to which it converges. Use Numpy\n\nsomeMatrix = np.array([\n    [1,2],\n    [3,4]\n])\n\nsomeVector = np.array([2,3])\n\nprint(f\"Shape of Matrix: {someMatrix.shape}\")\nprint(f\"Shape of Vecotr: {someVector.shape}\")\n\nShape of Matrix: (2, 2)\nShape of Vecotr: (2,)\n\n\n\ndef angleBetween(v1, v2):\n    unit_v1 = v1/np.linalg.norm(v1)\n    unit_v2 = v2/np.linalg.norm(v2)\n    # Finding cos inverse of dot product to get angle in radians\n    # Convertring to degree\n    return 360*(np.arccos(np.dot(unit_v1, unit_v2))) / (2*np.pi)\n\n\nx_axis = np.array([1,0])\nsomeVectorAngle = [angleBetween(someVector, x_axis)]\n\nfor _ in range(999):\n    someVector = np.matmul(someMatrix, someVector)\n    someVector = someVector/np.linalg.norm(someVector)\n    someVectorAngle.append(angleBetween(someVector, x_axis))\n\n\nnp.linalg.eig(someMatrix)\n\n(array([-0.37228132,  5.37228132]),\n array([[-0.82456484, -0.41597356],\n        [ 0.56576746, -0.90937671]]))\n\n\n\nprin_eig = np.linalg.eig(someMatrix)[1][:,1]\n\n\nprin_eig, someVector\n\n(array([-0.41597356, -0.90937671]), array([0.41597356, 0.90937671]))\n\n\n\nx = list(range(1,1001))\ny = [angleBetween(prin_eig, x_axis)]*1000\nplt.plot(np.log(x),someVectorAngle, label=\"Convergence\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nQuestion 3\n\n\nProbabilityMatrix = np.array([\n        [0,0,0,0.5,0.5,1,1,1],\n        [0.5,0,0,0,0,0,0,0],\n        [0.5,0,0,0,0,0,0,0],\n        [0,0.5,0,0,0,0,0,0],\n        [0,0.5,0,0,0,0,0,0],\n        [0,0,0.5,0,0,0,0,0],\n        [0,0,0.5,0,0,0,0,0],\n        [0,0,0,0.5,0.5,0,0,0]\n    ])\n\n\nnp.linalg.eig(ProbabilityMatrix)[1][:,0]/np.linalg.norm(np.linalg.eig(ProbabilityMatrix)[1][:,0])\n\narray([0.74278135+0.j, 0.37139068+0.j, 0.37139068+0.j, 0.18569534+0.j,\n       0.18569534+0.j, 0.18569534+0.j, 0.18569534+0.j, 0.18569534+0.j])\n\n\n\nnp.linalg.matrix_power(ProbabilityMatrix, 1000)[:,0]*2.4141\n\narray([0.7428, 0.3714, 0.3714, 0.1857, 0.1857, 0.1857, 0.1857, 0.1857])\n\n\nQuestion 4\nWrite down the edge list of the above graph and use networkx to run a random walk on it. What is the distribution of visits?\n\ngraph = {\n    'a': ['b', 'c'],\n    'b': ['d', 'e'],\n    'c': ['f', 'g'],\n    'd': ['a', 'h'],\n    'e': ['a', 'h'],\n    'f': ['a'],\n    'g': ['a'],\n    'h': ['a']\n}\n\n\npointer = random.choice(list(graph.keys()))\nvisits = {k:0 for k,v in graph.items()}\nfor _ in range(8000):\n    pointer = random.choice(graph[pointer])\n    visits[pointer] += 1\n\n\npointer = random.choice(list(graph.keys()))\nvisits = {k:0 for k,v in graph.items()}\nfor _ in range(10000):\n    for _ in range(1000):\n        pointer = random.choice(graph[pointer])\n    visits[pointer] += 1\n\n\nnodes = list(visits.keys())\nvalues = list(visits.values())\nplt.bar(nodes, values)\nplt.plot(nodes,values, color='red')\n\n\n\n\nQuestion 5\nAre the following two questions equivalent?:\n\nIf I were to give pocket money to two of my daughters: 500 rupees each.\nI toss a coin and give my elder daughter 1000 rupees if its heads or the younger daughter 1000 rupees if its tails.\nAre these two statements equivalent? In the sense that, by the end of the year, do you think both my daughters would have received, more or less, the same amount of money?\n\n\n# Case 1 - Always Rs.500\n\nyounger_daughter_equal = 0\nelder_daughter_equal = 0\n\nyd_eq_list = []\ned_eq_list = []\n\nfor _ in range(365):\n    younger_daughter_equal += 500\n    yd_eq_list.append(younger_daughter_equal)\n    elder_daughter_equal += 500\n    ed_eq_list.append(elder_daughter_equal)\n\n\n# Case 2 - Giving Rs.1000 based on Toss Coin.\n\nyounger_daughter_coin = 0\nelder_daughter_coin = 0\n\nyd_coin_list = []\ned_coin_list = []\n\nfor _ in range(365):\n    if random.choice([0,1]):\n        younger_daughter_coin += 1000\n        yd_coin_list.append(younger_daughter_coin)\n        ed_coin_list.append(elder_daughter_coin)\n    else:\n        elder_daughter_coin += 1000\n        ed_coin_list.append(elder_daughter_coin)\n        yd_coin_list.append(younger_daughter_coin)\n\n\nplt.plot(list(range(365)), yd_eq_list, label=\"Equal Distribution\")\nplt.plot(list(range(365)), yd_coin_list, label=\"Coin Toss Distribution\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nQuestion 6 Consider the adjacency matrix of the above graph, tweak it and observe that the principal eigen vector is same as the answer to the previous question. (Principal Eigen Vector is defined as the eigen vector corresponding to the highest eigen value) — INCORRECT QUESTION —\nNew Question 6 Solve for Two Simultaneous Equations\n\ndef findIntersection(slope1, intercept1, slope2, intercept2):\n    if slope1 == slope2:\n        return -1\n    x = (intercept1-intercept2)/(slope2-slope1)\n    return (x, slope1*x+intercept1)\n\n\nfindIntersection(0,0,1,0)\n\n(0.0, 0.0)\n\n\nQuestion 7\nCan you consider a network of vertices, in the order of thousands and figure out the answer? You will observe that the best method to use is the random walk (with teleportation). why?\nIf the number of vertices are very high like thousands, the method to calculate eigen vectors requires matrix multiplications which is \\(O(n^2)\\), where as a random walk is a linear operation."
  },
  {
    "objectID": "01_Mathematics_I/01_Maths_09_Sept.html",
    "href": "01_Mathematics_I/01_Maths_09_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "import math\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef findMinima(equation: callable, step_size: float, step_reduce: float, iterations: int):\n    oldX = random.random()\n    print(f\"Starting with Random Numebr of {oldX}\")\n    oldY = equation(oldX)\n\n    oldestX = oldX\n    oldestY = oldY\n\n    valuesY = [oldY]\n\n    i = 1\n    while True:\n        newX = oldX + step_size\n        newY = equation(newX)\n        if newY > oldY:\n            oldY = oldestY\n            step_size /= step_reduce\n            oldX = oldestX\n        else:\n            valuesY.append(newY)\n            oldY, oldestY = newY, oldY\n            oldX, oldestX = newX, oldX\n\n        i += 1\n        if i == iterations:\n            break\n    \n    return valuesY, newX\n\n\niterations = 100\ndistanceValues, finalX = findMinima(lambda x : math.sqrt(50*(x**2) - 142*x + 101), 0.1, 10, iterations)\nxAxis = np.log(list(range(1,iterations+1)))\nplt.plot(distanceValues)\n\nStarting with Random Numebr of 0.3019662499232486\n\n\n\n\n\n\nprint(f\"Final value of X is: {finalX} and Loss Value is {distanceValues[-1]}\")\n\nFinal value of X is: 1.420000001024248 and Loss Value is 0.42426406871190303\n\n\n\n\n\nimport random\nnumHeads = []\nfor ball in range(1000):\n    sum = 0\n    for nail in range(50):\n        sum += random.randint(0,1)\n    numHeads.append(sum)"
  },
  {
    "objectID": "01_Mathematics_I/index.html",
    "href": "01_Mathematics_I/index.html",
    "title": "Mathematics I",
    "section": "",
    "text": "Linear Algebra Questionverse\n9th September Notes\n11th September Notes\n12th September Notes\n12th September Problems\n13th September Notes\nPage Rank\nProject - Predict Leader"
  },
  {
    "objectID": "01_Mathematics_I/03_Maths_12_Sept.html",
    "href": "01_Mathematics_I/03_Maths_12_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "05 Maths Assignment 12th September\nQuestion1\nConsider the matrix\n\\[\\begin{bmatrix}\n1 & 2\\\\\n3 & 4\n\\end{bmatrix}\\]\n\nWhat does it remind you of?\nWhat does it denote?\nWhere and why do we use a matrix?\nCan you enlist a few applications of matrices?\n\nThe above matrix reminds of the system of equations with two equations and two unknowns.\nIt denotes two vectors in the 2D-space (\\(ℝ^2\\)) which can also be interpreted as two equations.\nWe use a matrix as a function, and also to solve for unknows given equations.\nMatrices are functions which transforms vectors from one space to another space.\nMatrices are used to solve equations as they are represented in a simple manner.\nMatrices helps in storing data like images where elements are values of pixel intensity.\nQuestion 2\nDefine a function. What is a surjective, injective and bijective function?\nA function is a mapping from one space to another space, where each element in the left space(Domain) can have one and only mapping to the element in the right space(Co-domain).\nFunction takes an input from Domain and maps it to an element in co-domain.\nThere can be two or more elements in Domain mapping to same element in co-domain, but there can never be an element in Domain mapping multiple elements in co-domain.\nA Valid Function\n\nAn Invalid Function\n\nImage Credits: https://en.wikipedia.org/wiki/Function_%28mathematics%29\nSurjective Function\nThe onto function, is where every element in co-domain has atleast one pre-image in the domain.\n\nImage Credits: https://en.wikipedia.org/wiki/Surjective_function\nInjective Function\nThe one-one function, is where there is only one unique mapping to the element in the co-domain, there can be elements in co-domain which are not mapped, but function must map the elements of the domain to unique element in co-domain.\n\nImage Credits: https://en.wikipedia.org/wiki/Injective_function\nBijective Function\nIt is the combination of both the surjection and injection function, where every element in X is mapped to a unique(exactly one) element in Y, and there are no unpaired elements.\n\nImage Credits: https://en.wikipedia.org/wiki/Bijection\nQuestion 3\nGiven an example of a function \\(f:ℝ^2 → ℝ^2\\)\nA function which maps from \\(ℝ^2\\) to \\(ℝ^2\\) will be \\((x,y)\\) to \\((y,x)\\), where the function is mirror reflection along the \\(x=y\\) line\nQuestion 4\nGiven an example of a very nice function \\(f:ℝ^2 → ℝ^2\\) * Make extra efforts to make this function really nice. * Explain what is so nice about your function? * Why should one study such functions?\nA really nice function is the softmax function. The excellent property of this function is it can convert any point in \\(ℝ^2\\) to a probability distribution, which is a vector of probabilities and they are relative to the scale of value in the original vector.\n\\(softmax = \\frac{e^{z_i}}{\\displaystyle\\sum_{j=1}^{k} e^{z_j}}\\)\nFor each value in vector, it exponentiates the value, and divide it by the sum of all values being exponentiated, which gives the sum of all values after applying softmax to 1.\nThe important reason to study such functions, is to make use of it for various applications like classification, where a binary classification problem which gives some arbitrary values can be converted to a nice probability looking value.\nQuestion 7\nConsider \\(ℝ^2\\). What are all the properties of this set? \\(ℝ^2\\) is called a space of all vectors, a.k.a a vector space. Lookup for the definition of a vector space.\nThis set contains all the elements of the form \\((x,y)\\) where \\(x,y \\in ℝ\\). Any vector from this space, when added with another vector from the same space gives a resultant vector, which will also be part of the same space(\\(ℝ^2\\)). Hence it is called the space of all vectors of size 2.\nA vector space can be defined if a set of vectors \\(V\\) and two operations \\(*, +\\) which can be defined in any way, and for our purposes we consider the operatins as general addition and multiplication, must follow the following rules.\n\nThere must be an element 0, where \\(x+0=x, x \\in R^2\\)\nThe vector \\(\\alpha x\\) must belong to \\(ℝ^2\\)\nTwo vectors \\(x,y \\in ℝ^2\\) when added \\(x + y\\) must belong to \\(ℝ^2\\)\n\nQuestion 8\nA subset of a vector space which in itself is a vector space is caled a subspace. Given an example of a subspace of \\(ℝ^2\\).\nThe subspaces of \\(ℝ^2\\) are the zero vector \\((0,0)\\). Any line \\(\\alpha(x,y)\\) and the entire \\(ℝ^2\\) itself.\nQuestion 9\nGiven a vector \\((1,7)\\) what does the set \\({α(1,7)|α∈ℝ}\\) represent? Is it a subspace of \\(ℝ^2\\)?\nThe set of all vectors \\(\\alpha(1,7)\\) represent a line in \\(ℝ^2\\), which is a subspace of \\(ℝ^2\\)\nQuestion 10\nIs \\(ℝ^3\\) a vector space?\nYes, \\(ℝ^3\\) is a subspace as it follows all the 10 axioms of the vector space.\nQuestion 11\nConside the two points \\((1,2,3)\\) and \\((4,5,7)∈ℝ^3\\). What does the following set denote: \\({α(1,2,3)+β(4,5,7)|α,β∈ℝ}\\). Is this a subspace?\nThe following set \\({α(1,2,3)+β(4,5,7)|α,β∈ℝ}\\) denotes the plane in \\(ℝ^3\\). Since both the vectors \\((1,2,3)\\) and \\((4,5,7)\\) are indepent of each other.\nYes, the above plane is a subspace of \\(ℝ^3\\)\nQuestion 53\nSolve the following : \\[\nx−2y=15\n\\] \\[\nx+4y=19\n\\]\n\nIsn’t this the same as \\[\n\\begin{equation}\n\\begin{bmatrix}\n1 & -2\\\\\n1 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\\\\ny\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n15\\\\\n19\n\\end{bmatrix}\n\\end{equation}\n\\]\nYou are trying to find if there is an element in the domain which maps to \\((15,19)\\).\nWhat exactly is happening here ? (Say all that you can)\n\nYes, \\((1)\\) is just the matrix representation of the system of equations which are needed to be solved.\nYes, by representing the equations in the matrix form, we are trying to find the unknown \\(x\\) and \\(y\\) which maps to \\((15,19)\\)"
  },
  {
    "objectID": "01_Mathematics_I/02_Maths_11_Sept.html",
    "href": "01_Mathematics_I/02_Maths_11_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "04 Maths Assignment 11th September\n\nfrom typing import List\nimport random\nimport math\n\n\nBubble Sort\n\ndef bubbleSort(array: List):\n    length = len(array)\n    for i in range(length, 0, -1):\n        noSwap = True\n        for j in range(i-1):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n                noSwap = False\n    if noSwap:\n        return array\n    return array\n\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\n\nassert sorted(randomArray) == bubbleSort(randomArray)\n\n\n\nSelection Sort\n\ndef selectionSort(array:List) -> List:\n    for i in range(len(array)-1):\n        min = i\n        for j in range(i+1,len(array)):\n            if array[j] < array[min]:\n                min = j\n        array[min], array[i] = array[i], array[min]\n    return array\n\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\n\nassert sorted(randomArray) == selectionSort(randomArray)\n\n\n\nLinear Search\n\ndef linearSearch(array: List, numToFind: int) -> int :\n    for idx, element in enumerate(array):\n        if element == numToFind:\n            return idx\n    return -1\n\n\nassert linearSearch(list(range(1000)),465) == 465\n\nprint(linearSearch(random.sample(list(range(1000)), 900), 1456))\nprint(linearSearch(random.sample(list(range(1000)), 900), 156))\n\n-1\n800\n\n\n\n\nBinary Search\n\ndef binarySearch(array:List, numToFind: int) -> int:\n    start = 0\n    end = len(array) - 1\n\n    while start < end:\n        mid = (start+end) // 2\n        \n        if array[mid] == numToFind:\n            return mid\n        elif array[mid] > numToFind:\n            end = mid - 1\n        else:\n            start = mid + 1\n    \n    return -1\n\n\nassert binarySearch(list(range(1000)), 1678) == -1\nassert binarySearch(list(range(1000)), 678) == 678\n\n\n\nTime Taken for Each Algo\n\nsomeArray = list(range(1000))\nsortedArray = someArray.copy()\nrandomArray = random.sample(someArray, 1000)\n\nTime Taken for Bubble Sort\n\n%timeit bubbleSort(randomArray)\n\n22 ms ± 162 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nTime Taken for Selection Sort\n\n%timeit selectionSort(randomArray)\n\n17.7 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nTime Taken for In-Built Sort\n\n%timeit sorted(randomArray)\n\n3.55 µs ± 14.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nIn-Built sort is 15215 times faster than Bubble Sort.\nIn-Built sort is 12957 times faster than Selection Sort.\nTime Taken for Linear Search\n\n%timeit linearSearch(sortedArray, random.choice(someArray))\n\n11.7 µs ± 115 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nTime Taken for Binary Search\n\n%timeit binarySearch(sortedArray, random.choice(someArray))\n\n1.24 µs ± 37.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nTime Taken for In-Built Search\n\n%timeit random.choice(someArray) in sortedArray\n\n3.26 µs ± 47.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%timeit random.choice(someArray) in randomArray\n\n3.32 µs ± 105 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nIn-Built search is fast for both Sorted and Random Array."
  },
  {
    "objectID": "01_Mathematics_I/07_Maths_Project.html",
    "href": "01_Mathematics_I/07_Maths_Project.html",
    "title": "Plaksha",
    "section": "",
    "text": "09 Predict the Leader\n\nImports\n\nfrom typing import List\nimport random\n\nimport csv\nimport matplotlib.pyplot as plt\n\n\n\nMaking the Network from the CSV file\n\ndef make_graph(file_name: str) -> dict:\n    with open(file_name, \"r\") as csv_file:\n        csv_data = csv.reader(csv_file)\n        # Take All Names of Nominations from 1st column.\n        # Since 0th column is Title \"Name\"\n        NOMINATIONS = next(csv_data)[1:]\n        NETWORK = {name: [] for name in NOMINATIONS}\n\n        for row in csv_data:\n            votee = row[0]\n            for idx, isVote in enumerate(row[1:]):\n                if isVote == '1':\n                    NETWORK[NOMINATIONS[idx]].append(votee)\n    return NETWORK\n\n\nNETWORK = make_graph(\"./data/TLP_Leader_Data.csv\")\n\n\nNetwork Test to match values\n\n# Netwrok Check\nassert NETWORK['Abdul Khader, Syed'] == [\"Gupta, Aayush\", \"R K, Vysakh\"], \"Network is INCORRECT\"\n\n\nassert NETWORK['R K, Vysakh'] == ['Gupta, Aayush', 'Gowda, Adarsh', 'Narayan, Anchit', 'Sankar, Kirubananth', \n                                'Bhardwaj, Kunal', 'Walia, Muskaan', 'Krishna K, Pramod', 'Pandey, Savyasachi', \n                                'Pasricha, Stuti', 'Abdul Khader, Syed', 'Kumar Singh, Tejasvi', 'Pani, Tirtha', \n                                'Kumar, Vivek']\n\n\n\n\nRandom Walk for NUMBER_WALKS times\n\npointer = random.choice(list(NETWORK.keys()))\nvisits = {k:0 for k,v in NETWORK.items()}\nNUMBER_WALKS = 40_000\nfor _ in range(NUMBER_WALKS):\n    pointer = random.choice(NETWORK[pointer])\n    visits[pointer] += 1\n\n\n\nPlotting\n\norderedVisits = sorted(visits.items(), key = lambda item: item[1], reverse=True)\nnodes = [key for key,val in orderedVisits]\nvalues = [val/NUMBER_WALKS for key,val in orderedVisits]\n\n# The Adjust the Plot Size\nplt.figure(figsize = (18,12))\n\n# To Have the x-axis label as vertical\nplt.xticks(rotation=90)\n\n# Bar Graph\nplt.bar(nodes, values)\n\n# To plot the number value on each Bar\nfor i, v in enumerate(values):\n    plt.text(i-0.2, v+0.0007, str(round(v*100,2))+\"%\", color='black', fontweight='bold', rotation=90)\n\nplt.show()\n\n\n\n\n\nTop 5 Standings\n\nfor idx, (name, percentageVoting) in enumerate(orderedVisits[:5]):\n    print(f\"{name} has Rank:{idx+1} with the vote percentage of: {round(percentageVoting/NUMBER_WALKS * 100,2)}%\")\n\nGupta, Aayush has Rank:1 with the vote percentage of: 10.21%\nR K, Vysakh has Rank:2 with the vote percentage of: 4.51%\nPasricha, Stuti has Rank:3 with the vote percentage of: 3.57%\nSonnathi, Sumanth has Rank:4 with the vote percentage of: 3.28%\nGupta, Shivanshu has Rank:5 with the vote percentage of: 3.21%"
  },
  {
    "objectID": "01_Mathematics_I/00_LinearAlgebra.html",
    "href": "01_Mathematics_I/00_LinearAlgebra.html",
    "title": "Linear Algebra Questionverse",
    "section": "",
    "text": "1. What is scaling? What is a scalar? What is a vector?\nThe act of either extending/stretching or contracting/squishing something is known as scaling, and the amount it is changed, is given by a number, which is known as a scalar, which denotes that it scales something. This scaling can also change the direction.\nA vector is a list of numbers in some space, where the dimension of space is the number of elements in the vector representation. For example, if the vector is represented by a pair of numbers, than it lies in 2D space, where we can show it as an arrow, with it tail always at the origin and head pointing at the co-ordinates by which the vector is being represented, here the pair of numbers.\n\n2. How do you add two vectors? What is its physical significance?\nTo add two vectors, we do component wise addition of both the vectors, and the requirement to add both the vectors is that they must be in same system, like both the vectors must be represented as pair of numbers, or triplets of numbers, or some n-collection of numbers.\n\\(eg: [a,b,c] + [x,y,z] = [a+x, b+y, c+z]\\)\nThe physical significance of adding two vectors, can be understood as, the combined movement of both the vecors.\nHere the first vector is represented as moving along a units on the x-axis, b units parrallel to y-axis, and finally c units alongs the z-axis. When this is added to the second vector, it is the combined movement along each axis, which is a units on x-axis and than moving x units again on the x-axis, similarly first moving b units on y-axis and further moving y units parallel to y-axis and finally moving c units along z-axis and than moving z units along the z-axis.\nThis entire movement is equal to moving a+x units on x-axis, b+y units parallel to y-axis and finally c+z units along the z-axis.\nHence adding two vectors is equal to just adding the vectors corresponding components.\n\n3. How do you mulitply a scalar with a vector? Think of an application.\nWhen we multiply a scalar with a vector, it either expands/strech out the vector by the scalar(‘number’) it is being multiplied or contracts/squished the vector if the scalar is between 0 and 1.\nAlso, if the vector is multiplied by a negative scalar, the vector change it’s direction by 180°, which means the vector now go in opposite direction and also is extends or contracts by the scalar value which is being multiplied.\nIn mathematical terms, scalar multiplication is just multiplying each component of the vector with the scalar value.\n\nThe scalar multiplication can be used to estimate one variable as scalar multiplication of other variable. For example if we have a vector of weights of a person, and if we know the relation between weight and height of a person(where \\(height = \\alpha * weight\\)), than we can represent the height of all persons as the scalar multiplication of the weight vector with \\(\\alpha\\)."
  },
  {
    "objectID": "01_Mathematics_I/00_LinearAlgebra.html#chapter-2",
    "href": "01_Mathematics_I/00_LinearAlgebra.html#chapter-2",
    "title": "Linear Algebra Questionverse",
    "section": "Chapter 2",
    "text": "Chapter 2\n4. What is \\(\\hat{i}\\) vector and a \\(\\hat{j}\\) vector. Why are they important?\nThe \\(\\hat{i}\\) vector is a unit vector, having the length 1, along the x-axis and similarly \\(\\hat{j}\\) is a unit vector in the y-direction.\nSo, now with this two vectors, we can represent any vector (x,y), by thinking as the x-coordinate of the vector, that scales the \\(\\hat{i}\\), stretching/contracting by a factor of x. And similarly the y-cordinate of the vector as the scalar that scales the \\(\\hat{j}\\) vector by a factor of y.\nAnd the final vector is the resultant of adding those two sclaed vectors. In this sense, the vectors these co-ordinate (x,y) desctibe is the sum of two scaled vectors \\(\\hat{i}\\) and \\(\\hat{j}\\).\n\n5. Why not consider any other basis vectors instead of the standard unit vectors?\nThe standard unit vectors can be used to easily generate any vector in its vector space, where the first co-ordinate of the vector is just the scalar multlication of the first standard basis vector(x-axis), and second co-ordinate is scalar multiplication of second standard basis vector and so on.\nAny vector can be represented by any set of random basis vectors, but the linear combinations in the cartesian system will be difficult to identify, since those random basis vectors are itself having some x and y co-ordinates value, which directly multiplying with scalar doesn’t give the co-ordinates of the new vector.\n6. How can one get any vector from the basis vectors? What is the intuition?\nSince, the basis vectors are linearly independent and the linear combinations of the basis vectors span the entire the space, we can generate any vector."
  },
  {
    "objectID": "01_Mathematics_I/04_Maths_12_Sept_12_24.html",
    "href": "01_Mathematics_I/04_Maths_12_Sept_12_24.html",
    "title": "Plaksha",
    "section": "",
    "text": "06 Maths Assignment 12th September - 12th to 24th Question\nQuestion 12\nConsider a straight line \\(y=2x+1\\) in \\(ℝ^2\\), does it form a subspace of \\(ℝ^2\\)?\nNo, the straight line doesn’t pass through the origin, which mean any point on the line, when multiplied with \\(0\\), gives \\((0,0)\\) which doesn’t lie on the line, not making it a subspace of \\(ℝ^2\\)\n\nQuestion 13\nConsider a unit circle in \\(ℝ^2\\), centered at origin, is it a subspace of \\(ℝ^2\\)?\nNo, a unit circle in \\(ℝ^2\\) which is centered at origin, is not a subspace in \\(ℝ^2\\). Since, two points in the circle, \\((0,1)\\) and \\((1,0)\\) when added together, goes beyond the circle.\n\nQuestion 14\nWhat are all the subspaces of \\(ℝ^2\\)?\nAll the subspaces of the vector space \\(ℝ^2\\) are:\n\nOrigin - \\((0,0)\\)\nAny Line passing through origin. \\(\\alpha x = 0\\) where \\(x\\in ℝ^2\\) and \\(\\alpha \\in ℝ\\)\n\nQuestion 15\nWhat are all the subspaces of ℝ3?\nAll the subspaces of the vector space \\(ℝ^2\\) are:\n\nOrigin - \\((0,0)\\)\nAny Line passing through the origin. \\(\\alpha x\\) where \\(x \\in ℝ^3\\) and \\(\\alpha \\in ℝ\\)\nAny Plane passing through the origin. \\(\\alpha x + \\beta y\\) where \\(x,y \\in ℝ^3\\) and \\(\\alpha, \\beta \\in ℝ\\)\n\nQuestion 16\nGiven \\(ℝ^3\\), pick any two points \\(u,v∈ℝ^3\\). Note that \\({αu+βv | α,β∈ℝ}\\) is a subspace of \\(ℝ^3\\). Generalize this idea!\nAll the possible linear combinations of two points in \\(ℝ^3\\) is the linear span which is the subspace of \\(ℝ^3\\).\nGeneralizing it, for \\(ℝ^n\\), we have to prove the linear combinations of \\((n-1)\\) points in this \\(ℝ^n\\) is the subspace of \\(ℝ^n\\)\n\\(S = \\{u_1, u_2, u_3, ...., u_{n-1}\\}\\)\n\\(L(S) =\\) is all the possible linear combinations of S, which is known as Linear Span of S\nNow, we have to prove that \\(L(S)\\) is a subspace of \\(ℝ^n\\).\n\\[\nx = \\alpha_1 u_1 + \\alpha_2 u_2 + \\alpha_3 u_3 + ... + \\alpha_{n-1} u_{n-1}\n\\]\n\\[\ny = \\beta_1 u_1 + \\beta_2 u_2 + \\beta_3 u_3 + ... + \\beta_{n-1} u_{n-1}\n\\]\n\\[\nx,y \\in L(S)\n\\]\nNow, to prove that \\(L(S)\\) is a subspace of \\(ℝ^n\\), we have to prove two following conditions:\n\\[\nx+y = z \\in L(S)\n\\]\n\\[\n\\lambda x \\in L(S)\n\\]\nProof:\n\\[\nx + y = (\\alpha_1 u_1 + \\beta_1 u_1) + (\\alpha_2 u_2 + \\beta_2 u_2) + ... + (\\alpha_{n-1} u_{n-1} + \\beta_{n-1} u_{n-1})\n\\]\n\\[\nx + y = (\\alpha_1 + \\beta_1)u_1 + (\\alpha_1 + \\beta_1)u_2 + ... + (\\alpha_{n-1} + \\beta_{n-1})u_{n-1}\n\\] Since, \\(L(S)\\) contains all possible combinations, the above \\(x+y \\in L(S)\\)\n\\[\n\\lambda x = \\lambda \\alpha_1 u_1 + \\lambda \\alpha_2 u_2 + ... + \\lambda \\alpha_{n-1} u_{n-1}\n\\] Again, since \\(L(S)\\) contains all possible combinations, the above \\(\\lambda x \\in L(S)\\)\nQuestion 17\nThe set \\({αu+βv|α,β∈ℝ}\\) is called the linear combination of vectors \\(u\\) and \\(v\\). We can generalize this to \\(k\\) vectors. Observe what this set is all about?\nThe entire set when generalized to \\(k\\) vectors, is the linear span of \\(k\\) vectors, which is the subspace.\nQuestion 18\nWe say that a vector \\(w\\) is manufactured by \\(u\\) and \\(v\\) if \\(w∈{αu+βv|α,β∈ℝ}\\).\nYes, when we do \\(w∈{αu+βv|α,β∈ℝ}\\), we get all possible vectors in \\(ℝ^2\\), so we can manufacture any \\(w\\) by the linear combinations of \\(u\\) and \\(v\\)\nQuestion 19\nShow that \\((1,2,3)\\) and \\((4,5,6)\\) can manufacture \\((7,8,9)\\). Also \\((4,5,6)\\) and \\((7,8,9)\\) can manufacture \\((1,2,3)\\). Finally \\((4,5,6)\\) can be manufactured by the other two vectors.\nFor 1st case \\(\\alpha = -1\\) and \\(\\beta = 2\\)\nFor 2nd case \\(\\alpha = 2\\) and \\(\\beta = -1\\)\nFor 3rd case \\(\\alpha = 1/2\\) and \\(\\beta = 1/2\\)\nQuestion 20\nCan \\((2,1,0)\\) and \\((3,0,8)\\) manufacture \\((1,1,1)\\) ?\nNo, since we have inconsistent system, where \\[\n2x + 3y = 1\n\\]\n\\[\nx = 1\n\\]\n\\[\n8y = 1\n\\]\nAll the three equations doesn’t satisfy simultaneously.\nQuestion 21\nCan \\((0,0,1)\\) and \\((0,1,0)\\) manufacture \\((1,0,0)\\)?\nNo, since \\((0,0,1)\\) and \\((0,1,0)\\) is the \\(yz\\) plane, we can never get \\((1,0,0)\\)\nQuestion 22\nWhen can two vectors in \\(ℝ^3\\) manufacture a given third vector?\nIf we have the third vector in the span of the first two vector, it can be manufactured.\nQuestion 23\nWhen can two vectors in \\(ℝ^3\\) fail to manufacture a given third vector?\nIf the third vector doesn’t lie in the span of the first two vectors, it can’t be manufactured\nQuestion 24\nIf \\(\\{u,v,w\\}\\) are such that a vector in this set can be manufactured by some vectors in the same set, then such a set is called a linearly dependent set.\nYes, since one vector is the combination of the other vectors, and add NO extra value, which means it is dependent on other vectors, hence the set is linearly dependent set"
  },
  {
    "objectID": "01_Mathematics_I/05_Maths_13_Sept.html",
    "href": "01_Mathematics_I/05_Maths_13_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "07 Maths Assignment 13th Sept\n\n\\[\\begin{equation}\nMarkov =\n\\begin{bmatrix}\n0.5 & 0.1 & 0.2\\\\\n0.3 & 0.6 & 0.4\\\\\n0.2 & 0.3 & 0.4\n\\end{bmatrix}\n\\end{equation}\\]\nWe know that the Highest Eigen Value \\(\\lambda\\) is 1, which converges the Markov matrix towards the highest eigen value corresponding vector \\(V_{1}\\) So our final probability, which the Markov matrix converges to after many simulations is \\([v_1, v_2, v_3]\\)\n$$ \\[\\begin{equation}\n\\begin{bmatrix}\n0.5 & 0.1 & 0.2\\\\\n0.3 & 0.6 & 0.4\\\\\n0.2 & 0.3 & 0.4\n\\end{bmatrix}\n\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3\n\\end{bmatrix}\n\n=\n\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3\n\\end{bmatrix}\n\\end{equation}\\] $$\nNow we have \\[\n\\begin{equation}\n    0.5v_1 + 0.1v_2 + 0.2v_3 = v_1 \\tag{1}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n    0.3v_1 + 0.6v_2 + 0.4v_3 = v_2 \\tag{2}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n0.2v_1 + 0.3v_2 + 0.4v_3 = v_3 \\tag{3}\n\\end{equation}\n\\]\nAlso, we know the values of the principal eigen vector, are probabilities of the states, so the sum of these values must be 1.\n\\[\n\\begin{equation}\n    v_1 + v_2 + v_3 = 1 \\tag{4}\n\\end{equation}\n\\]\nSolving \\((1)\\), \\((2)\\) and \\((4)\\) we get\n\\[\nv_1 = 0.2181\n\\] \\[\nv_2 = 0.4727\n\\] \\[\nv_3 = 0.3090\n\\]\nNow, we start with 100 people at Hostel, 100 people at Plaksha and 100 people at Sector-17\n\n.2181*300, 0.4727*300, 0.3090*300\n\n(65.42999999999999, 141.81, 92.7)\n\n\nAfter the convergence, the state the system is in:\nPeople at Hostel: \\(0.2181*300 = 65.5\\)\nPeople at Plaksha: \\(0.4727*300 = 141.8\\)\nPeople at Sector-17: \\(0.3090*300 = 92.7\\)"
  },
  {
    "objectID": "00_Implementations/01_BackPropogation/main.html",
    "href": "00_Implementations/01_BackPropogation/main.html",
    "title": "Plaksha",
    "section": "",
    "text": "from typing import List, Union\nimport math\nimport numpy as np\n\n\nclass Network():\n    def __init__(self,\n                    input_size: int,\n                    num_layers: int, \n                    units: Union[int, List[int]],\n                    output_size: int\n                    ):\n        '''\n        The class creates a Feed Forward Fully connected Network\n\n        Arguments:\n            input_size: int -> The number of neurons in the input layer\n            num_layers: int -> The number of hidden layers in the network\n            units: Union[int, List[int]] -> If it's `int`, all layers have same size\n                                            If it's list of ints, length should be \n                                            equal to number of layers\n            output_size: int -> The number of neurons in the output layer\n        '''\n        self.num_layers = num_layers\n        units = [units]*num_layers if type(units) == int else units\n        self.units = [input_size] + units + [output_size]\n\n        self.layers = []\n        for i in range(self.num_layers + 1):\n            # Weight Matrix Transpose\n            self.layers.append( [ [0]*self.units[i] ]*self.units[i+1] )\n    \n    def _sigmoid(self, input: List[float]):\n        output = []\n        for val in input:\n            output.append( 1/(1+math.e**(-val)) )\n        return output\n\n    def forward(self, input: List):\n        x = input.copy()\n        for layer in self.layers:\n            new_x = []\n            for weights in layer:\n                val = 0\n                for idx in range(len(weights)):\n                    val += x[idx]*weights[idx]\n                new_x.append(val)\n            x = self._sigmoid(new_x)\n        \n        return x\n\n    def backward(self):\n        \n\n\nnet = Network(5, 4, 3, 1)\n\n\nnet.layers\n\n[[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],\n [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n [[0, 0, 0]]]\n\n\n\nnet.forward([1,1,1,1,1])\n\n[0.5]\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass pyNet(nn.Module):\n    \n    def __init__(self):\n        super(pyNet, self).__init__()\n        self.linear1 = nn.Linear(5,3,bias=False)\n        self.linear1.weight.data.fill_(0.0)\n\n        self.linear2 = nn.Linear(3,3,bias=False)\n        self.linear2.weight.data.fill_(0.0)\n\n        self.linear3 = nn.Linear(3,3,bias=False)\n        self.linear3.weight.data.fill_(0.0)\n\n        self.linear4 = nn.Linear(3,1,bias=False)\n        self.linear4.weight.data.fill_(0.0)\n\n\n    def forward(self, x):\n\n        x = torch.sigmoid(self.linear1(x))\n        x = torch.sigmoid(self.linear2(x))\n        x = torch.sigmoid(self.linear3(x))\n        x = torch.sigmoid(self.linear4(x))\n        \n        return x\n\n\ntorchNet = pyNet()\nprint(torchNet(torch.tensor([1.,1.,1.,1.,1.])))\n\ntensor([0.5000], grad_fn=<SigmoidBackward0>)"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html",
    "href": "02_Python_Programming/00_Assignment.html",
    "title": "01 Programming Assignment",
    "section": "",
    "text": "import math\nfrom typing import List\nimport random\nimport numpy as np"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html#chapter-01",
    "href": "02_Python_Programming/00_Assignment.html#chapter-01",
    "title": "01 Programming Assignment",
    "section": "Chapter 01",
    "text": "Chapter 01\nQuestion 1\nPrint the following using four print statements:\n*\n**\n***\n****\n\nprint(\"*\")\nprint(\"**\")\nprint(\"***\")\nprint(\"****\")\n\n*\n**\n***\n****\n\n\nQuesiton 2:\nFigure out how to input a number and display it using print statement.\n\nn = 5 #int(input(\"Enter the Number of Lines to be printed\"))\nfor i in range(1,n+1):\n    print(\"*\"*i)\n\n*\n**\n***\n****\n*****\n\n\nQuestion 3\nUnderstand how to use a if loop in python. Ask the user to enter a number and check if the number is even or odd.\n\nnum = 15 # int(input(\"Enter the Number\"))\nif num % 2 == 0 : print(\"Even\")\nelse: print(\"Odd\")\n\nOdd\n\n\nQuestion 4:\nPrint a sequence of numbers starting from the number a with common difference d. Go on till you reach the number b.\nEnter a value for a: 10\nEnter a value for d: 3\nEnter a value for b: 20\nOutput: 10 13 16 19\n\na = 10 # int(input(\"Enter value for a:\"))\nd = 3 # int(input(\"Enter value for d:\"))\nb = 20 # int(input(\"Enter value for b:\"))\n\nfor i in range(a,b,d):\n    print(i)\n\n10\n13\n16\n19\n\n\nQuestion 5:\nInput integers i and j and create a list comprising of all prime numbers between them.\n\nm = 12 # int(input('Enter the First Number'))\nn = 88 # int(input('Enter the Second Number'))\n\nprimeList = []\nfor i in range(m,n+1):\n    if i in [0,1]:\n        break\n    prime = True\n    for j in range(2,int(math.sqrt(i))+1):\n        if i%j == 0:\n            prime = False\n            break\n    if prime: primeList.append(i)\n\nprint(primeList)\n\n[13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83]\n\n\nQuestion 6:\nObserve the following code :\nWhat does the following code do:\nprint(\"Enter a number\")\nk=input()\nk=int(k)\nfor i in range(k):\n   for j in range(i):\n   print(\"*\",end=' ')\nprint(\"\")\nSomething is wrong in this code. Fix it!\n\nprint(\"Enter a number\")\nk= 8 # input()\nk=int(k)\nfor i in range(k):\n   for j in range(i):\n    print(\"*\",end=' ')\nprint(\"\")\n\nEnter a number\n* * * * * * * * * * * * * * * * * * * * * * * * * * * * \n\n\nQuestion 7:\nWrite a script to print the following:\nInput a number: 5\nOutput: \n    *\n   ***\n  *****\n *******\n*********\n\nn = 5 # int(input(\"Enter the Number of Lines\"))\n\nfor i in range(1,n+1):\n    m = (n-i) \n    print(\" \"*m + \"*\"*(2*i - 1))# + \" \"*m)\n\n    *\n   ***\n  *****\n *******\n*********\n\n\nQuestion 8:\nWrite a script to print the following zig zag:\n*\n *\n  *\n   *\n    *\nInput parameters: number of lines, inclination\nNote: Input parameters are self-explanatory, inclination is a value between 0 and 1. Lesser the inclination, more vertical the stick.\n\nnumLines = 8 # int(input(\"Enter Number of Lines\"))\ninclination = 0.2 # float(input(\"Inclination of Line\"))\n\nif inclination == 1:\n    print(\"*\"*numLines)\nelse:\n    for i in range(numLines+1):\n        print(\" \"*int(10*inclination*i) + \"*\")\n\n*\n  *\n    *\n      *\n        *\n          *\n            *\n              *\n                *\n\n\n\nAssignment 01\nImplement the following patter as in this video\n\nx = 20\na = -0.2\n\nfor i in range(-x//2, x//2):\n        spaces = int(4*i*i*a)\n        if a > 0:\n            print(\" \"*spaces + \"*\")\n        if a < 0:\n            max_spaces = int( 4 * (x//2)**2 * abs(a) )\n            spaces = -1 * spaces\n            print(\" \" * ( max_spaces - spaces) + \"*\")\n\n*\n                *\n                             *\n                                         *\n                                                    *\n                                                            *\n                                                                    *\n                                                                         *\n                                                                             *\n                                                                                *\n                                                                                *\n                                                                                *\n                                                                             *\n                                                                         *\n                                                                    *\n                                                            *\n                                                    *\n                                         *\n                             *\n                *\n\n\nExtra Question\nFind the Square Root of a Number, using an approach of guessing and moving towards the answer.\n\nUsing Binary Search\n\n\ndef sqrtBinary(number, start, end, numDecimal):\n    mid = (start+end)/2\n    midSq = mid**2\n    if math.isclose(number, midSq, abs_tol=10e-100):\n        return mid\n    elif number - midSq > 0:\n        return sqrtBinary(number, mid, end, numDecimal)\n    else:\n        return sqrtBinary(number, start, mid, numDecimal)\n\ndef sqrtBinary(number, start, end, numDecimal):\n    mid = (start+end)/2\n    midSq = mid**2\n    if math.isclose(number, midSq, abs_tol=10e-100):\n        return mid\n    elif number - midSq > 0:\n        return sqrtBinary(number, mid, end, numDecimal)\n    else:\n        return sqrtBinary(number, start, mid, numDecimal)\n\nnum = 10\nstart = 0\nend = num//2\nnumDecimal = 5\nprint(\"Square Root using our function:\", sqrtBinary(num, start, end, numDecimal))\nprint(\"Square Root using math.sqrt:\", math.sqrt(num))\n\n# test_close(sqrtBinary(num, start, end, numDecimal), math.sqrt(num), eps = 0.000_000_001)\n\nSquare Root using our function: 3.162277659866959\nSquare Root using math.sqrt: 3.1622776601683795"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html#chapter-02",
    "href": "02_Python_Programming/00_Assignment.html#chapter-02",
    "title": "01 Programming Assignment",
    "section": "Chapter 02",
    "text": "Chapter 02"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html#bubble-sort",
    "href": "02_Python_Programming/00_Assignment.html#bubble-sort",
    "title": "01 Programming Assignment",
    "section": "Bubble Sort",
    "text": "Bubble Sort\n\ndef bubbleSort(array: List):\n    length = len(array)\n    for i in range(length, 0, -1):\n        noSwap = True\n        for j in range(i-1):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n                noSwap = False\n    if noSwap:\n        return array\n    return array\n\ndef bubbleSort(array: List):\n    length = len(array)\n    for i in range(length, 0, -1):\n        noSwap = True\n        for j in range(i-1):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n                noSwap = False\n    if noSwap:\n        return array\n    return array\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\n\n# test_eq(sorted(randomArray), bubbleSort(randomArray))\n\n\ndef findKth(array:List, k: int):\n    return bubbleSort(array)[k-1]   \n\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\nfindKth(randomArray, 7)\n\n40"
  },
  {
    "objectID": "02_Python_Programming/Programming_Assignment_4.html",
    "href": "02_Python_Programming/Programming_Assignment_4.html",
    "title": "03 Programming Assignment 4 - File Handling",
    "section": "",
    "text": "import random\nimport matplotlib.pyplot as plt\nfrom QuickSort import quickSort\n\nQuestion\nAll possible English words are available to download here. Save this to a file and read from this file, randomzize the order and write it back to the file.\nTip: use wget http://www.mieliestronk.com/corncob_lowercase.txt to download\n\nwith open('./data/corncob_lowercase.txt', 'r') as english_file:\n    all_words = english_file.read()\n\nall_words = all_words.split('\\n')\n\nassert not '' in all_words\n\n\nrandom.shuffle(all_words)\n\nrandomized_string = '\\n'.join(all_words)\nwith open('./data/corncob_lowercase.txt', 'w') as randomized_file:\n    randomized_file.write(randomized_string)\n\nQuestion\nConsider the list of randomized words as created in the previous program, sort it and write it back to the file.\n\nwith open('./data/corncob_lowercase.txt', 'r') as randomized_file:\n    toSort = randomized_file.read().split('\\n')\n\n\ntoSort = quickSort(toSort, 0, len(toSort)-1)\n\n\nsorted_words = '\\n'.join(toSort)\nwith open('./data/corncob_lowercase.txt', 'w') as sorted_file:\n    sorted_file.write(sorted_words)\n\n\nAssignment\nDownload this to your hard-disk and sort it. (Note: Any complicated sorting technique will not help. You are not allowed to browse the internet for the code or for any other hint. You cannot discuss with your neighbors. Solve this from first principles.)\nthis file: DONT OPEN https://sudarshansudarshan.github.io/plakshaprog/files/tosort.txt\nThe file has 10 Million rows, freezing the computer.\nTip: download using wget or request.get().\n\n# !wget https://sudarshansudarshan.github.io/plakshaprog/files/tosort.txt\n\n\nwith open('./data/tosort.txt', 'r') as largeLargeFile:\n    largeText = largeLargeFile.read().split('\\n')\n# To remove empty line at end\nlargeText.pop()\nlen(largeText)\n\n10000000\n\n\n\n# largeText = quickSort(largeText, 0, len(largeText)-1)\n\nSince sorting it gives recursion, error. Let’s usefirst principles.\nLet’s count the number of character present in the file, and since we know the inherent ordering of english alphabets. We can than just write to the file the same number of alphabet characters in order.\n\nalphabet_dict = {}\nfor alpha in largeText:\n    alphabet_dict[alpha] = alphabet_dict.get(alpha, 0) + 1\n\n\nalphabet_dict = sorted(alphabet_dict.items(), key=lambda item: item[0])\nalphabet_dict\n\n[('a', 385058),\n ('b', 385520),\n ('c', 385394),\n ('d', 384992),\n ('e', 383359),\n ('f', 384207),\n ('g', 385701),\n ('h', 384611),\n ('i', 383741),\n ('j', 384317),\n ('k', 383037),\n ('l', 384399),\n ('m', 385140),\n ('n', 383715),\n ('o', 384455),\n ('p', 385395),\n ('q', 385536),\n ('r', 385217),\n ('s', 384427),\n ('t', 385128),\n ('u', 383785),\n ('v', 384986),\n ('w', 384205),\n ('x', 384219),\n ('y', 384845),\n ('z', 384611)]\n\n\n\n# Runs the first loop, and gets alpha,it's count value\n# Than run the second loop for each alpha,value pair of first loop\n# In second loop it runs for value times, and add alpha each time to list\nsortedList = [alpha for alpha,value in alphabet_dict for _ in range(value)]\nlen(sortedList)\n\n10000000\n\n\n\nsorted_words = '\\n'.join(sortedList)\nwith open('./data/sorted.txt', 'w') as sortedFile:\n    sortedFile.write(sorted_words)"
  },
  {
    "objectID": "02_Python_Programming/index.html",
    "href": "02_Python_Programming/index.html",
    "title": "Introduction to Prgramming",
    "section": "",
    "text": "Assignemnt 1-3\nAssignemnt 4\nMedian"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Plaksha",
    "section": "",
    "text": "import math\n\n\nHY = -(6/10)*math.log(6/10, 2) - (4/10)*math.log(4/10, 2)\nHY\n\n0.9709505944546686\n\n\n\nq = 3/7\nw = 1/3\na = -(q)*math.log(q, 2) - (1-q)*math.log(1-q, 2)\nb = -(w)*math.log(w, 2) - (1-w)*math.log(1-w, 2)\na,b\n\n(0.9852281360342516, 0.9182958340544896)\n\n\n\nq = 7/10\na = (q) * a\nb = (1-q) * b\na,b, a+b\n\n(0.6896596952239761, 0.2754887502163469, 0.965148445440323)\n\n\n\nHY - (a+b)\n\n0.0058021490143456145\n\n\n\n6/10\n\n0.6\n\n\n\n3/7 * (-(1/3)*math.log(1/3, 2) - (2/3)*math.log(2/3, 2))\n\n0.39355535745192405\n\n\n\n-(3/7)*math.log(3/7, 2) - (4/7*math.log(4/7, 2)) - (4/7*(-(3/4)*math.log(3/4, 2) - (1/4)*math.log(1/4, 2)))\n\n0.5216406363433186\n\n\n\n(-(3/7)*math.log(3/7, 2) - (4/7*math.log(4/7, 2)) ) - ((3/7 * (-(1/3)*math.log(1/3, 2) - (2/3)*math.log(2/3, 2))) + (4/7*(-(1/2)*math.log(1/2, 2) - (1/2)*math.log(1/2, 2))))\n\n0.02024420715375619\n\n\n\n0.98-0.46\n\n0.52\n\n\n\nimport time\n\n\nimport matplotlib.pyplot as plt\n\n\ntimelist = []\nfor i in range(10_000):\n    l = list(range(i))\n    a = time.process_time()\n    for i in range(100):\n        print(l)\n    timelist.append(time.process_time() - a)"
  },
  {
    "objectID": "05_Mathematics_II/02_PCA.html",
    "href": "05_Mathematics_II/02_PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\n\nExercise 1\n\nDetermine PCA of a 3x2 matrix\n\ndefine a matrix\n\nA = np.array([[1, 2], [3, 4], [5, 6]])\nprint(A)\n\n[[1 2]\n [3 4]\n [5 6]]\n\n\n\n\nFirst do it manually!\n\nX = A[:,0]\nY = A[:,1]\nprint(X, Y)\n\n[1 3 5] [2 4 6]\n\n\n\n\n1. Subtract the mean of each variable\n\nmean_sub_X = X - np.mean(X)\nmean_sub_Y = Y - np.mean(Y)\n\nmean_matrix = A - np.mean(A, axis=0)\nprint(mean_sub_X, mean_sub_Y)\nprint(mean_matrix)\n\n[-2.  0.  2.] [-2.  0.  2.]\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\n\n\n\n\n2. Calculate the Covariance Matrix\n\nvar_X = np.sum(mean_sub_X**2)/(len(X)-1)\nvar_Y = np.sum(mean_sub_Y**2)/(len(Y)-1)\ncov_XY = np.sum(mean_sub_X * mean_sub_Y)/(len(X)-1)\n\ncov_matrix = np.array([\n    [var_X, cov_XY],\n    [cov_XY, var_Y]\n])\nprint(cov_matrix)\n\n[[4. 4.]\n [4. 4.]]\n\n\n\n\n3. Compute the Eigenvalues and Eigenvectors\n\neig_values, eig_vectors = np.linalg.eig(cov_matrix)\n\nprint(eig_values)\nprint(eig_vectors)\n\n[8. 0.]\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n\n\n\n\n4. project data of the original matrix to the new basis\n\nprin_eig_vector = eig_vectors[:,0].reshape(-1,1)\nprojected_A = np.matmul(mean_matrix, prin_eig_vector)\n\nprint(projected_A)\n\n[[-2.82842712]\n [ 0.        ]\n [ 2.82842712]]\n\n\n\nprojected_matrix = np.matmul(mean_matrix, eig_vectors)\n\nprint(projected_matrix)\n\n[[-2.82842712  0.        ]\n [ 0.          0.        ]\n [ 2.82842712  0.        ]]\n\n\n\n\nconclusion?\nThe Eigen values are 8 and 0 for two columns respectively, since the second eigen value is 0, there is no extra information in the second column aftert transformation, and hence can be dropped after performing the PCA transformation. And can consider only the first column.\n\n\n\n\nExercise 2\n\nOk Let’s do it again but for a larger matrix 20x5\n\nGenerate a dummy dataset.\n\nX = np.random.randint(10,50,100).reshape(20,5)\nprint(X)\n\n[[42 21 35 14 10]\n [20 41 26 46 31]\n [36 37 19 24 20]\n [37 14 14 17 12]\n [42 14 38 17 29]\n [46 35 27 42 35]\n [19 34 32 12 49]\n [37 19 30 22 39]\n [14 46 34 40 43]\n [13 10 44 25 33]\n [30 48 27 24 36]\n [12 39 45 49 24]\n [11 36 34 10 14]\n [14 36 15 38 44]\n [40 29 13 15 42]\n [44 39 17 21 26]\n [17 20 26 14 48]\n [38 34 48 10 40]\n [39 37 42 48 14]\n [42 27 28 18 44]]\n\n\n\n\n1. Subtract the mean of each variable\nSubtract the mean of each variable from the dataset so that the dataset should be centered on the origin. Doing this proves to be very helpful when calculating the covariance matrix.\n\ncentral_X = X - np.mean(X, axis = 0)\n\n\n\n2. Calculate the Covariance Matrix\nCalculate the Covariance Matrix of the mean-centered data.\n\ncov_mat = np.cov(central_X, rowvar= False)\nprint(cov_mat)\n\n[[165.08157895 -26.23157895 -28.11052632 -37.31052632 -33.65526316]\n [-26.23157895 121.11578947  -7.32631579  66.95789474  13.08421053]\n [-28.11052632  -7.32631579 111.90526316  16.56842105 -12.26842105]\n [-37.31052632  66.95789474  16.56842105 178.53684211 -12.99473684]\n [-33.65526316  13.08421053 -12.26842105 -12.99473684 156.66052632]]\n\n\nNote: the matrix is symmetrical\n\n\n3. Compute the Eigenvalues and Eigenvectors\nNow, compute the Eigenvalues and Eigenvectors for the calculated Covariance matrix.\n\neig_values, eig_vectors = np.linalg.eig(cov_mat)\n# The above Eigen Vectors is column wise for each eigen value\n# Converting it to Row wise\neig_vectors = eig_vectors.T\nprint(eig_values)\nprint(eig_vectors)\n\n[253.01066884 181.35485507 141.1469473   88.82152611  68.96600268]\n[[-0.52753037  0.45832807  0.15014975  0.68623713  0.13483595]\n [ 0.43675002  0.09465772  0.05213214  0.41635417 -0.79007644]\n [-0.49880679 -0.41149083  0.64073475 -0.17512103 -0.37504492]\n [-0.52633992  0.11431773 -0.66767148 -0.24624489 -0.45108313]\n [-0.0715486  -0.77368084 -0.34410909  0.51422163  0.11603365]]\n\n\nNote: The Eigenvectors of the Covariance matrix we get are Orthogonal to each other and each vector represents a principal axis. A Higher Eigenvalue corresponds to a higher variability. Hence the principal axis with the higher Eigenvalue will be an axis capturing higher variability in the data.\n\n\n4. Sort Eigenvalues in descending order\nSort the Eigenvalues in the descending order along with their corresponding Eigenvector.\n\ndesc_order = np.argsort(eig_values)[::-1]\neig_values = eig_values[desc_order]\neig_vectors = eig_vectors[desc_order]\n\nprint(eig_values)\nprint(eig_vectors)\n\n[253.01066884 181.35485507 141.1469473   88.82152611  68.96600268]\n[[-0.52753037  0.45832807  0.15014975  0.68623713  0.13483595]\n [ 0.43675002  0.09465772  0.05213214  0.41635417 -0.79007644]\n [-0.49880679 -0.41149083  0.64073475 -0.17512103 -0.37504492]\n [-0.52633992  0.11431773 -0.66767148 -0.24624489 -0.45108313]\n [-0.0715486  -0.77368084 -0.34410909  0.51422163  0.11603365]]\n\n\nNote: Each column in the Eigen vector-matrix corresponds to a principal component, so arranging them in descending order of their Eigenvalue will automatically arrange the principal component in descending order of their variability. Hence the first column in our rearranged Eigen vector-matrix will be a principal component that captures the highest variability.\n\n\n5. Select a subset from the rearranged Eigenvalue matrix\nSelect a subset of n first eigenvectors from the rearranged Eigenvector matrix as per our need, n is desired dimension of your final reduced data. i.e. “n_components=2” means you selected the first two principal components.\n\nn_components = 2\n\ndesired_eig_vectors = eig_vectors[:2]\nprint(desired_eig_vectors)\n\n[[-0.52753037  0.45832807  0.15014975  0.68623713  0.13483595]\n [ 0.43675002  0.09465772  0.05213214  0.41635417 -0.79007644]]\n\n\nNote: The final dimensions of X_reduced will be ( 20, 2 ) and originally the data was of higher dimensions ( 20, 5 ).\n\n\n6. Transform the data\nFinally, transform the data by having a dot product between the Transpose of the Eigenvector subset and the Transpose of the mean-centered data. By transposing the outcome of the dot product, the result we get is the data reduced to lower dimensions from higher dimensions.\n\nprojected_matrix = np.matmul(central_X, desired_eig_vectors.T)\n\nprint(projected_matrix)\n\n[[-2.08844995e+01  1.71428703e+01]\n [ 2.33275256e+01  5.69006310e+00]\n [-4.57773329e+00  1.14655567e+01]\n [-2.22799056e+01  1.28706507e+01]\n [-1.90217522e+01  2.87427279e+00]\n [ 4.80631252e+00  1.17040271e+01]\n [ 6.42642644e-01 -2.34739157e+01]\n [-1.05141129e+01 -5.07223941e+00]\n [ 2.74861549e+01 -8.01913353e+00]\n [ 1.37345567e+00 -9.68678831e+00]\n [ 6.98763100e+00 -2.33787436e+00]\n [ 3.05988175e+01  9.77685579e+00]\n [-1.18913037e-02  1.45630833e-01]\n [ 1.88123905e+01 -1.15790063e+01]\n [-1.44651211e+01 -8.98636706e+00]\n [-9.43131532e+00  9.05508688e+00]\n [-4.38214982e+00 -2.43626320e+01]\n [-9.56403624e+00 -8.06357138e+00]\n [ 1.29537954e+01  2.87078047e+01]\n [-1.18562084e+01 -7.85129066e+00]]\n\n\n\n\n\n\nExercise 3\n\nNow, let’s just combine everything above by making a function and try our Principal Component analysis from scratch on an example.\n\nCreate a PCA function accepting data matrix and the number of components as input arguments.\n\ndef PCA(data, num_components):\n    # Finding the Centralised Matrix (Matrix - Column Mean)\n    central_X = data - np.mean(data, axis = 0)\n\n    # Finding the Co-Variance Matrix\n    cov_mat = np.cov(central_X, rowvar= False)\n\n    # Finding the Eigen Values and Eigen Vectors\n    eig_values, eig_vectors = np.linalg.eig(cov_mat)\n    # Eigen vectors are column wise, making them row wise\n    eig_vectors = eig_vectors.T\n\n    # Taking the Descinding order of Eigen Values\n    # and their corresponding Eigen Vectors\n    desc_order = np.argsort(eig_values)[::-1]\n    eig_values = eig_values[desc_order]\n    eig_vectors = eig_vectors[desc_order]\n\n    # Desired Eigen Vectors\n    desired_eig_vectors = eig_vectors[:num_components]\n\n    projected_matrix = np.matmul(central_X, desired_eig_vectors.T)\n\n    return projected_matrix, eig_values\n\n\n\nLet’s use the IRIS dataset to test our PCA function, and by the same way see if we can classify the dataset in the projected space\n\n#Get the IRIS dataset\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\ndata = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\ndata.head()\n\n\n\n\n\n  \n    \n      \n      sepal length\n      sepal width\n      petal length\n      petal width\n      target\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n    \n  \n\n\n\n\n\n\n1. prepare the dataset & target set for classification\n\ntarget = data.iloc[:,-1]\ndata = data.iloc[:,:-1]\nprint(data.head())\nprint(target.head())\n\n   sepal length  sepal width  petal length  petal width\n0           5.1          3.5           1.4          0.2\n1           4.9          3.0           1.4          0.2\n2           4.7          3.2           1.3          0.2\n3           4.6          3.1           1.5          0.2\n4           5.0          3.6           1.4          0.2\n0    Iris-setosa\n1    Iris-setosa\n2    Iris-setosa\n3    Iris-setosa\n4    Iris-setosa\nName: target, dtype: object\n\n\n\n\n2. Apply the PCA function\n\nreduced_data_2, eig_val_2 = PCA(data.to_numpy(), 2)\nreduced_data_1, eig_val_2 = PCA(data.to_numpy(), 1)\n\n\n\n3. Create a Pandas Dataframe of reduced Dataset with target data\n\nreduced_df1 = np.c_[reduced_data_1, target]\nreduced_df1 = pd.DataFrame(reduced_df1)\nreduced_df1\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      -2.684207\n      Iris-setosa\n    \n    \n      1\n      -2.715391\n      Iris-setosa\n    \n    \n      2\n      -2.88982\n      Iris-setosa\n    \n    \n      3\n      -2.746437\n      Iris-setosa\n    \n    \n      4\n      -2.728593\n      Iris-setosa\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      145\n      1.944017\n      Iris-virginica\n    \n    \n      146\n      1.525664\n      Iris-virginica\n    \n    \n      147\n      1.764046\n      Iris-virginica\n    \n    \n      148\n      1.901629\n      Iris-virginica\n    \n    \n      149\n      1.389666\n      Iris-virginica\n    \n  \n\n150 rows × 2 columns\n\n\n\n\nreduced_df = np.c_[reduced_data_2, target]\nreduced_df = pd.DataFrame(reduced_df)\nreduced_df\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      -2.684207\n      -0.326607\n      Iris-setosa\n    \n    \n      1\n      -2.715391\n      0.169557\n      Iris-setosa\n    \n    \n      2\n      -2.88982\n      0.137346\n      Iris-setosa\n    \n    \n      3\n      -2.746437\n      0.311124\n      Iris-setosa\n    \n    \n      4\n      -2.728593\n      -0.333925\n      Iris-setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      1.944017\n      -0.187415\n      Iris-virginica\n    \n    \n      146\n      1.525664\n      0.375021\n      Iris-virginica\n    \n    \n      147\n      1.764046\n      -0.078519\n      Iris-virginica\n    \n    \n      148\n      1.901629\n      -0.115877\n      Iris-virginica\n    \n    \n      149\n      1.389666\n      0.282887\n      Iris-virginica\n    \n  \n\n150 rows × 3 columns\n\n\n\n\n\n4. Vizualize the data with one and two principal components\n\ncolors = {cat:color for cat,color in zip(target.unique(), ['tab:blue','tab:orange','tab:green','tab:red','tab:purple','tab:brown','tab:pink'])}\nplt.scatter(reduced_df1[0], reduced_df1[0], c=reduced_df1[1].map(colors))\nplt.axis('equal')\n# plt.legend(title='color', handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\ncolors = {cat:color for cat,color in zip(target.unique(), ['tab:blue','tab:orange','tab:green','tab:red','tab:purple','tab:brown','tab:pink'])}\nplt.scatter(reduced_df[0], reduced_df[1], c=reduced_df[2].map(colors))\nplt.axis('equal')\n# plt.legend(title='color', handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\n\n5. Conclusion\n\nprint(np.cumsum(eig_val_2/eig_val_2.sum()))\n\n[0.92461621 0.97763178 0.99481691 1.        ]\n\n\nAs we can see, the first column after transformation will have 92.46% of the original data, and if we take the first two component of the eigen vectors set, and use those to generate the data, we can get 97.76% of the original data.\n\n\n\n\nMore?\n\nGo to: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"
  },
  {
    "objectID": "05_Mathematics_II/index.html",
    "href": "05_Mathematics_II/index.html",
    "title": "Mathematics II",
    "section": "",
    "text": "Independence Test for Grocery Shop Mailer Type\nPrincipal Component Analysis"
  },
  {
    "objectID": "05_Mathematics_II/01_Chi_Square.html",
    "href": "05_Mathematics_II/01_Chi_Square.html",
    "title": "Independence Testing using Chi-Square Test",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport random\n\nfrom scipy.stats import chi2_contingency, chi2"
  },
  {
    "objectID": "05_Mathematics_II/01_Chi_Square.html#conclusion",
    "href": "05_Mathematics_II/01_Chi_Square.html#conclusion",
    "title": "Independence Testing using Chi-Square Test",
    "section": "Conclusion",
    "text": "Conclusion\nThe p-value is greater than 0.05 or chisquare value is less than critical value, we CAN NOT REJECT the null hypothesis.\nTherefore, there is no significant relationship between the mailer type used(Fancy or Classical) and the Signing up of User.\n\nWhy are we using chi-square distribution in this case?\nHere we have,\n\nAtleast one Categorial Variable(The type of Mail)\nMutually Exculsive values of the Categorial Variable\nThe observations are independent\n\n\n\nHow it looks with Guassian Distribution\nWe use guassian distribution when we need to compate an observation variable with a number value like that of mean."
  }
]