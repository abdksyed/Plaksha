[
  {
    "objectID": "04_DataVisualization/index.html",
    "href": "04_DataVisualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data Visualiation on iHerb Mask Dataset"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html",
    "href": "04_DataVisualization/02_MaskEDA.html",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "",
    "text": "Code\nimport math\nimport random\nimport os\nimport re\nimport string\nfrom collections import Counter\n\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pygal\n\nimport nltk\nnltk.download(\"stopwords\")\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom PIL import Image\nfrom os import path\nfrom wordcloud import WordCloud\n\n\n\n\nCode\nreviews = pd.read_csv(\"./data/reviews.csv\")\nproducts = pd.read_csv(\"./data/products.csv\")"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html#which-are-the-most-popular-face-masks-out-there",
    "href": "04_DataVisualization/02_MaskEDA.html#which-are-the-most-popular-face-masks-out-there",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "Which are the most popular face masks out there?",
    "text": "Which are the most popular face masks out there?\n\nTop 3 Products Overall\n\nLozperi, Copper Mask, Adult, Dot, 1 Count\nHwipure, Disposable KF94 ( N95 / KN95/ FFP2 ) Mask, 25 Masks\nLozperi, Copper Mask, Kids, Gray, 1 Count\n\n\n\nTop Product in each Category\n\nOthers - Lozperi, Copper Mask, Adult, Dot, 1 Count\nDisposable - Hwipure, Disposable KF94 ( N95 / KN95/ FFP2 ) Mask, 25 Masks\nReusable - Kosette, Nano Reusable Face Protection Mask, Medium, 1 Mask"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html#what-do-consumers-like-about-them-why",
    "href": "04_DataVisualization/02_MaskEDA.html#what-do-consumers-like-about-them-why",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "What do consumers like about them? Why?",
    "text": "What do consumers like about them? Why?\n\n\nCode\ndef getFreqDict(df: pd.DataFrame) -> dict:\n    stop_words = set(stopwords.words('english'))\n    stop_words = stop_words.union(set(['masks', 'mask', 'face']))\n\n    d = Counter()\n    for rev in df.review:\n            \n            # Removing punctions from string\n            rev = rev.translate(str.maketrans('', '', string.punctuation))\n            \n            # Removing stop words\n            word_tokens = word_tokenize(rev)\n            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n            \n            d.update(filtered_sentence)\n    \n    return d\n\n\n\n\nCode\ndef makeImage(freq_dict: dict) -> None:\n    \"\"\"\n    Generate Word Cloud of the given frequency dictionary\n    \"\"\"\n    wc = WordCloud(width=1200, height=1200, background_color=\"white\", max_words=20)\n    # generate word cloud\n    wc.generate_from_frequencies(freq_dict)\n\n    # show\n    plt.figure(figsize=(10,10))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\nCode\ndf_good = df[df.ratingValue>30]\ndf_bad = df[df.ratingValue<30]\n\n\n\nGood Rating Word Cloud\n\n\nCode\nfreq_dict = getFreqDict(df_good)\nmakeImage(freq_dict)\n\n\n\n\n\n\n\nBad Rating Word Cloud\n\n\nCode\nfreq_dict = getFreqDict(df_bad)\nmakeImage(freq_dict)\n\n\n\n\n\nSince itâ€™s only one word, some words which may be positive are in bad reviews, like good (where as in negative it may be not good)\n\n\nBiGram Word Cloud\n\nWorld Cloud for Good Ratings with BiGrams\n\n\nCode\ndef getFreqDict(df: pd.DataFrame) -> dict:\n    stop_words = set(stopwords.words('english'))\n    stop_words = stop_words.union(set(['masks', 'mask', 'face']))\n\n    d = Counter()\n    for rev in df.review:\n            \n            # Removing punctions from string\n            rev = rev.translate(str.maketrans('', '', string.punctuation))\n            \n            # Removing stop words\n            word_tokens = word_tokenize(rev)\n            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n            bigram_seq = []\n            for i in range(len(filtered_sentence)-1):\n                bigram_seq.append(filtered_sentence[i]+\" \"+filtered_sentence[i+1])\n            \n            d.update(bigram_seq)\n    \n    return d\n\n\n\n\nWorld Cloud for Bad Ratings with BiGrams\n\n\nCode\nfreq_dict = getFreqDict(df_good)\nmakeImage(freq_dict)\n\n\n\n\n\n\n\nCode\nfreq_dict = getFreqDict(df_bad)\nmakeImage(freq_dict)\n\n\n\n\n\nIn BiGrams itâ€™s more easy to see, which words are occuring more in Negative reviews and depicts negative sentiment"
  },
  {
    "objectID": "04_DataVisualization/02_MaskEDA.html#what-different-profiles-of-consumers-buy-masks",
    "href": "04_DataVisualization/02_MaskEDA.html#what-different-profiles-of-consumers-buy-masks",
    "title": "Data Visulization for iHerb Mask Dataset",
    "section": "What different profiles of consumers buy masks?",
    "text": "What different profiles of consumers buy masks?\n\nCustomer Profile based on Language Code\n\n\nCode\nlang = set()\ncountry = set()\nlang_iso = {'BR':\"BRA\", 'CN':\"CHN\", 'DE':\"DEU\", 'FR':\"FRA\", 'IL':\"ISR\", 'JP':\"JPN\", 'KR':\"KOR\", 'MX':\"MEX\", \n            'RU':\"RUS\", 'SA':\"SAU\", 'TW':\"TWN\", 'US':\"USA\"}\nfor i in df.languageCode.unique():\n    l,c = i.split(\"-\")\n    lang.add(l)\n    country.add(lang_iso[c])\n\n\n\n\nCode\nresult = df.groupby(['languageCode']).agg({'review': 'count'})\nresult.reset_index(inplace=True)\nresult[['lang','country']] = result['languageCode'].str.split('-',expand=True)\nresult['country'] = result['country'].map(lang_iso)\nresult.drop(['languageCode'], inplace=True, axis=1)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld = world.merge(result, how='left', left_on=\"iso_a3\", right_on=\"country\" )\n# world.review.fillna(0, inplace=True)\nworld['coords'] = world['geometry'].apply(lambda x: x.representative_point().coords[:])\nworld['coords'] = [coords[0] for coords in world['coords']]\n\n\n# plot confirmed cases world map \nax = world.plot(column='review', #scheme=\"quantiles\",\n           figsize=(25, 20),\n           legend=True,\n           cmap='coolwarm',\n           missing_kwds={\n        \"color\": \"lightgrey\",\n        \"edgecolor\": \"grey\",\n        # \"hatch\": \"///\",\n        \"label\": \"Missing values\",\n        },\n           legend_kwds={'orientation': \"horizontal\"})\n\n\nax.set_axis_off()\n\nplt.title('Number of Users based on Language',fontsize=25)\n\nfor idx, row in world.iterrows():\n   if not pd.isna(row.review):\n      text = f\"{row.lang}-{row.country} - {row.review}\" \n      plt.annotate(text=text, xy=row['coords'],\n                 horizontalalignment='center',\n                 fontsize=14)\n                \nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Plaksha",
    "section": "",
    "text": "Thie website contains all the content regarding TLP 2022-23\nMathematics I\nIntroduction to Programming\nIntroduction to Artificial Intelligence\nData Visualization\nMathematics II\nFoundations of Leadership"
  },
  {
    "objectID": "06_Foundation_of_Leadership/index.html",
    "href": "06_Foundation_of_Leadership/index.html",
    "title": "Foundations of Leadership",
    "section": "",
    "text": "Managing Oneself by Peter Drucker"
  },
  {
    "objectID": "06_Foundation_of_Leadership/01_ManagingOneself.html",
    "href": "06_Foundation_of_Leadership/01_ManagingOneself.html",
    "title": "Managing Oneself",
    "section": "",
    "text": "If you have got ambition, drive and smarts you can rise to top of your field.\nCultivate deep understanding of yourself\n\nWhat are your most valuble strengths and most dangerous weaknesses?\nHow do you learn and work with others?\nWhat are your most deeply held values?\n\nOnly when you operate from combination of your strengths and self-knowledge you can acheive true and lasting excellence"
  },
  {
    "objectID": "06_Foundation_of_Leadership/01_ManagingOneself.html#the-idea-in-practice",
    "href": "06_Foundation_of_Leadership/01_ManagingOneself.html#the-idea-in-practice",
    "title": "Managing Oneself",
    "section": "The Idea in Practice",
    "text": "The Idea in Practice\nWhat are my strengths\nEverytime you make a key decision, write down outcome you expect.\nSeveral months later, look for patterns in what youâ€™re seeing.\nDonâ€™t waste time cultivating skill areas you have little competence. INstead concentrate on and build on your strengths\nHow do I work\nUnderstand how you work best.\n\nReading\nListening others discuss"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "03_Intro_AI/03_Lab4_Chainning.html",
    "href": "03_Intro_AI/03_Lab4_Chainning.html",
    "title": "Rule Based Knowledge Assignment",
    "section": "",
    "text": "from typing import List, Dict, Set\n\n\nwith open(\"./data/file.txt\") as f:\n    observations = set(f.readline().strip().split(\",\"))\n    # Empty Line\n    f.readline()\n    rules = {}\n    # Loop over the remaining lines\n    for line in f.readlines():\n        # For rules\n        if \"=>\" in line:\n            # Split with \"=>\"\n            rule, goal = line.strip().split(\"=>\")\n            # If the goal is already present in dictionary, append the\n            # new rule_line as a set to the list\n            # If goal not present, create an empty list and than\n            # append the rule set\n            rules.setdefault(goal, []).append(set(rule.split(\"+\")))\n        # For Final Goal\n        elif line.isalpha():\n            finalgoal = line\n\nprint(observations)\nprint(finalgoal)\nprint(rules)\n    \n\n{'g', 'a', 'k', 'e', 'c', 'h', 'd'}\nq\n{'i': [{'k', 'l', 'm'}], 'q': [{'j', 'l', 'i'}, {'a', 'b'}, {'n', 'l', 'p', 'o'}], 'b': [{'e', 'c', 'd'}, {'f', 'h'}], 'r': [{'h', 'c'}], 's': [{'j', 'm', 'r'}], 'f': [{'g'}]}\n\n\n\nBackward Chainning\nGiven a Final Goal and a set of observations, determine if that set of observations leads to the final goal or not.\n\ndef backwardChainning(rules_set: Dict, \n                    observation_set: Set,\n                    final_goal: str,\n                    path: List) -> bool:\n    \"\"\"\n    Predict the given Final Goal is reachable or not from the given observations.\n    \"\"\"\n    # Base Cases\n    # If final_goal in observation, return True\n    if final_goal in observation_set:\n        return True, path\n    # If Goal/Final Goal not in observation\n    # Not even in knowledge base, we can't reach it.\n    # Return False\n    elif final_goal not in rules_set:\n        return False, path\n\n    for rule_set in rules_set[final_goal]:\n        for rule in rule_set:\n            # Recursion with Depth First Search\n            temp_result, path = backwardChainning(rules_set, observation_set, rule, path)\n            if temp_result == False:\n                break # Go to next rule_set\n        # If there was no break, which means all rule in the set\n        # was present, return True.\n        if temp_result: \n            path.append(f\"{rule_set} => {final_goal}\")\n            return temp_result, path\n        \n    return temp_result, path\n\n\nisFinalGoalReached, path = backwardChainning(rules, observations, finalgoal, [])\nprint(f\"Is the expected Final Goal {finalgoal} is reachable: {isFinalGoalReached}\")\nprint(f\"The path to reach the Final Goal is: {'; '.join(path)}\")\n\nIs the expected Final Goal q is reachable: True\nThe path to reach the Final Goal is: {'e', 'c', 'd'} => b; {'a', 'b'} => q\n\n\n\n\nForward Chainning\nGiven a set of observations and knowledge base, identify the most deep goal possible\n\ndef forwardChainning(rules_set: Dict, observation_set: set) -> str|None:\n    reached_goals = []\n    addition = True\n\n    while addition:\n        # Loop until there is no addition to observation\n        addition = False\n        # Go through each goal in the rules\n        for goal, rule_list in rules_set.items():\n            # Loop through all the set of rules(paths) to reach\n            # the goal\n            if goal in observation_set:\n                continue\n            # Check for each observation in the rule list(each line in txt file)\n            for rule_set in rule_list:\n                for rule in rule_set:\n                    # If observation is not present, skip the list\n                    if rule not in observation_set:\n                        break\n                else:\n                    # All rule of rule_set is present in observation\n                    # So goal acheived, add to observation set\n                    observation_set.add(goal)\n                    \n                    # Add the goal to listr of goals reached.\n                    reached_goals.append(goal)\n\n                    # Since there was an addition, we have to do one more run\n                    addition = True\n\n                    # Since goal is reached no need to check for further in that list.\n                    break\n    \n    return reached_goals\n\n\nobs_copy = observations.copy()\nacheivableGoals = forwardChainning(rules, obs_copy)\nprint(f\"The acheivable goals from the given observations are: {', '.join(acheivableGoals)}\")\n\nThe acheivable goals from the given observations are: b, r, f, q"
  },
  {
    "objectID": "03_Intro_AI/index.html",
    "href": "03_Intro_AI/index.html",
    "title": "Introduction to Artificial Intelligence",
    "section": "",
    "text": "AI Today compared to Dartmouth Workshop\nA* Algorithm\nBackward and Forward Chainning\nPOS Tagging using Hidden Markov Model\nLearnig Gates using Single and Multi Layer Perceptron"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html",
    "href": "03_Intro_AI/04_Lab5_HMM.html",
    "title": "Hidden Markov Model",
    "section": "",
    "text": "# Importing libraries\nimport nltk\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n \n# download the treebank corpus from nltk\nnltk.download('treebank')\n \n# download the universal tagset from nltk\nnltk.download('universal_tagset')\n \n# reading the Treebank tagged sentences\nnltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n \n# print the first two sentences along with tags\nprint(nltk_data[:2])\n\n[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html#random-10-sentences-test-accuracy",
    "href": "03_Intro_AI/04_Lab5_HMM.html#random-10-sentences-test-accuracy",
    "title": "Hidden Markov Model",
    "section": "Random 10 sentences Test Accuracy",
    "text": "Random 10 sentences Test Accuracy\n\n# test the Viterbi algorithm on a few sample sentences of test dataset\nrandom.seed(1234)      # define a random seed to get same sentences when run multiple times\nnp.random.seed(1234)\n\n# choose random 10 numbers\nrndom = [random.randint(1, len(test_set)) for x in range(10)]\n \n# list of 10 sentencess on which to test the model\ntest_run = [test_set[i] for i in rndom]\n \n# list of tagged words\ntest_run_base = [tup for sent in test_run for tup in sent]\n \n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_run for tup in sent]\n\n# testing 10 sentences to check the accuracy\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end - start\n \nprint(\"Time taken in seconds:\", difference)\n \n# accuracy should be good enough (> 90%) to be a satisfactory model\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n \naccuracy = len(check) / len(tagged_seq)\nprint('Viterbi Algorithm Accuracy:', accuracy * 100)\n\nTime taken in seconds: 0.06524825096130371\nViterbi Algorithm Accuracy: 92.82296650717703"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html#test-accuracy-for-the-entire-test-set",
    "href": "03_Intro_AI/04_Lab5_HMM.html#test-accuracy-for-the-entire-test-set",
    "title": "Hidden Markov Model",
    "section": "Test Accuracy for the entire Test Set",
    "text": "Test Accuracy for the entire Test Set\n\n# test the Viterbi algorithm on a few sample sentences of test dataset\nrandom.seed(1234)      # define a random seed to get same sentences when run multiple times\nnp.random.seed(1234)\n\n# list of tagged words\ntest_run_base = [tup for sent in test_set for tup in sent]\n \n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_set for tup in sent]\n\n# testing 10 sentences to check the accuracy\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end - start\n \nprint(\"Time taken in seconds:\", difference)\n \n# accuracy should be good enough (> 90%) to be a satisfactory model\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n \naccuracy = len(check) / len(tagged_seq)\nprint('Viterbi Algorithm Accuracy:', round(accuracy * 100))\n\nTime taken in seconds: 3.193380355834961\nViterbi Algorithm Accuracy: 90"
  },
  {
    "objectID": "03_Intro_AI/04_Lab5_HMM.html#hot-cold-state-problem",
    "href": "03_Intro_AI/04_Lab5_HMM.html#hot-cold-state-problem",
    "title": "Hidden Markov Model",
    "section": "Hot Cold State Problem",
    "text": "Hot Cold State Problem\nTwo Hidden States: HotðŸ¥µ and ColdðŸ¥¶\nThree Observations: 1, 2, and 3\n\nProbability of Sequence 3-1-3 Occuring?\nAns: 2.86%\n\n\nMost Probable States Sequence given the Observation 3-1-3\nAns: Hot-Cold-Hold"
  },
  {
    "objectID": "03_Intro_AI/01_Lab01.html",
    "href": "03_Intro_AI/01_Lab01.html",
    "title": "AI Today Compared to Dartmouth Workshop",
    "section": "",
    "text": "How has the focus on AI shifted from this original proposal to today? Specifically, talk about imbuing AI with human thinking and intelligence.\nThe major change in todayâ€™s AI would be the proper definition of tasks and problems being solved, whereas, in the Dartmouth workshop proposal, the problems which were trying to be solved were abstract and high-level.\nTodayâ€™s AI is more and more data dependent where with lots of data available today, the models are based on learning patterns in the data rather than learning the general environment. This approach of learning from data which is curated and marked, known as supervised learning, has limitations in scaling.\nWe humans from childhood start learning patterns from what we see even without many labels, and we use the prior information we learn through life to learn new things. The field of AI has been trying to simulate this phenomenon of learning from the environment directly without having labelled data, by exposing the AI to a large magnitude of unlabelled data, which allows the system to learn the representation of the world and objects and form an approximate model of common-sense. This method today was coined as the dark matter of intelligence[1].\nAnother common method today to build AI is how humans build intelligence by playing games and practising. AI today is trained to play games by playing against itself a very large number of times and to self-improve with time, which was also part of the proposal in the workshop.\nCommentary on the long-term implications of the workshop.\nThe workshop was the spark to start a huge field, which went through multiple ups and downs. The idea of computers being able to simulate or solve any real-world problem is still not achieved, but the progress so far has been great with various successes in the field of vision and language. The workshop brainstormed about different problems computers can solve on their own, and made people think about computers as not just tools to perform instructions but as intelligent beings.\nReferences\n[1] LeCun, Yan & Misra, Ishan. Self-supervised learning: The dark matter of intelligence. Meta AI. https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/"
  },
  {
    "objectID": "03_Intro_AI/02_Lab2_Submission_A_Star.html",
    "href": "03_Intro_AI/02_Lab2_Submission_A_Star.html",
    "title": "01 Programming Assignment",
    "section": "",
    "text": "from collections import deque\n\nclass Graph:\n    # example of adjacency list (or rather map) containing node: (neigbhor node, weight of edge)\n    # adjacency_list = {\n    # 'A': [('B', 1), ('C', 3), ('D', 7)],\n    # 'B': [('D', 5)],\n    # 'C': [('D', 12)]\n    # }\n\n    # initialises object with the given node: neighbor list\n    def __init__(self, adjacency_list: dict):\n        self.adjacency_list = adjacency_list\n\n    # gets the neighbor of a given node v from the list\n    def get_neighbors(self, v: str):\n        \"\"\"\n        Returns the List of nodes connected to the node v.\n        \"\"\"\n        try:\n            return self.adjacency_list[v]\n        except KeyError:\n            return []\n\n    # heuristic function with values for all nodes\n    def h(self, n):\n        \"\"\"\n        Heuristic Function, denoting the cost value from n to the goal state.\n        \"\"\"\n        H_dist = {\n            'A': 11,\n            'B': 6,\n            'C': 99,\n            'D': 1,\n            'E': 7,\n            'G': 0,    \n        }\n        return H_dist[n]\n    \n    def back_track(self,start_node, goal_state, back_track_dict):\n        \"\"\"\n        Returns the Path from goal_state to start_node\n        It traces the dictionary, starting from goal_state\n        and check it's parent, than again check parent's parent\n        and so on, till the parent node is initial node while adding\n        each parent node to a string.\n\n        Return the reverse of string to get path.\n        \"\"\"\n        curr_node = goal_state\n        path = goal_state\n        while start_node != curr_node:\n            curr_node = back_track_dict[curr_node]\n            path += curr_node\n\n        return path[::-1]\n\n    def a_star(self, start_node, stop_node):\n        # store visited nodes with uninspected neighbors in open_list, starting with the start node\n        # store visited nodes with inspected neighbors in closed_list\n        open_list = set([start_node])\n        closed_list = set([])\n\n        # g contains current distances from start_node to all other nodes\n        # the default value (if it's not found in the map) is +infinity\n        # It is the priority queue to select the next node for exploration\n        # node: (heuristic_cost to each here(h), cost to go to node(c), total_cost (h+c))\n        g = {}\n\n        g[start_node] = self.h(start_node), 0, self.h(start_node)+0\n\n        # Parents: dictionary where key is a node and it's value \n        # is the parent of that node.\n        parents = {}\n        parents[start_node] = start_node\n        current_node = start_node\n\n        while len(g) > 0:\n\n            # if current node is the goal state, return the path.\n            if current_node == stop_node:\n                return self.back_track(start_node, current_node, parents)\n\n            closed_list.add(current_node)\n            # Traversing the Graph\n            for (child, cost) in self.get_neighbors(current_node):\n                # if node is not in frontier and explored sets\n                if (child not in g) and (child not in closed_list):\n                    # Add the new node to frontier as key, and it's value\n                    # as a tuple of 3 values.\n                    g[child] = (self.h(child) , cost, self.h(child) + cost)\n                    parents[child] = current_node\n                \n                # if the child is present in frontier\n                elif child in g:\n                    # if the new path to the node is cheaper than already present\n                    # in frontier, replace the new cost\n                    if g[child][-1] > g[current_node][1] + cost + self.h(child):\n                        g[child] = (g[current_node][1] + cost, \n                                    self.h(child), \n                                    g[current_node][1] + self.h(child) + cost)\n                        # Also replace the parent of node\n                        parents[child] = current_node\n            \n            # Remove from frontier the explored node.\n            del g[current_node]\n            # Select new node to be explored based on minimum cost value.\n            current_node = min(g.items(), key=lambda item: item[1][-1])[0]\n            \n            \n        return None\n\n\nadjacency_list = {\n    'A': [('B', 1), ('C', 3), ('D', 7)],\n    'B': [('D', 5)],\n    'C': [('D', 12)]\n}\n\n# Driver code for the given graph\ngraph = Graph(adjacency_list)\ngraph.a_star('A', 'D')\n\n'ABD'"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html",
    "href": "03_Intro_AI/05_Lab6_MLP.html",
    "title": "Multi Perceptron Layer",
    "section": "",
    "text": "# Imports\nimport math\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#and-gate",
    "href": "03_Intro_AI/05_Lab6_MLP.html#and-gate",
    "title": "Multi Perceptron Layer",
    "section": "AND Gate",
    "text": "AND Gate\n\nand_data = np.array([[0,0,0],\n                     [0,1,0],\n                     [1,0,0],\n                     [1,1,1]])\n\n# Creating a PyTorch tensor\nand_data = torch.Tensor(and_data)\n\n\n# Same slicing as np arrays\nX = and_data[:,:-1]\ny = and_data[:,-1:]\n\n\nW = torch.randn((1,2), requires_grad=True)\nb = torch.randn((1,1), requires_grad=True)\n\nCreate the training loop (1 point)\n\nn_epochs = 100\nlr = 5e-1\nlosses = []\nfor _ in range(n_epochs):\n    # Define the Training Loop here\n\n    # Get predictions\n    output = sigmoid(perceptron(X, W, b))\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    with torch.no_grad():\n        W = W - lr*W.grad\n        b = b - lr*b.grad\n    W.requires_grad = True\n    b.requires_grad = True\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((perceptron(X, W, b) > 0.5).int())\n\ntensor([[0],\n        [0],\n        [0],\n        [1]], dtype=torch.int32)"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#or-gate",
    "href": "03_Intro_AI/05_Lab6_MLP.html#or-gate",
    "title": "Multi Perceptron Layer",
    "section": "OR Gate",
    "text": "OR Gate\n\nor_data = np.array([[0,0,0],\n                     [0,1,1],\n                     [1,0,1],\n                     [1,1,1]])\n\n# Creating a PyTorch tensor\nor_data = torch.Tensor(or_data)\n\n\n# Same slicing as np arrays\nX = or_data[:,:-1]\ny = or_data[:,-1:]\n\n\nW = torch.randn((1,2), requires_grad=True)\nb = torch.randn((1,1), requires_grad=True)\n\nReuse the training loop\n\nn_epochs = 100\nlr = 5e-1\nlosses = []\n\nfor _ in range(n_epochs):\n    # Get predictions\n    output = sigmoid(perceptron(X, W, b))\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    with torch.no_grad():\n        W = W - lr*W.grad\n        b = b - lr*b.grad\n    W.requires_grad = True\n    b.requires_grad = True\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((perceptron(X, W, b) > 0.5).int())\n\ntensor([[0],\n        [1],\n        [1],\n        [1]], dtype=torch.int32)"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#xor-gate",
    "href": "03_Intro_AI/05_Lab6_MLP.html#xor-gate",
    "title": "Multi Perceptron Layer",
    "section": "XOR Gate",
    "text": "XOR Gate\n\nxor_data = np.array([[0,0,0],\n                     [0,1,1],\n                     [1,0,1],\n                     [1,1,0]])\n\n# Creating a PyTorch tensor\nxor_data = torch.Tensor(xor_data)\n\n\n# Same slicing as np arrays\nX = xor_data[:,:-1]\ny = xor_data[:,-1:]\n\n\nW = torch.randn((1,2), requires_grad=True)\nb = torch.randn((1,1), requires_grad=True)\n\nReuse the training loop\n\nn_epochs = 100\nlr = 5e-1\nlosses = []\n\nfor _ in range(n_epochs):\n    # Get predictions\n    output = sigmoid(perceptron(X, W, b))\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    with torch.no_grad():\n        W = W - lr*W.grad\n        b = b - lr*b.grad\n    W.requires_grad = True\n    b.requires_grad = True\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((perceptron(X, W, b) > 0.5).int())\n\ntensor([[0],\n        [0],\n        [0],\n        [0]], dtype=torch.int32)"
  },
  {
    "objectID": "03_Intro_AI/05_Lab6_MLP.html#need-for-mlp",
    "href": "03_Intro_AI/05_Lab6_MLP.html#need-for-mlp",
    "title": "Multi Perceptron Layer",
    "section": "Need for MLP",
    "text": "Need for MLP\nAs seen above, we are unable to model the XOR gate using a single layer perceptron, so we need to add a hidden layer.\n\nW1 = torch.randn((10,2), requires_grad=True)\nW2 = torch.randn((1,10), requires_grad=True)\nb1 = torch.randn((1,10), requires_grad=True)\nb2 = torch.randn((1,1), requires_grad=True)\n\nImplement the mlp function (1 point)\n\ndef mlp(inputs, W1, W2, b1, b2):\n    '''\n    Defines the multi-layer perceptron model\n\n    Note: Only 1 hidden layer\n    '''\n    output = sigmoid(perceptron(inputs, W1, b1))\n    output = sigmoid(perceptron(output, W2, b2))\n\n    return output\n\n\ndef weights_update(weights_list, bias_list):\n    # Update Weights\n    updated_w = []\n    updated_b = []\n    for w,b in zip(weights_list, bias_list):\n        with torch.no_grad():\n                w = w - lr*w.grad\n                b = b - lr*b.grad\n        w.requires_grad = True\n        b.requires_grad = True\n        updated_w.append(w)\n        updated_b.append(b)\n    \n    return updated_w, updated_b\n\nReuse the training loop\nNOTE: It will require slight modification due to the hidden layer\n\nn_epochs = 1000\nlr = 5e-1\nlosses = []\nfor _ in range(n_epochs):\n     # Get predictions\n    output = mlp(X, W1, W2, b1, b2)\n\n    # Calculate Loss\n    loss = binary_cross_entropy(output, y)\n\n    # Do a backward step (to calculate gradients)\n    loss.backward()\n\n    # Update Weights\n    (W1,W2),(b1,b2) = weights_update([W1,W2], [b1,b2])\n\n    # Append Loss\n    losses.append(loss.item())\n\n\nplt.plot(losses)\n\n\n\n\n\nwith torch.no_grad():\n  print((mlp(X, W1, W2, b1, b2) > 0.5).int())\n\ntensor([[0],\n        [1],\n        [1],\n        [0]], dtype=torch.int32)"
  },
  {
    "objectID": "01_Mathematics_I/06_Maths_PageRank.html",
    "href": "01_Mathematics_I/06_Maths_PageRank.html",
    "title": "Plaksha",
    "section": "",
    "text": "08 Assignment - Page Rank\n\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nQuestion 1\nConsider any \\(2Ã—2\\) matrix \\(A\\) and apply the matrix on a \\(2Ã—1\\) vector \\(v\\). Keep applying this matrix and notice that the direction converges, while the magnitude may get larger. Keep normalizing \\(v\\) and notice the direction to which it converges. Use Numpy\n\nsomeMatrix = np.array([\n    [1,2],\n    [3,4]\n])\n\nsomeVector = np.array([2,3])\n\nprint(f\"Shape of Matrix: {someMatrix.shape}\")\nprint(f\"Shape of Vecotr: {someVector.shape}\")\n\nShape of Matrix: (2, 2)\nShape of Vecotr: (2,)\n\n\n\ndef angleBetween(v1, v2):\n    unit_v1 = v1/np.linalg.norm(v1)\n    unit_v2 = v2/np.linalg.norm(v2)\n    # Finding cos inverse of dot product to get angle in radians\n    # Convertring to degree\n    return 360*(np.arccos(np.dot(unit_v1, unit_v2))) / (2*np.pi)\n\n\nx_axis = np.array([1,0])\nsomeVectorAngle = [angleBetween(someVector, x_axis)]\n\nfor _ in range(999):\n    someVector = np.matmul(someMatrix, someVector)\n    someVector = someVector/np.linalg.norm(someVector)\n    someVectorAngle.append(angleBetween(someVector, x_axis))\n\n\nnp.linalg.eig(someMatrix)\n\n(array([-0.37228132,  5.37228132]),\n array([[-0.82456484, -0.41597356],\n        [ 0.56576746, -0.90937671]]))\n\n\n\nprin_eig = np.linalg.eig(someMatrix)[1][:,1]\n\n\nprin_eig, someVector\n\n(array([-0.41597356, -0.90937671]), array([0.41597356, 0.90937671]))\n\n\n\nx = list(range(1,1001))\ny = [angleBetween(prin_eig, x_axis)]*1000\nplt.plot(np.log(x),someVectorAngle, label=\"Convergence\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nQuestion 3\n\n\nProbabilityMatrix = np.array([\n        [0,0,0,0.5,0.5,1,1,1],\n        [0.5,0,0,0,0,0,0,0],\n        [0.5,0,0,0,0,0,0,0],\n        [0,0.5,0,0,0,0,0,0],\n        [0,0.5,0,0,0,0,0,0],\n        [0,0,0.5,0,0,0,0,0],\n        [0,0,0.5,0,0,0,0,0],\n        [0,0,0,0.5,0.5,0,0,0]\n    ])\n\n\nnp.linalg.eig(ProbabilityMatrix)[1][:,0]/np.linalg.norm(np.linalg.eig(ProbabilityMatrix)[1][:,0])\n\narray([0.74278135+0.j, 0.37139068+0.j, 0.37139068+0.j, 0.18569534+0.j,\n       0.18569534+0.j, 0.18569534+0.j, 0.18569534+0.j, 0.18569534+0.j])\n\n\n\nnp.linalg.matrix_power(ProbabilityMatrix, 1000)[:,0]*2.4141\n\narray([0.7428, 0.3714, 0.3714, 0.1857, 0.1857, 0.1857, 0.1857, 0.1857])\n\n\nQuestion 4\nWrite down the edge list of the above graph and use networkx to run a random walk on it. What is the distribution of visits?\n\ngraph = {\n    'a': ['b', 'c'],\n    'b': ['d', 'e'],\n    'c': ['f', 'g'],\n    'd': ['a', 'h'],\n    'e': ['a', 'h'],\n    'f': ['a'],\n    'g': ['a'],\n    'h': ['a']\n}\n\n\npointer = random.choice(list(graph.keys()))\nvisits = {k:0 for k,v in graph.items()}\nfor _ in range(8000):\n    pointer = random.choice(graph[pointer])\n    visits[pointer] += 1\n\n\npointer = random.choice(list(graph.keys()))\nvisits = {k:0 for k,v in graph.items()}\nfor _ in range(10000):\n    for _ in range(1000):\n        pointer = random.choice(graph[pointer])\n    visits[pointer] += 1\n\n\nnodes = list(visits.keys())\nvalues = list(visits.values())\nplt.bar(nodes, values)\nplt.plot(nodes,values, color='red')\n\n\n\n\nQuestion 5\nAre the following two questions equivalent?:\n\nIf I were to give pocket money to two of my daughters: 500 rupees each.\nI toss a coin and give my elder daughter 1000 rupees if its heads or the younger daughter 1000 rupees if its tails.\nAre these two statements equivalent? In the sense that, by the end of the year, do you think both my daughters would have received, more or less, the same amount of money?\n\n\n# Case 1 - Always Rs.500\n\nyounger_daughter_equal = 0\nelder_daughter_equal = 0\n\nyd_eq_list = []\ned_eq_list = []\n\nfor _ in range(365):\n    younger_daughter_equal += 500\n    yd_eq_list.append(younger_daughter_equal)\n    elder_daughter_equal += 500\n    ed_eq_list.append(elder_daughter_equal)\n\n\n# Case 2 - Giving Rs.1000 based on Toss Coin.\n\nyounger_daughter_coin = 0\nelder_daughter_coin = 0\n\nyd_coin_list = []\ned_coin_list = []\n\nfor _ in range(365):\n    if random.choice([0,1]):\n        younger_daughter_coin += 1000\n        yd_coin_list.append(younger_daughter_coin)\n        ed_coin_list.append(elder_daughter_coin)\n    else:\n        elder_daughter_coin += 1000\n        ed_coin_list.append(elder_daughter_coin)\n        yd_coin_list.append(younger_daughter_coin)\n\n\nplt.plot(list(range(365)), yd_eq_list, label=\"Equal Distribution\")\nplt.plot(list(range(365)), yd_coin_list, label=\"Coin Toss Distribution\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nQuestion 6 Consider the adjacency matrix of the above graph, tweak it and observe that the principal eigen vector is same as the answer to the previous question. (Principal Eigen Vector is defined as the eigen vector corresponding to the highest eigen value) â€” INCORRECT QUESTION â€”\nNew Question 6 Solve for Two Simultaneous Equations\n\ndef findIntersection(slope1, intercept1, slope2, intercept2):\n    if slope1 == slope2:\n        return -1\n    x = (intercept1-intercept2)/(slope2-slope1)\n    return (x, slope1*x+intercept1)\n\n\nfindIntersection(0,0,1,0)\n\n(0.0, 0.0)\n\n\nQuestion 7\nCan you consider a network of vertices, in the order of thousands and figure out the answer? You will observe that the best method to use is the random walk (with teleportation). why?\nIf the number of vertices are very high like thousands, the method to calculate eigen vectors requires matrix multiplications which is \\(O(n^2)\\), where as a random walk is a linear operation."
  },
  {
    "objectID": "01_Mathematics_I/01_Maths_09_Sept.html",
    "href": "01_Mathematics_I/01_Maths_09_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "import math\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef findMinima(equation: callable, step_size: float, step_reduce: float, iterations: int):\n    oldX = random.random()\n    print(f\"Starting with Random Numebr of {oldX}\")\n    oldY = equation(oldX)\n\n    oldestX = oldX\n    oldestY = oldY\n\n    valuesY = [oldY]\n\n    i = 1\n    while True:\n        newX = oldX + step_size\n        newY = equation(newX)\n        if newY > oldY:\n            oldY = oldestY\n            step_size /= step_reduce\n            oldX = oldestX\n        else:\n            valuesY.append(newY)\n            oldY, oldestY = newY, oldY\n            oldX, oldestX = newX, oldX\n\n        i += 1\n        if i == iterations:\n            break\n    \n    return valuesY, newX\n\n\niterations = 100\ndistanceValues, finalX = findMinima(lambda x : math.sqrt(50*(x**2) - 142*x + 101), 0.1, 10, iterations)\nxAxis = np.log(list(range(1,iterations+1)))\nplt.plot(distanceValues)\n\nStarting with Random Numebr of 0.3019662499232486\n\n\n\n\n\n\nprint(f\"Final value of X is: {finalX} and Loss Value is {distanceValues[-1]}\")\n\nFinal value of X is: 1.420000001024248 and Loss Value is 0.42426406871190303\n\n\n\n\n\nimport random\nnumHeads = []\nfor ball in range(1000):\n    sum = 0\n    for nail in range(50):\n        sum += random.randint(0,1)\n    numHeads.append(sum)"
  },
  {
    "objectID": "01_Mathematics_I/index.html",
    "href": "01_Mathematics_I/index.html",
    "title": "Mathematics I",
    "section": "",
    "text": "Linear Algebra Questionverse\n9th September Notes\n11th September Notes\n12th September Notes\n12th September Problems\n13th September Notes\nPage Rank\nProject - Predict Leader"
  },
  {
    "objectID": "01_Mathematics_I/03_Maths_12_Sept.html",
    "href": "01_Mathematics_I/03_Maths_12_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "05 Maths Assignment 12th September\nQuestion1\nConsider the matrix\n\\[\\begin{bmatrix}\n1 & 2\\\\\n3 & 4\n\\end{bmatrix}\\]\n\nWhat does it remind you of?\nWhat does it denote?\nWhere and why do we use a matrix?\nCan you enlist a few applications of matrices?\n\nThe above matrix reminds of the system of equations with two equations and two unknowns.\nIt denotes two vectors in the 2D-space (\\(â„^2\\)) which can also be interpreted as two equations.\nWe use a matrix as a function, and also to solve for unknows given equations.\nMatrices are functions which transforms vectors from one space to another space.\nMatrices are used to solve equations as they are represented in a simple manner.\nMatrices helps in storing data like images where elements are values of pixel intensity.\nQuestion 2\nDefine a function. What is a surjective, injective and bijective function?\nA function is a mapping from one space to another space, where each element in the left space(Domain) can have one and only mapping to the element in the right space(Co-domain).\nFunction takes an input from Domain and maps it to an element in co-domain.\nThere can be two or more elements in Domain mapping to same element in co-domain, but there can never be an element in Domain mapping multiple elements in co-domain.\nA Valid Function\n\nAn Invalid Function\n\nImage Credits: https://en.wikipedia.org/wiki/Function_%28mathematics%29\nSurjective Function\nThe onto function, is where every element in co-domain has atleast one pre-image in the domain.\n\nImage Credits: https://en.wikipedia.org/wiki/Surjective_function\nInjective Function\nThe one-one function, is where there is only one unique mapping to the element in the co-domain, there can be elements in co-domain which are not mapped, but function must map the elements of the domain to unique element in co-domain.\n\nImage Credits: https://en.wikipedia.org/wiki/Injective_function\nBijective Function\nIt is the combination of both the surjection and injection function, where every element in X is mapped to a unique(exactly one) element in Y, and there are no unpaired elements.\n\nImage Credits: https://en.wikipedia.org/wiki/Bijection\nQuestion 3\nGiven an example of a function \\(f:â„^2 â†’ â„^2\\)\nA function which maps from \\(â„^2\\) to \\(â„^2\\) will be \\((x,y)\\) to \\((y,x)\\), where the function is mirror reflection along the \\(x=y\\) line\nQuestion 4\nGiven an example of a very nice function \\(f:â„^2 â†’ â„^2\\) * Make extra efforts to make this function really nice. * Explain what is so nice about your function? * Why should one study such functions?\nA really nice function is the softmax function. The excellent property of this function is it can convert any point in \\(â„^2\\) to a probability distribution, which is a vector of probabilities and they are relative to the scale of value in the original vector.\n\\(softmax = \\frac{e^{z_i}}{\\displaystyle\\sum_{j=1}^{k} e^{z_j}}\\)\nFor each value in vector, it exponentiates the value, and divide it by the sum of all values being exponentiated, which gives the sum of all values after applying softmax to 1.\nThe important reason to study such functions, is to make use of it for various applications like classification, where a binary classification problem which gives some arbitrary values can be converted to a nice probability looking value.\nQuestion 7\nConsider \\(â„^2\\). What are all the properties of this set? \\(â„^2\\) is called a space of all vectors, a.k.a a vector space. Lookup for the definition of a vector space.\nThis set contains all the elements of the form \\((x,y)\\) where \\(x,y \\in â„\\). Any vector from this space, when added with another vector from the same space gives a resultant vector, which will also be part of the same space(\\(â„^2\\)). Hence it is called the space of all vectors of size 2.\nA vector space can be defined if a set of vectors \\(V\\) and two operations \\(*, +\\) which can be defined in any way, and for our purposes we consider the operatins as general addition and multiplication, must follow the following rules.\n\nThere must be an element 0, where \\(x+0=x, x \\in R^2\\)\nThe vector \\(\\alpha x\\) must belong to \\(â„^2\\)\nTwo vectors \\(x,y \\in â„^2\\) when added \\(x + y\\) must belong to \\(â„^2\\)\n\nQuestion 8\nA subset of a vector space which in itself is a vector space is caled a subspace. Given an example of a subspace of \\(â„^2\\).\nThe subspaces of \\(â„^2\\) are the zero vector \\((0,0)\\). Any line \\(\\alpha(x,y)\\) and the entire \\(â„^2\\) itself.\nQuestion 9\nGiven a vector \\((1,7)\\) what does the set \\({Î±(1,7)|Î±âˆˆâ„}\\) represent? Is it a subspace of \\(â„^2\\)?\nThe set of all vectors \\(\\alpha(1,7)\\) represent a line in \\(â„^2\\), which is a subspace of \\(â„^2\\)\nQuestion 10\nIs \\(â„^3\\) a vector space?\nYes, \\(â„^3\\) is a subspace as it follows all the 10 axioms of the vector space.\nQuestion 11\nConside the two points \\((1,2,3)\\) and \\((4,5,7)âˆˆâ„^3\\). What does the following set denote: \\({Î±(1,2,3)+Î²(4,5,7)|Î±,Î²âˆˆâ„}\\). Is this a subspace?\nThe following set \\({Î±(1,2,3)+Î²(4,5,7)|Î±,Î²âˆˆâ„}\\) denotes the plane in \\(â„^3\\). Since both the vectors \\((1,2,3)\\) and \\((4,5,7)\\) are indepent of each other.\nYes, the above plane is a subspace of \\(â„^3\\)\nQuestion 53\nSolve the following : \\[\nxâˆ’2y=15\n\\] \\[\nx+4y=19\n\\]\n\nIsnâ€™t this the same as \\[\n\\begin{equation}\n\\begin{bmatrix}\n1 & -2\\\\\n1 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\\\\ny\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n15\\\\\n19\n\\end{bmatrix}\n\\end{equation}\n\\]\nYou are trying to find if there is an element in the domain which maps to \\((15,19)\\).\nWhat exactly is happening here ? (Say all that you can)\n\nYes, \\((1)\\) is just the matrix representation of the system of equations which are needed to be solved.\nYes, by representing the equations in the matrix form, we are trying to find the unknown \\(x\\) and \\(y\\) which maps to \\((15,19)\\)"
  },
  {
    "objectID": "01_Mathematics_I/02_Maths_11_Sept.html",
    "href": "01_Mathematics_I/02_Maths_11_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "04 Maths Assignment 11th September\n\nfrom typing import List\nimport random\nimport math\n\n\nBubble Sort\n\ndef bubbleSort(array: List):\n    length = len(array)\n    for i in range(length, 0, -1):\n        noSwap = True\n        for j in range(i-1):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n                noSwap = False\n    if noSwap:\n        return array\n    return array\n\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\n\nassert sorted(randomArray) == bubbleSort(randomArray)\n\n\n\nSelection Sort\n\ndef selectionSort(array:List) -> List:\n    for i in range(len(array)-1):\n        min = i\n        for j in range(i+1,len(array)):\n            if array[j] < array[min]:\n                min = j\n        array[min], array[i] = array[i], array[min]\n    return array\n\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\n\nassert sorted(randomArray) == selectionSort(randomArray)\n\n\n\nLinear Search\n\ndef linearSearch(array: List, numToFind: int) -> int :\n    for idx, element in enumerate(array):\n        if element == numToFind:\n            return idx\n    return -1\n\n\nassert linearSearch(list(range(1000)),465) == 465\n\nprint(linearSearch(random.sample(list(range(1000)), 900), 1456))\nprint(linearSearch(random.sample(list(range(1000)), 900), 156))\n\n-1\n800\n\n\n\n\nBinary Search\n\ndef binarySearch(array:List, numToFind: int) -> int:\n    start = 0\n    end = len(array) - 1\n\n    while start < end:\n        mid = (start+end) // 2\n        \n        if array[mid] == numToFind:\n            return mid\n        elif array[mid] > numToFind:\n            end = mid - 1\n        else:\n            start = mid + 1\n    \n    return -1\n\n\nassert binarySearch(list(range(1000)), 1678) == -1\nassert binarySearch(list(range(1000)), 678) == 678\n\n\n\nTime Taken for Each Algo\n\nsomeArray = list(range(1000))\nsortedArray = someArray.copy()\nrandomArray = random.sample(someArray, 1000)\n\nTime Taken for Bubble Sort\n\n%timeit bubbleSort(randomArray)\n\n22 ms Â± 162 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n\n\nTime Taken for Selection Sort\n\n%timeit selectionSort(randomArray)\n\n17.7 ms Â± 122 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n\n\nTime Taken for In-Built Sort\n\n%timeit sorted(randomArray)\n\n3.55 Âµs Â± 14.1 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)\n\n\nIn-Built sort is 15215 times faster than Bubble Sort.\nIn-Built sort is 12957 times faster than Selection Sort.\nTime Taken for Linear Search\n\n%timeit linearSearch(sortedArray, random.choice(someArray))\n\n11.7 Âµs Â± 115 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)\n\n\nTime Taken for Binary Search\n\n%timeit binarySearch(sortedArray, random.choice(someArray))\n\n1.24 Âµs Â± 37.2 ns per loop (mean Â± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nTime Taken for In-Built Search\n\n%timeit random.choice(someArray) in sortedArray\n\n3.26 Âµs Â± 47.9 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%timeit random.choice(someArray) in randomArray\n\n3.32 Âµs Â± 105 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)\n\n\nIn-Built search is fast for both Sorted and Random Array."
  },
  {
    "objectID": "01_Mathematics_I/07_Maths_Project.html",
    "href": "01_Mathematics_I/07_Maths_Project.html",
    "title": "Plaksha",
    "section": "",
    "text": "09 Predict the Leader\n\nImports\n\nfrom typing import List\nimport random\n\nimport csv\nimport matplotlib.pyplot as plt\n\n\n\nMaking the Network from the CSV file\n\ndef make_graph(file_name: str) -> dict:\n    with open(file_name, \"r\") as csv_file:\n        csv_data = csv.reader(csv_file)\n        # Take All Names of Nominations from 1st column.\n        # Since 0th column is Title \"Name\"\n        NOMINATIONS = next(csv_data)[1:]\n        NETWORK = {name: [] for name in NOMINATIONS}\n\n        for row in csv_data:\n            votee = row[0]\n            for idx, isVote in enumerate(row[1:]):\n                if isVote == '1':\n                    NETWORK[NOMINATIONS[idx]].append(votee)\n    return NETWORK\n\n\nNETWORK = make_graph(\"./data/TLP_Leader_Data.csv\")\n\n\nNetwork Test to match values\n\n# Netwrok Check\nassert NETWORK['Abdul Khader, Syed'] == [\"Gupta, Aayush\", \"R K, Vysakh\"], \"Network is INCORRECT\"\n\n\nassert NETWORK['R K, Vysakh'] == ['Gupta, Aayush', 'Gowda, Adarsh', 'Narayan, Anchit', 'Sankar, Kirubananth', \n                                'Bhardwaj, Kunal', 'Walia, Muskaan', 'Krishna K, Pramod', 'Pandey, Savyasachi', \n                                'Pasricha, Stuti', 'Abdul Khader, Syed', 'Kumar Singh, Tejasvi', 'Pani, Tirtha', \n                                'Kumar, Vivek']\n\n\n\n\nRandom Walk for NUMBER_WALKS times\n\npointer = random.choice(list(NETWORK.keys()))\nvisits = {k:0 for k,v in NETWORK.items()}\nNUMBER_WALKS = 40_000\nfor _ in range(NUMBER_WALKS):\n    pointer = random.choice(NETWORK[pointer])\n    visits[pointer] += 1\n\n\n\nPlotting\n\norderedVisits = sorted(visits.items(), key = lambda item: item[1], reverse=True)\nnodes = [key for key,val in orderedVisits]\nvalues = [val/NUMBER_WALKS for key,val in orderedVisits]\n\n# The Adjust the Plot Size\nplt.figure(figsize = (18,12))\n\n# To Have the x-axis label as vertical\nplt.xticks(rotation=90)\n\n# Bar Graph\nplt.bar(nodes, values)\n\n# To plot the number value on each Bar\nfor i, v in enumerate(values):\n    plt.text(i-0.2, v+0.0007, str(round(v*100,2))+\"%\", color='black', fontweight='bold', rotation=90)\n\nplt.show()\n\n\n\n\n\nTop 5 Standings\n\nfor idx, (name, percentageVoting) in enumerate(orderedVisits[:5]):\n    print(f\"{name} has Rank:{idx+1} with the vote percentage of: {round(percentageVoting/NUMBER_WALKS * 100,2)}%\")\n\nGupta, Aayush has Rank:1 with the vote percentage of: 10.21%\nR K, Vysakh has Rank:2 with the vote percentage of: 4.51%\nPasricha, Stuti has Rank:3 with the vote percentage of: 3.57%\nSonnathi, Sumanth has Rank:4 with the vote percentage of: 3.28%\nGupta, Shivanshu has Rank:5 with the vote percentage of: 3.21%"
  },
  {
    "objectID": "01_Mathematics_I/00_LinearAlgebra.html",
    "href": "01_Mathematics_I/00_LinearAlgebra.html",
    "title": "Linear Algebra Questionverse",
    "section": "",
    "text": "1. What is scaling? What is a scalar? What is a vector?\nThe act of either extending/stretching or contracting/squishing something is known as scaling, and the amount it is changed, is given by a number, which is known as a scalar, which denotes that it scales something. This scaling can also change the direction.\nA vector is a list of numbers in some space, where the dimension of space is the number of elements in the vector representation. For example, if the vector is represented by a pair of numbers, than it lies in 2D space, where we can show it as an arrow, with it tail always at the origin and head pointing at the co-ordinates by which the vector is being represented, here the pair of numbers.\n\n2. How do you add two vectors? What is its physical significance?\nTo add two vectors, we do component wise addition of both the vectors, and the requirement to add both the vectors is that they must be in same system, like both the vectors must be represented as pair of numbers, or triplets of numbers, or some n-collection of numbers.\n\\(eg: [a,b,c] + [x,y,z] = [a+x, b+y, c+z]\\)\nThe physical significance of adding two vectors, can be understood as, the combined movement of both the vecors.\nHere the first vector is represented as moving along a units on the x-axis, b units parrallel to y-axis, and finally c units alongs the z-axis. When this is added to the second vector, it is the combined movement along each axis, which is a units on x-axis and than moving x units again on the x-axis, similarly first moving b units on y-axis and further moving y units parallel to y-axis and finally moving c units along z-axis and than moving z units along the z-axis.\nThis entire movement is equal to moving a+x units on x-axis, b+y units parallel to y-axis and finally c+z units along the z-axis.\nHence adding two vectors is equal to just adding the vectors corresponding components.\n\n3. How do you mulitply a scalar with a vector? Think of an application.\nWhen we multiply a scalar with a vector, it either expands/strech out the vector by the scalar(â€˜numberâ€™) it is being multiplied or contracts/squished the vector if the scalar is between 0 and 1.\nAlso, if the vector is multiplied by a negative scalar, the vector change itâ€™s direction by 180Â°, which means the vector now go in opposite direction and also is extends or contracts by the scalar value which is being multiplied.\nIn mathematical terms, scalar multiplication is just multiplying each component of the vector with the scalar value.\n\nThe scalar multiplication can be used to estimate one variable as scalar multiplication of other variable. For example if we have a vector of weights of a person, and if we know the relation between weight and height of a person(where \\(height = \\alpha * weight\\)), than we can represent the height of all persons as the scalar multiplication of the weight vector with \\(\\alpha\\)."
  },
  {
    "objectID": "01_Mathematics_I/00_LinearAlgebra.html#chapter-2",
    "href": "01_Mathematics_I/00_LinearAlgebra.html#chapter-2",
    "title": "Linear Algebra Questionverse",
    "section": "Chapter 2",
    "text": "Chapter 2\n4. What is \\(\\hat{i}\\) vector and a \\(\\hat{j}\\) vector. Why are they important?\nThe \\(\\hat{i}\\) vector is a unit vector, having the length 1, along the x-axis and similarly \\(\\hat{j}\\) is a unit vector in the y-direction.\nSo, now with this two vectors, we can represent any vector (x,y), by thinking as the x-coordinate of the vector, that scales the \\(\\hat{i}\\), stretching/contracting by a factor of x. And similarly the y-cordinate of the vector as the scalar that scales the \\(\\hat{j}\\) vector by a factor of y.\nAnd the final vector is the resultant of adding those two sclaed vectors. In this sense, the vectors these co-ordinate (x,y) desctibe is the sum of two scaled vectors \\(\\hat{i}\\) and \\(\\hat{j}\\).\n\n5. Why not consider any other basis vectors instead of the standard unit vectors?\nThe standard unit vectors can be used to easily generate any vector in its vector space, where the first co-ordinate of the vector is just the scalar multlication of the first standard basis vector(x-axis), and second co-ordinate is scalar multiplication of second standard basis vector and so on.\nAny vector can be represented by any set of random basis vectors, but the linear combinations in the cartesian system will be difficult to identify, since those random basis vectors are itself having some x and y co-ordinates value, which directly multiplying with scalar doesnâ€™t give the co-ordinates of the new vector.\n6. How can one get any vector from the basis vectors? What is the intuition?\nSince, the basis vectors are linearly independent and the linear combinations of the basis vectors span the entire the space, we can generate any vector."
  },
  {
    "objectID": "01_Mathematics_I/04_Maths_12_Sept_12_24.html",
    "href": "01_Mathematics_I/04_Maths_12_Sept_12_24.html",
    "title": "Plaksha",
    "section": "",
    "text": "06 Maths Assignment 12th September - 12th to 24th Question\nQuestion 12\nConsider a straight line \\(y=2x+1\\) in \\(â„^2\\), does it form a subspace of \\(â„^2\\)?\nNo, the straight line doesnâ€™t pass through the origin, which mean any point on the line, when multiplied with \\(0\\), gives \\((0,0)\\) which doesnâ€™t lie on the line, not making it a subspace of \\(â„^2\\)\n\nQuestion 13\nConsider a unit circle in \\(â„^2\\), centered at origin, is it a subspace of \\(â„^2\\)?\nNo, a unit circle in \\(â„^2\\) which is centered at origin, is not a subspace in \\(â„^2\\). Since, two points in the circle, \\((0,1)\\) and \\((1,0)\\) when added together, goes beyond the circle.\n\nQuestion 14\nWhat are all the subspaces of \\(â„^2\\)?\nAll the subspaces of the vector space \\(â„^2\\) are:\n\nOrigin - \\((0,0)\\)\nAny Line passing through origin. \\(\\alpha x = 0\\) where \\(x\\in â„^2\\) and \\(\\alpha \\in â„\\)\n\nQuestion 15\nWhat are all the subspaces of â„3?\nAll the subspaces of the vector space \\(â„^2\\) are:\n\nOrigin - \\((0,0)\\)\nAny Line passing through the origin. \\(\\alpha x\\) where \\(x \\in â„^3\\) and \\(\\alpha \\in â„\\)\nAny Plane passing through the origin. \\(\\alpha x + \\beta y\\) where \\(x,y \\in â„^3\\) and \\(\\alpha, \\beta \\in â„\\)\n\nQuestion 16\nGiven \\(â„^3\\), pick any two points \\(u,vâˆˆâ„^3\\). Note that \\({Î±u+Î²v | Î±,Î²âˆˆâ„}\\) is a subspace of \\(â„^3\\). Generalize this idea!\nAll the possible linear combinations of two points in \\(â„^3\\) is the linear span which is the subspace of \\(â„^3\\).\nGeneralizing it, for \\(â„^n\\), we have to prove the linear combinations of \\((n-1)\\) points in this \\(â„^n\\) is the subspace of \\(â„^n\\)\n\\(S = \\{u_1, u_2, u_3, ...., u_{n-1}\\}\\)\n\\(L(S) =\\) is all the possible linear combinations of S, which is known as Linear Span of S\nNow, we have to prove that \\(L(S)\\) is a subspace of \\(â„^n\\).\n\\[\nx = \\alpha_1 u_1 + \\alpha_2 u_2 + \\alpha_3 u_3 + ... + \\alpha_{n-1} u_{n-1}\n\\]\n\\[\ny = \\beta_1 u_1 + \\beta_2 u_2 + \\beta_3 u_3 + ... + \\beta_{n-1} u_{n-1}\n\\]\n\\[\nx,y \\in L(S)\n\\]\nNow, to prove that \\(L(S)\\) is a subspace of \\(â„^n\\), we have to prove two following conditions:\n\\[\nx+y = z \\in L(S)\n\\]\n\\[\n\\lambda x \\in L(S)\n\\]\nProof:\n\\[\nx + y = (\\alpha_1 u_1 + \\beta_1 u_1) + (\\alpha_2 u_2 + \\beta_2 u_2) + ... + (\\alpha_{n-1} u_{n-1} + \\beta_{n-1} u_{n-1})\n\\]\n\\[\nx + y = (\\alpha_1 + \\beta_1)u_1 + (\\alpha_1 + \\beta_1)u_2 + ... + (\\alpha_{n-1} + \\beta_{n-1})u_{n-1}\n\\] Since, \\(L(S)\\) contains all possible combinations, the above \\(x+y \\in L(S)\\)\n\\[\n\\lambda x = \\lambda \\alpha_1 u_1 + \\lambda \\alpha_2 u_2 + ... + \\lambda \\alpha_{n-1} u_{n-1}\n\\] Again, since \\(L(S)\\) contains all possible combinations, the above \\(\\lambda x \\in L(S)\\)\nQuestion 17\nThe set \\({Î±u+Î²v|Î±,Î²âˆˆâ„}\\) is called the linear combination of vectors \\(u\\) and \\(v\\). We can generalize this to \\(k\\) vectors. Observe what this set is all about?\nThe entire set when generalized to \\(k\\) vectors, is the linear span of \\(k\\) vectors, which is the subspace.\nQuestion 18\nWe say that a vector \\(w\\) is manufactured by \\(u\\) and \\(v\\) if \\(wâˆˆ{Î±u+Î²v|Î±,Î²âˆˆâ„}\\).\nYes, when we do \\(wâˆˆ{Î±u+Î²v|Î±,Î²âˆˆâ„}\\), we get all possible vectors in \\(â„^2\\), so we can manufacture any \\(w\\) by the linear combinations of \\(u\\) and \\(v\\)\nQuestion 19\nShow that \\((1,2,3)\\) and \\((4,5,6)\\) can manufacture \\((7,8,9)\\). Also \\((4,5,6)\\) and \\((7,8,9)\\) can manufacture \\((1,2,3)\\). Finally \\((4,5,6)\\) can be manufactured by the other two vectors.\nFor 1st case \\(\\alpha = -1\\) and \\(\\beta = 2\\)\nFor 2nd case \\(\\alpha = 2\\) and \\(\\beta = -1\\)\nFor 3rd case \\(\\alpha = 1/2\\) and \\(\\beta = 1/2\\)\nQuestion 20\nCan \\((2,1,0)\\) and \\((3,0,8)\\) manufacture \\((1,1,1)\\) ?\nNo, since we have inconsistent system, where \\[\n2x + 3y = 1\n\\]\n\\[\nx = 1\n\\]\n\\[\n8y = 1\n\\]\nAll the three equations doesnâ€™t satisfy simultaneously.\nQuestion 21\nCan \\((0,0,1)\\) and \\((0,1,0)\\) manufacture \\((1,0,0)\\)?\nNo, since \\((0,0,1)\\) and \\((0,1,0)\\) is the \\(yz\\) plane, we can never get \\((1,0,0)\\)\nQuestion 22\nWhen can two vectors in \\(â„^3\\) manufacture a given third vector?\nIf we have the third vector in the span of the first two vector, it can be manufactured.\nQuestion 23\nWhen can two vectors in \\(â„^3\\) fail to manufacture a given third vector?\nIf the third vector doesnâ€™t lie in the span of the first two vectors, it canâ€™t be manufactured\nQuestion 24\nIf \\(\\{u,v,w\\}\\) are such that a vector in this set can be manufactured by some vectors in the same set, then such a set is called a linearly dependent set.\nYes, since one vector is the combination of the other vectors, and add NO extra value, which means it is dependent on other vectors, hence the set is linearly dependent set"
  },
  {
    "objectID": "01_Mathematics_I/05_Maths_13_Sept.html",
    "href": "01_Mathematics_I/05_Maths_13_Sept.html",
    "title": "Plaksha",
    "section": "",
    "text": "07 Maths Assignment 13th Sept\n\n\\[\\begin{equation}\nMarkov =\n\\begin{bmatrix}\n0.5 & 0.1 & 0.2\\\\\n0.3 & 0.6 & 0.4\\\\\n0.2 & 0.3 & 0.4\n\\end{bmatrix}\n\\end{equation}\\]\nWe know that the Highest Eigen Value \\(\\lambda\\) is 1, which converges the Markov matrix towards the highest eigen value corresponding vector \\(V_{1}\\) So our final probability, which the Markov matrix converges to after many simulations is \\([v_1, v_2, v_3]\\)\n$$ \\[\\begin{equation}\n\\begin{bmatrix}\n0.5 & 0.1 & 0.2\\\\\n0.3 & 0.6 & 0.4\\\\\n0.2 & 0.3 & 0.4\n\\end{bmatrix}\n\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3\n\\end{bmatrix}\n\n=\n\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3\n\\end{bmatrix}\n\\end{equation}\\] $$\nNow we have \\[\n\\begin{equation}\n    0.5v_1 + 0.1v_2 + 0.2v_3 = v_1 \\tag{1}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n    0.3v_1 + 0.6v_2 + 0.4v_3 = v_2 \\tag{2}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n0.2v_1 + 0.3v_2 + 0.4v_3 = v_3 \\tag{3}\n\\end{equation}\n\\]\nAlso, we know the values of the principal eigen vector, are probabilities of the states, so the sum of these values must be 1.\n\\[\n\\begin{equation}\n    v_1 + v_2 + v_3 = 1 \\tag{4}\n\\end{equation}\n\\]\nSolving \\((1)\\), \\((2)\\) and \\((4)\\) we get\n\\[\nv_1 = 0.2181\n\\] \\[\nv_2 = 0.4727\n\\] \\[\nv_3 = 0.3090\n\\]\nNow, we start with 100 people at Hostel, 100 people at Plaksha and 100 people at Sector-17\n\n.2181*300, 0.4727*300, 0.3090*300\n\n(65.42999999999999, 141.81, 92.7)\n\n\nAfter the convergence, the state the system is in:\nPeople at Hostel: \\(0.2181*300 = 65.5\\)\nPeople at Plaksha: \\(0.4727*300 = 141.8\\)\nPeople at Sector-17: \\(0.3090*300 = 92.7\\)"
  },
  {
    "objectID": "00_Implementations/01_BackPropogation/main.html",
    "href": "00_Implementations/01_BackPropogation/main.html",
    "title": "Plaksha",
    "section": "",
    "text": "from typing import List, Union\nimport math\nimport numpy as np\n\n\nclass Network():\n    def __init__(self,\n                    input_size: int,\n                    num_layers: int, \n                    units: Union[int, List[int]],\n                    output_size: int\n                    ):\n        '''\n        The class creates a Feed Forward Fully connected Network\n\n        Arguments:\n            input_size: int -> The number of neurons in the input layer\n            num_layers: int -> The number of hidden layers in the network\n            units: Union[int, List[int]] -> If it's `int`, all layers have same size\n                                            If it's list of ints, length should be \n                                            equal to number of layers\n            output_size: int -> The number of neurons in the output layer\n        '''\n        self.num_layers = num_layers\n        units = [units]*num_layers if type(units) == int else units\n        self.units = [input_size] + units + [output_size]\n\n        self.layers = []\n        for i in range(self.num_layers + 1):\n            # Weight Matrix Transpose\n            self.layers.append( [ [0]*self.units[i] ]*self.units[i+1] )\n    \n    def _sigmoid(self, input: List[float]):\n        output = []\n        for val in input:\n            output.append( 1/(1+math.e**(-val)) )\n        return output\n\n    def forward(self, input: List):\n        x = input.copy()\n        for layer in self.layers:\n            new_x = []\n            for weights in layer:\n                val = 0\n                for idx in range(len(weights)):\n                    val += x[idx]*weights[idx]\n                new_x.append(val)\n            x = self._sigmoid(new_x)\n        \n        return x\n\n    def backward(self):\n        \n\n\nnet = Network(5, 4, 3, 1)\n\n\nnet.layers\n\n[[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],\n [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n [[0, 0, 0]]]\n\n\n\nnet.forward([1,1,1,1,1])\n\n[0.5]\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass pyNet(nn.Module):\n    \n    def __init__(self):\n        super(pyNet, self).__init__()\n        self.linear1 = nn.Linear(5,3,bias=False)\n        self.linear1.weight.data.fill_(0.0)\n\n        self.linear2 = nn.Linear(3,3,bias=False)\n        self.linear2.weight.data.fill_(0.0)\n\n        self.linear3 = nn.Linear(3,3,bias=False)\n        self.linear3.weight.data.fill_(0.0)\n\n        self.linear4 = nn.Linear(3,1,bias=False)\n        self.linear4.weight.data.fill_(0.0)\n\n\n    def forward(self, x):\n\n        x = torch.sigmoid(self.linear1(x))\n        x = torch.sigmoid(self.linear2(x))\n        x = torch.sigmoid(self.linear3(x))\n        x = torch.sigmoid(self.linear4(x))\n        \n        return x\n\n\ntorchNet = pyNet()\nprint(torchNet(torch.tensor([1.,1.,1.,1.,1.])))\n\ntensor([0.5000], grad_fn=<SigmoidBackward0>)"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html",
    "href": "02_Python_Programming/00_Assignment.html",
    "title": "01 Programming Assignment",
    "section": "",
    "text": "import math\nfrom typing import List\nimport random\nimport numpy as np"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html#chapter-01",
    "href": "02_Python_Programming/00_Assignment.html#chapter-01",
    "title": "01 Programming Assignment",
    "section": "Chapter 01",
    "text": "Chapter 01\nQuestion 1\nPrint the following using four print statements:\n*\n**\n***\n****\n\nprint(\"*\")\nprint(\"**\")\nprint(\"***\")\nprint(\"****\")\n\n*\n**\n***\n****\n\n\nQuesiton 2:\nFigure out how to input a number and display it using print statement.\n\nn = 5 #int(input(\"Enter the Number of Lines to be printed\"))\nfor i in range(1,n+1):\n    print(\"*\"*i)\n\n*\n**\n***\n****\n*****\n\n\nQuestion 3\nUnderstand how to use a if loop in python. Ask the user to enter a number and check if the number is even or odd.\n\nnum = 15 # int(input(\"Enter the Number\"))\nif num % 2 == 0 : print(\"Even\")\nelse: print(\"Odd\")\n\nOdd\n\n\nQuestion 4:\nPrint a sequence of numbers starting from the number a with common difference d.Â Go on till you reach the number b.\nEnter a value for a: 10\nEnter a value for d: 3\nEnter a value for b: 20\nOutput: 10 13 16 19\n\na = 10 # int(input(\"Enter value for a:\"))\nd = 3 # int(input(\"Enter value for d:\"))\nb = 20 # int(input(\"Enter value for b:\"))\n\nfor i in range(a,b,d):\n    print(i)\n\n10\n13\n16\n19\n\n\nQuestion 5:\nInput integers i and j and create a list comprising of all prime numbers between them.\n\nm = 12 # int(input('Enter the First Number'))\nn = 88 # int(input('Enter the Second Number'))\n\nprimeList = []\nfor i in range(m,n+1):\n    if i in [0,1]:\n        break\n    prime = True\n    for j in range(2,int(math.sqrt(i))+1):\n        if i%j == 0:\n            prime = False\n            break\n    if prime: primeList.append(i)\n\nprint(primeList)\n\n[13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83]\n\n\nQuestion 6:\nObserve the following code :\nWhat does the following code do:\nprint(\"Enter a number\")\nk=input()\nk=int(k)\nfor i in range(k):\n   for j in range(i):\n   print(\"*\",end=' ')\nprint(\"\")\nSomething is wrong in this code. Fix it!\n\nprint(\"Enter a number\")\nk= 8 # input()\nk=int(k)\nfor i in range(k):\n   for j in range(i):\n    print(\"*\",end=' ')\nprint(\"\")\n\nEnter a number\n* * * * * * * * * * * * * * * * * * * * * * * * * * * * \n\n\nQuestion 7:\nWrite a script to print the following:\nInput a number: 5\nOutput: \n    *\n   ***\n  *****\n *******\n*********\n\nn = 5 # int(input(\"Enter the Number of Lines\"))\n\nfor i in range(1,n+1):\n    m = (n-i) \n    print(\" \"*m + \"*\"*(2*i - 1))# + \" \"*m)\n\n    *\n   ***\n  *****\n *******\n*********\n\n\nQuestion 8:\nWrite a script to print the following zig zag:\n*\n *\n  *\n   *\n    *\nInput parameters: number of lines, inclination\nNote: Input parameters are self-explanatory, inclination is a value between 0 and 1. Lesser the inclination, more vertical the stick.\n\nnumLines = 8 # int(input(\"Enter Number of Lines\"))\ninclination = 0.2 # float(input(\"Inclination of Line\"))\n\nif inclination == 1:\n    print(\"*\"*numLines)\nelse:\n    for i in range(numLines+1):\n        print(\" \"*int(10*inclination*i) + \"*\")\n\n*\n  *\n    *\n      *\n        *\n          *\n            *\n              *\n                *\n\n\n\nAssignment 01\nImplement the following patter as in this video\n\nx = 20\na = -0.2\n\nfor i in range(-x//2, x//2):\n        spaces = int(4*i*i*a)\n        if a > 0:\n            print(\" \"*spaces + \"*\")\n        if a < 0:\n            max_spaces = int( 4 * (x//2)**2 * abs(a) )\n            spaces = -1 * spaces\n            print(\" \" * ( max_spaces - spaces) + \"*\")\n\n*\n                *\n                             *\n                                         *\n                                                    *\n                                                            *\n                                                                    *\n                                                                         *\n                                                                             *\n                                                                                *\n                                                                                *\n                                                                                *\n                                                                             *\n                                                                         *\n                                                                    *\n                                                            *\n                                                    *\n                                         *\n                             *\n                *\n\n\nExtra Question\nFind the Square Root of a Number, using an approach of guessing and moving towards the answer.\n\nUsing Binary Search\n\n\ndef sqrtBinary(number, start, end, numDecimal):\n    mid = (start+end)/2\n    midSq = mid**2\n    if math.isclose(number, midSq, abs_tol=10e-100):\n        return mid\n    elif number - midSq > 0:\n        return sqrtBinary(number, mid, end, numDecimal)\n    else:\n        return sqrtBinary(number, start, mid, numDecimal)\n\ndef sqrtBinary(number, start, end, numDecimal):\n    mid = (start+end)/2\n    midSq = mid**2\n    if math.isclose(number, midSq, abs_tol=10e-100):\n        return mid\n    elif number - midSq > 0:\n        return sqrtBinary(number, mid, end, numDecimal)\n    else:\n        return sqrtBinary(number, start, mid, numDecimal)\n\nnum = 10\nstart = 0\nend = num//2\nnumDecimal = 5\nprint(\"Square Root using our function:\", sqrtBinary(num, start, end, numDecimal))\nprint(\"Square Root using math.sqrt:\", math.sqrt(num))\n\n# test_close(sqrtBinary(num, start, end, numDecimal), math.sqrt(num), eps = 0.000_000_001)\n\nSquare Root using our function: 3.162277659866959\nSquare Root using math.sqrt: 3.1622776601683795"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html#chapter-02",
    "href": "02_Python_Programming/00_Assignment.html#chapter-02",
    "title": "01 Programming Assignment",
    "section": "Chapter 02",
    "text": "Chapter 02"
  },
  {
    "objectID": "02_Python_Programming/00_Assignment.html#bubble-sort",
    "href": "02_Python_Programming/00_Assignment.html#bubble-sort",
    "title": "01 Programming Assignment",
    "section": "Bubble Sort",
    "text": "Bubble Sort\n\ndef bubbleSort(array: List):\n    length = len(array)\n    for i in range(length, 0, -1):\n        noSwap = True\n        for j in range(i-1):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n                noSwap = False\n    if noSwap:\n        return array\n    return array\n\ndef bubbleSort(array: List):\n    length = len(array)\n    for i in range(length, 0, -1):\n        noSwap = True\n        for j in range(i-1):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n                noSwap = False\n    if noSwap:\n        return array\n    return array\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\n\n# test_eq(sorted(randomArray), bubbleSort(randomArray))\n\n\ndef findKth(array:List, k: int):\n    return bubbleSort(array)[k-1]   \n\n\nrandomArray = list(range(34, 1225))\nrandom.shuffle(randomArray)\nfindKth(randomArray, 7)\n\n40"
  },
  {
    "objectID": "02_Python_Programming/Programming_Assignment_4.html",
    "href": "02_Python_Programming/Programming_Assignment_4.html",
    "title": "03 Programming Assignment 4 - File Handling",
    "section": "",
    "text": "import random\nimport matplotlib.pyplot as plt\nfrom QuickSort import quickSort\n\nQuestion\nAll possible English words are available to download here. Save this to a file and read from this file, randomzize the order and write it back to the file.\nTip: use wget http://www.mieliestronk.com/corncob_lowercase.txt to download\n\nwith open('./data/corncob_lowercase.txt', 'r') as english_file:\n    all_words = english_file.read()\n\nall_words = all_words.split('\\n')\n\nassert not '' in all_words\n\n\nrandom.shuffle(all_words)\n\nrandomized_string = '\\n'.join(all_words)\nwith open('./data/corncob_lowercase.txt', 'w') as randomized_file:\n    randomized_file.write(randomized_string)\n\nQuestion\nConsider the list of randomized words as created in the previous program, sort it and write it back to the file.\n\nwith open('./data/corncob_lowercase.txt', 'r') as randomized_file:\n    toSort = randomized_file.read().split('\\n')\n\n\ntoSort = quickSort(toSort, 0, len(toSort)-1)\n\n\nsorted_words = '\\n'.join(toSort)\nwith open('./data/corncob_lowercase.txt', 'w') as sorted_file:\n    sorted_file.write(sorted_words)\n\n\nAssignment\nDownload this to your hard-disk and sort it. (Note: Any complicated sorting technique will not help. You are not allowed to browse the internet for the code or for any other hint. You cannot discuss with your neighbors. Solve this from first principles.)\nthis file: DONT OPEN https://sudarshansudarshan.github.io/plakshaprog/files/tosort.txt\nThe file has 10 Million rows, freezing the computer.\nTip: download using wget or request.get().\n\n# !wget https://sudarshansudarshan.github.io/plakshaprog/files/tosort.txt\n\n\nwith open('./data/tosort.txt', 'r') as largeLargeFile:\n    largeText = largeLargeFile.read().split('\\n')\n# To remove empty line at end\nlargeText.pop()\nlen(largeText)\n\n10000000\n\n\n\n# largeText = quickSort(largeText, 0, len(largeText)-1)\n\nSince sorting it gives recursion, error. Letâ€™s usefirst principles.\nLetâ€™s count the number of character present in the file, and since we know the inherent ordering of english alphabets. We can than just write to the file the same number of alphabet characters in order.\n\nalphabet_dict = {}\nfor alpha in largeText:\n    alphabet_dict[alpha] = alphabet_dict.get(alpha, 0) + 1\n\n\nalphabet_dict = sorted(alphabet_dict.items(), key=lambda item: item[0])\nalphabet_dict\n\n[('a', 385058),\n ('b', 385520),\n ('c', 385394),\n ('d', 384992),\n ('e', 383359),\n ('f', 384207),\n ('g', 385701),\n ('h', 384611),\n ('i', 383741),\n ('j', 384317),\n ('k', 383037),\n ('l', 384399),\n ('m', 385140),\n ('n', 383715),\n ('o', 384455),\n ('p', 385395),\n ('q', 385536),\n ('r', 385217),\n ('s', 384427),\n ('t', 385128),\n ('u', 383785),\n ('v', 384986),\n ('w', 384205),\n ('x', 384219),\n ('y', 384845),\n ('z', 384611)]\n\n\n\n# Runs the first loop, and gets alpha,it's count value\n# Than run the second loop for each alpha,value pair of first loop\n# In second loop it runs for value times, and add alpha each time to list\nsortedList = [alpha for alpha,value in alphabet_dict for _ in range(value)]\nlen(sortedList)\n\n10000000\n\n\n\nsorted_words = '\\n'.join(sortedList)\nwith open('./data/sorted.txt', 'w') as sortedFile:\n    sortedFile.write(sorted_words)"
  },
  {
    "objectID": "02_Python_Programming/index.html",
    "href": "02_Python_Programming/index.html",
    "title": "Introduction to Prgramming",
    "section": "",
    "text": "Assignemnt 1-3\nAssignemnt 4\nMedian"
  },
  {
    "objectID": "05_Mathematics_II/02_PCA.html",
    "href": "05_Mathematics_II/02_PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\n\nExercise 1\n\nDetermine PCA of a 3x2 matrix\n\ndefine a matrix\n\nA = np.array([[1, 2], [3, 4], [5, 6]])\nprint(A)\n\n[[1 2]\n [3 4]\n [5 6]]\n\n\n\n\nFirst do it manually!\n\nX = A[:,0]\nY = A[:,1]\nprint(X, Y)\n\n[1 3 5] [2 4 6]\n\n\n\n\n1. Subtract the mean of each variable\n\nmean_sub_X = X - np.mean(X)\nmean_sub_Y = Y - np.mean(Y)\n\nmean_matrix = A - np.mean(A, axis=0)\nprint(mean_sub_X, mean_sub_Y)\nprint(mean_matrix)\n\n[-2.  0.  2.] [-2.  0.  2.]\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\n\n\n\n\n2. Calculate the Covariance Matrix\n\nvar_X = np.sum(mean_sub_X**2)/(len(X)-1)\nvar_Y = np.sum(mean_sub_Y**2)/(len(Y)-1)\ncov_XY = np.sum(mean_sub_X * mean_sub_Y)/(len(X)-1)\n\ncov_matrix = np.array([\n    [var_X, cov_XY],\n    [cov_XY, var_Y]\n])\nprint(cov_matrix)\n\n[[4. 4.]\n [4. 4.]]\n\n\n\n\n3. Compute the Eigenvalues and Eigenvectors\n\neig_values, eig_vectors = np.linalg.eig(cov_matrix)\n\nprint(eig_values)\nprint(eig_vectors)\n\n[8. 0.]\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n\n\n\n\n4. project data of the original matrix to the new basis\n\nprin_eig_vector = eig_vectors[:,0].reshape(-1,1)\nprojected_A = np.matmul(mean_matrix, prin_eig_vector)\n\nprint(projected_A)\n\n[[-2.82842712]\n [ 0.        ]\n [ 2.82842712]]\n\n\n\nprojected_matrix = np.matmul(mean_matrix, eig_vectors)\n\nprint(projected_matrix)\n\n[[-2.82842712  0.        ]\n [ 0.          0.        ]\n [ 2.82842712  0.        ]]\n\n\n\n\nconclusion?\nThe Eigen values are 8 and 0 for two columns respectively, since the second eigen value is 0, there is no extra information in the second column aftert transformation, and hence can be dropped after performing the PCA transformation. And can consider only the first column.\n\n\n\n\nExercise 2\n\nOk Letâ€™s do it again but for a larger matrix 20x5\n\nGenerate a dummy dataset.\n\nX = np.random.randint(10,50,100).reshape(20,5)\nprint(X)\n\n[[42 21 35 14 10]\n [20 41 26 46 31]\n [36 37 19 24 20]\n [37 14 14 17 12]\n [42 14 38 17 29]\n [46 35 27 42 35]\n [19 34 32 12 49]\n [37 19 30 22 39]\n [14 46 34 40 43]\n [13 10 44 25 33]\n [30 48 27 24 36]\n [12 39 45 49 24]\n [11 36 34 10 14]\n [14 36 15 38 44]\n [40 29 13 15 42]\n [44 39 17 21 26]\n [17 20 26 14 48]\n [38 34 48 10 40]\n [39 37 42 48 14]\n [42 27 28 18 44]]\n\n\n\n\n1. Subtract the mean of each variable\nSubtract the mean of each variable from the dataset so that the dataset should be centered on the origin. Doing this proves to be very helpful when calculating the covariance matrix.\n\ncentral_X = X - np.mean(X, axis = 0)\n\n\n\n2. Calculate the Covariance Matrix\nCalculate the Covariance Matrix of the mean-centered data.\n\ncov_mat = np.cov(central_X, rowvar= False)\nprint(cov_mat)\n\n[[165.08157895 -26.23157895 -28.11052632 -37.31052632 -33.65526316]\n [-26.23157895 121.11578947  -7.32631579  66.95789474  13.08421053]\n [-28.11052632  -7.32631579 111.90526316  16.56842105 -12.26842105]\n [-37.31052632  66.95789474  16.56842105 178.53684211 -12.99473684]\n [-33.65526316  13.08421053 -12.26842105 -12.99473684 156.66052632]]\n\n\nNote: the matrix is symmetrical\n\n\n3. Compute the Eigenvalues and Eigenvectors\nNow, compute the Eigenvalues and Eigenvectors for the calculated Covariance matrix.\n\neig_values, eig_vectors = np.linalg.eig(cov_mat)\n# The above Eigen Vectors is column wise for each eigen value\n# Converting it to Row wise\neig_vectors = eig_vectors.T\nprint(eig_values)\nprint(eig_vectors)\n\n[253.01066884 181.35485507 141.1469473   88.82152611  68.96600268]\n[[-0.52753037  0.45832807  0.15014975  0.68623713  0.13483595]\n [ 0.43675002  0.09465772  0.05213214  0.41635417 -0.79007644]\n [-0.49880679 -0.41149083  0.64073475 -0.17512103 -0.37504492]\n [-0.52633992  0.11431773 -0.66767148 -0.24624489 -0.45108313]\n [-0.0715486  -0.77368084 -0.34410909  0.51422163  0.11603365]]\n\n\nNote: The Eigenvectors of the Covariance matrix we get are Orthogonal to each other and each vector represents a principal axis. A Higher Eigenvalue corresponds to a higher variability. Hence the principal axis with the higher Eigenvalue will be an axis capturing higher variability in the data.\n\n\n4. Sort Eigenvalues in descending order\nSort the Eigenvalues in the descending order along with their corresponding Eigenvector.\n\ndesc_order = np.argsort(eig_values)[::-1]\neig_values = eig_values[desc_order]\neig_vectors = eig_vectors[desc_order]\n\nprint(eig_values)\nprint(eig_vectors)\n\n[253.01066884 181.35485507 141.1469473   88.82152611  68.96600268]\n[[-0.52753037  0.45832807  0.15014975  0.68623713  0.13483595]\n [ 0.43675002  0.09465772  0.05213214  0.41635417 -0.79007644]\n [-0.49880679 -0.41149083  0.64073475 -0.17512103 -0.37504492]\n [-0.52633992  0.11431773 -0.66767148 -0.24624489 -0.45108313]\n [-0.0715486  -0.77368084 -0.34410909  0.51422163  0.11603365]]\n\n\nNote: Each column in the Eigen vector-matrix corresponds to a principal component, so arranging them in descending order of their Eigenvalue will automatically arrange the principal component in descending order of their variability. Hence the first column in our rearranged Eigen vector-matrix will be a principal component that captures the highest variability.\n\n\n5. Select a subset from the rearranged Eigenvalue matrix\nSelect a subset of n first eigenvectors from the rearranged Eigenvector matrix as per our need, n is desired dimension of your final reduced data. i.e.Â â€œn_components=2â€ means you selected the first two principal components.\n\nn_components = 2\n\ndesired_eig_vectors = eig_vectors[:2]\nprint(desired_eig_vectors)\n\n[[-0.52753037  0.45832807  0.15014975  0.68623713  0.13483595]\n [ 0.43675002  0.09465772  0.05213214  0.41635417 -0.79007644]]\n\n\nNote: The final dimensions of X_reduced will be ( 20, 2 ) and originally the data was of higher dimensions ( 20, 5 ).\n\n\n6. Transform the data\nFinally, transform the data by having a dot product between the Transpose of the Eigenvector subset and the Transpose of the mean-centered data. By transposing the outcome of the dot product, the result we get is the data reduced to lower dimensions from higher dimensions.\n\nprojected_matrix = np.matmul(central_X, desired_eig_vectors.T)\n\nprint(projected_matrix)\n\n[[-2.08844995e+01  1.71428703e+01]\n [ 2.33275256e+01  5.69006310e+00]\n [-4.57773329e+00  1.14655567e+01]\n [-2.22799056e+01  1.28706507e+01]\n [-1.90217522e+01  2.87427279e+00]\n [ 4.80631252e+00  1.17040271e+01]\n [ 6.42642644e-01 -2.34739157e+01]\n [-1.05141129e+01 -5.07223941e+00]\n [ 2.74861549e+01 -8.01913353e+00]\n [ 1.37345567e+00 -9.68678831e+00]\n [ 6.98763100e+00 -2.33787436e+00]\n [ 3.05988175e+01  9.77685579e+00]\n [-1.18913037e-02  1.45630833e-01]\n [ 1.88123905e+01 -1.15790063e+01]\n [-1.44651211e+01 -8.98636706e+00]\n [-9.43131532e+00  9.05508688e+00]\n [-4.38214982e+00 -2.43626320e+01]\n [-9.56403624e+00 -8.06357138e+00]\n [ 1.29537954e+01  2.87078047e+01]\n [-1.18562084e+01 -7.85129066e+00]]\n\n\n\n\n\n\nExercise 3\n\nNow, letâ€™s just combine everything above by making a function and try our Principal Component analysis from scratch on an example.\n\nCreate a PCA function accepting data matrix and the number of components as input arguments.\n\ndef PCA(data, num_components):\n    # Finding the Centralised Matrix (Matrix - Column Mean)\n    central_X = data - np.mean(data, axis = 0)\n\n    # Finding the Co-Variance Matrix\n    cov_mat = np.cov(central_X, rowvar= False)\n\n    # Finding the Eigen Values and Eigen Vectors\n    eig_values, eig_vectors = np.linalg.eig(cov_mat)\n    # Eigen vectors are column wise, making them row wise\n    eig_vectors = eig_vectors.T\n\n    # Taking the Descinding order of Eigen Values\n    # and their corresponding Eigen Vectors\n    desc_order = np.argsort(eig_values)[::-1]\n    eig_values = eig_values[desc_order]\n    eig_vectors = eig_vectors[desc_order]\n\n    # Desired Eigen Vectors\n    desired_eig_vectors = eig_vectors[:num_components]\n\n    projected_matrix = np.matmul(central_X, desired_eig_vectors.T)\n\n    return projected_matrix, eig_values\n\n\n\nLetâ€™s use the IRIS dataset to test our PCA function, and by the same way see if we can classify the dataset in the projected space\n\n#Get the IRIS dataset\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\ndata = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\ndata.head()\n\n\n\n\n\n  \n    \n      \n      sepal length\n      sepal width\n      petal length\n      petal width\n      target\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n    \n  \n\n\n\n\n\n\n1. prepare the dataset & target set for classification\n\ntarget = data.iloc[:,-1]\ndata = data.iloc[:,:-1]\nprint(data.head())\nprint(target.head())\n\n   sepal length  sepal width  petal length  petal width\n0           5.1          3.5           1.4          0.2\n1           4.9          3.0           1.4          0.2\n2           4.7          3.2           1.3          0.2\n3           4.6          3.1           1.5          0.2\n4           5.0          3.6           1.4          0.2\n0    Iris-setosa\n1    Iris-setosa\n2    Iris-setosa\n3    Iris-setosa\n4    Iris-setosa\nName: target, dtype: object\n\n\n\n\n2. Apply the PCA function\n\nreduced_data_2, eig_val_2 = PCA(data.to_numpy(), 2)\nreduced_data_1, eig_val_2 = PCA(data.to_numpy(), 1)\n\n\n\n3. Create a Pandas Dataframe of reduced Dataset with target data\n\nreduced_df1 = np.c_[reduced_data_1, target]\nreduced_df1 = pd.DataFrame(reduced_df1)\nreduced_df1\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      -2.684207\n      Iris-setosa\n    \n    \n      1\n      -2.715391\n      Iris-setosa\n    \n    \n      2\n      -2.88982\n      Iris-setosa\n    \n    \n      3\n      -2.746437\n      Iris-setosa\n    \n    \n      4\n      -2.728593\n      Iris-setosa\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      145\n      1.944017\n      Iris-virginica\n    \n    \n      146\n      1.525664\n      Iris-virginica\n    \n    \n      147\n      1.764046\n      Iris-virginica\n    \n    \n      148\n      1.901629\n      Iris-virginica\n    \n    \n      149\n      1.389666\n      Iris-virginica\n    \n  \n\n150 rows Ã— 2 columns\n\n\n\n\nreduced_df = np.c_[reduced_data_2, target]\nreduced_df = pd.DataFrame(reduced_df)\nreduced_df\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      -2.684207\n      -0.326607\n      Iris-setosa\n    \n    \n      1\n      -2.715391\n      0.169557\n      Iris-setosa\n    \n    \n      2\n      -2.88982\n      0.137346\n      Iris-setosa\n    \n    \n      3\n      -2.746437\n      0.311124\n      Iris-setosa\n    \n    \n      4\n      -2.728593\n      -0.333925\n      Iris-setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      1.944017\n      -0.187415\n      Iris-virginica\n    \n    \n      146\n      1.525664\n      0.375021\n      Iris-virginica\n    \n    \n      147\n      1.764046\n      -0.078519\n      Iris-virginica\n    \n    \n      148\n      1.901629\n      -0.115877\n      Iris-virginica\n    \n    \n      149\n      1.389666\n      0.282887\n      Iris-virginica\n    \n  \n\n150 rows Ã— 3 columns\n\n\n\n\n\n4. Vizualize the data with one and two principal components\n\ncolors = {cat:color for cat,color in zip(target.unique(), ['tab:blue','tab:orange','tab:green','tab:red','tab:purple','tab:brown','tab:pink'])}\nplt.scatter(reduced_df1[0], reduced_df1[0], c=reduced_df1[1].map(colors))\nplt.axis('equal')\n# plt.legend(title='color', handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\ncolors = {cat:color for cat,color in zip(target.unique(), ['tab:blue','tab:orange','tab:green','tab:red','tab:purple','tab:brown','tab:pink'])}\nplt.scatter(reduced_df[0], reduced_df[1], c=reduced_df[2].map(colors))\nplt.axis('equal')\n# plt.legend(title='color', handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\n\n5. Conclusion\n\nprint(np.cumsum(eig_val_2/eig_val_2.sum()))\n\n[0.92461621 0.97763178 0.99481691 1.        ]\n\n\nAs we can see, the first column after transformation will have 92.46% of the original data, and if we take the first two component of the eigen vectors set, and use those to generate the data, we can get 97.76% of the original data.\n\n\n\n\nMore?\n\nGo to: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"
  },
  {
    "objectID": "05_Mathematics_II/index.html",
    "href": "05_Mathematics_II/index.html",
    "title": "Mathematics II",
    "section": "",
    "text": "Independence Test for Grocery Shop Mailer Type\nPrincipal Component Analysis"
  },
  {
    "objectID": "05_Mathematics_II/01_Chi_Square.html",
    "href": "05_Mathematics_II/01_Chi_Square.html",
    "title": "Independence Testing using Chi-Square Test",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport random\n\nfrom scipy.stats import chi2_contingency, chi2"
  },
  {
    "objectID": "05_Mathematics_II/01_Chi_Square.html#conclusion",
    "href": "05_Mathematics_II/01_Chi_Square.html#conclusion",
    "title": "Independence Testing using Chi-Square Test",
    "section": "Conclusion",
    "text": "Conclusion\nThe p-value is greater than 0.05 or chisquare value is less than critical value, we CAN NOT REJECT the null hypothesis.\nTherefore, there is no significant relationship between the mailer type used(Fancy or Classical) and the Signing up of User.\n\nWhy are we using chi-square distribution in this case?\nHere we have,\n\nAtleast one Categorial Variable(The type of Mail)\nMutually Exculsive values of the Categorial Variable\nThe observations are independent\n\n\n\nHow it looks with Guassian Distribution\nWe use guassian distribution when we need to compate an observation variable with a number value like that of mean."
  }
]