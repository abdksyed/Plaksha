{
  "cells": [
    {
      "cell_type": "raw",
      "id": "59671611",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Hidden Markov Model\"\n",
        "desciption: \"Viterbi Algorithm to Predict Most Probable Sequence of States in POS Tagging\"\n",
        "author: Abdul Khader, Syed\n",
        "format:\n",
        "    html:\n",
        "        code-fold: false\n",
        "        page-layout: article\n",
        "execute: \n",
        "  enabled: false\n",
        "  warning: false\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "d5ce1339",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ce1339",
        "outputId": "de55fef4-51f9-4cb9-c7eb-eccc4b328521"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# download the treebank corpus from nltk\n",
        "nltk.download('treebank')\n",
        " \n",
        "# download the universal tagset from nltk\n",
        "nltk.download('universal_tagset')\n",
        " \n",
        "# reading the Treebank tagged sentences\n",
        "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        " \n",
        "# print the first two sentences along with tags\n",
        "print(nltk_data[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "5139cc4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5139cc4d",
        "outputId": "e769c9a9-8b3d-46a8-f16a-871037364c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==============\n",
            "('Pierre', 'NOUN')\n",
            "('Vinken', 'NOUN')\n",
            "(',', '.')\n",
            "('61', 'NUM')\n",
            "('years', 'NOUN')\n",
            "('old', 'ADJ')\n",
            "(',', '.')\n",
            "('will', 'VERB')\n",
            "('join', 'VERB')\n",
            "('the', 'DET')\n",
            "('board', 'NOUN')\n",
            "('as', 'ADP')\n",
            "('a', 'DET')\n",
            "('nonexecutive', 'ADJ')\n",
            "('director', 'NOUN')\n",
            "('Nov.', 'NOUN')\n",
            "('29', 'NUM')\n",
            "('.', '.')\n",
            "==============\n",
            "('Mr.', 'NOUN')\n",
            "('Vinken', 'NOUN')\n",
            "('is', 'VERB')\n",
            "('chairman', 'NOUN')\n",
            "('of', 'ADP')\n",
            "('Elsevier', 'NOUN')\n",
            "('N.V.', 'NOUN')\n",
            "(',', '.')\n",
            "('the', 'DET')\n",
            "('Dutch', 'NOUN')\n",
            "('publishing', 'VERB')\n",
            "('group', 'NOUN')\n",
            "('.', '.')\n"
          ]
        }
      ],
      "source": [
        "# print each word with its respective tag for first two sentences\n",
        "for sent in nltk_data[:2]:\n",
        "    print('==============')\n",
        "    print(*sent, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "da8b463b",
      "metadata": {
        "id": "da8b463b"
      },
      "outputs": [],
      "source": [
        "# split data into training and validation set in the ratio 80:20\n",
        "train_set, test_set = train_test_split(nltk_data, train_size = 0.80, test_size = 0.20, random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "433a57bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "433a57bf",
        "outputId": "5e966165-03b6-40c9-d1f4-5340bc604bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80310\n",
            "20366\n"
          ]
        }
      ],
      "source": [
        "# create list of train and test tagged words\n",
        "train_tagged_words = [ tup for sent in train_set for tup in sent ]\n",
        "test_tagged_words = [ tup for sent in test_set for tup in sent ]\n",
        "print(len(train_tagged_words))\n",
        "print(len(test_tagged_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "47a8c1e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47a8c1e9",
        "outputId": "7cc1fe1f-8c9e-4721-f9d9-b938841ca9c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Drink', 'NOUN'),\n",
              " ('Carrier', 'NOUN'),\n",
              " ('Competes', 'VERB'),\n",
              " ('With', 'ADP'),\n",
              " ('Cartons', 'NOUN')]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check some of the tagged words.\n",
        "train_tagged_words[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "d813ad7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d813ad7c",
        "outputId": "ef4d4020-25bd-4646-890c-f8f55408077d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tags: 12\n",
            "{'ADP', 'DET', 'PRON', 'ADV', 'NOUN', '.', 'CONJ', 'VERB', 'NUM', 'ADJ', 'X', 'PRT'}\n",
            "Number of words in vocabulary: 11052\n"
          ]
        }
      ],
      "source": [
        "# use set datatype to check how many unique tags are present in training data\n",
        "tags = {tag for word, tag in train_tagged_words}\n",
        "print('Number of tags:', len(tags))\n",
        "print(tags)\n",
        " \n",
        "# check total words in vocabulary\n",
        "vocab = {word for word, tag in train_tagged_words}\n",
        "print('Number of words in vocabulary:', len(vocab))\n",
        "\n",
        "# creating the order of words based on alphabetical order\n",
        "word_order = {word:idx for idx,word, in enumerate(sorted(list(vocab)))}\n",
        "\n",
        "# creating the order of tags based on alphabetical order\n",
        "tags_list = sorted(list(tags))\n",
        "tag_order = {tag:idx for idx,tag, in enumerate(tags_list)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "tFO5AW3x9KYT",
      "metadata": {
        "id": "tFO5AW3x9KYT"
      },
      "outputs": [],
      "source": [
        "def emission_prob(vocab, tags, word_order=word_order, train_bag = train_tagged_words):\n",
        "    B = {t:np.zeros(len(vocab)) for t in tags}\n",
        "\n",
        "    for w,t in train_bag:\n",
        "        B[t][word_order[w]] += 1\n",
        "    \n",
        "    for key in B:\n",
        "        B[key] = B[key]/np.sum(B[key])\n",
        "\n",
        "    return B\n",
        "\n",
        "def state_prob(tags, word_order=word_order, tag_order=tag_order, train_bag = train_tagged_words):\n",
        "    A = np.zeros((len(tags),len(tags)))\n",
        "    \n",
        "    for i in range(len(train_tagged_words)-1):\n",
        "        # Getting the Tag of a Word\n",
        "        w1 = train_tagged_words[i][1]\n",
        "        # Getting the Tag of the subsequent Word\n",
        "        w2 = train_tagged_words[i+1][1]\n",
        "        A[tag_order[w1], tag_order[w2]] += 1\n",
        "\n",
        "    return A/A.sum(axis=1, keepdims=True)\n",
        "\n",
        "def state_init_prob(tags, tag_order=tag_order, train_bag = train_tagged_words):\n",
        "    pi = np.zeros(len(tags))\n",
        "    for w,t in train_bag:\n",
        "        pi[tag_order[t]] += 1\n",
        "    \n",
        "    return pi/pi.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "0ba30571",
      "metadata": {
        "id": "0ba30571"
      },
      "outputs": [],
      "source": [
        "# compute Emission Probability\n",
        "B = emission_prob(vocab, tags, word_order, train_tagged_words)\n",
        "def word_given_tag(word, tag, word_order=word_order):\n",
        "    return B[tag][word_order[word]] #or 1e-4\n",
        "    \n",
        "\n",
        "\n",
        "# compute Transition Probability\n",
        "A = state_prob(tags, word_order, tag_order, train_tagged_words)\n",
        "def t2_given_t1(t2, t1):\n",
        "    return A[t1,t2]\n",
        "\n",
        "pi = state_init_prob(tags, tag_order, train_tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "ae1f51e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "ae1f51e2",
        "outputId": "0555e8d9-876e-45b1-fa40-6389b167fe11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-06e3aa7d-2ba0-4505-99bd-df4e36d981d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>.</th>\n",
              "      <th>ADJ</th>\n",
              "      <th>ADP</th>\n",
              "      <th>ADV</th>\n",
              "      <th>CONJ</th>\n",
              "      <th>DET</th>\n",
              "      <th>NOUN</th>\n",
              "      <th>NUM</th>\n",
              "      <th>PRON</th>\n",
              "      <th>PRT</th>\n",
              "      <th>VERB</th>\n",
              "      <th>X</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.092382</td>\n",
              "      <td>0.046137</td>\n",
              "      <td>0.092918</td>\n",
              "      <td>0.052575</td>\n",
              "      <td>0.060086</td>\n",
              "      <td>0.172210</td>\n",
              "      <td>0.218562</td>\n",
              "      <td>0.078219</td>\n",
              "      <td>0.068777</td>\n",
              "      <td>0.002790</td>\n",
              "      <td>0.089700</td>\n",
              "      <td>0.025644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADJ</th>\n",
              "      <td>0.066019</td>\n",
              "      <td>0.063301</td>\n",
              "      <td>0.080583</td>\n",
              "      <td>0.005243</td>\n",
              "      <td>0.016893</td>\n",
              "      <td>0.005243</td>\n",
              "      <td>0.696893</td>\n",
              "      <td>0.021748</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.011456</td>\n",
              "      <td>0.011456</td>\n",
              "      <td>0.020971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADP</th>\n",
              "      <td>0.038724</td>\n",
              "      <td>0.107062</td>\n",
              "      <td>0.016958</td>\n",
              "      <td>0.014553</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.320931</td>\n",
              "      <td>0.323589</td>\n",
              "      <td>0.063275</td>\n",
              "      <td>0.069603</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.008479</td>\n",
              "      <td>0.034548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADV</th>\n",
              "      <td>0.139255</td>\n",
              "      <td>0.130721</td>\n",
              "      <td>0.119472</td>\n",
              "      <td>0.081458</td>\n",
              "      <td>0.006982</td>\n",
              "      <td>0.071373</td>\n",
              "      <td>0.032196</td>\n",
              "      <td>0.029868</td>\n",
              "      <td>0.012025</td>\n",
              "      <td>0.014740</td>\n",
              "      <td>0.339022</td>\n",
              "      <td>0.022886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CONJ</th>\n",
              "      <td>0.035126</td>\n",
              "      <td>0.113611</td>\n",
              "      <td>0.055982</td>\n",
              "      <td>0.057080</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.123491</td>\n",
              "      <td>0.349067</td>\n",
              "      <td>0.040615</td>\n",
              "      <td>0.060373</td>\n",
              "      <td>0.004391</td>\n",
              "      <td>0.150384</td>\n",
              "      <td>0.009330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DET</th>\n",
              "      <td>0.017393</td>\n",
              "      <td>0.206411</td>\n",
              "      <td>0.009918</td>\n",
              "      <td>0.012074</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>0.635906</td>\n",
              "      <td>0.022855</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.040247</td>\n",
              "      <td>0.045134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NOUN</th>\n",
              "      <td>0.240094</td>\n",
              "      <td>0.012584</td>\n",
              "      <td>0.176827</td>\n",
              "      <td>0.016895</td>\n",
              "      <td>0.042454</td>\n",
              "      <td>0.013106</td>\n",
              "      <td>0.262344</td>\n",
              "      <td>0.009144</td>\n",
              "      <td>0.004659</td>\n",
              "      <td>0.043935</td>\n",
              "      <td>0.149134</td>\n",
              "      <td>0.028825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NUM</th>\n",
              "      <td>0.119243</td>\n",
              "      <td>0.035345</td>\n",
              "      <td>0.037487</td>\n",
              "      <td>0.003570</td>\n",
              "      <td>0.014281</td>\n",
              "      <td>0.003570</td>\n",
              "      <td>0.351660</td>\n",
              "      <td>0.184220</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.026062</td>\n",
              "      <td>0.020707</td>\n",
              "      <td>0.202428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRON</th>\n",
              "      <td>0.041913</td>\n",
              "      <td>0.070615</td>\n",
              "      <td>0.022323</td>\n",
              "      <td>0.036902</td>\n",
              "      <td>0.005011</td>\n",
              "      <td>0.009567</td>\n",
              "      <td>0.212756</td>\n",
              "      <td>0.006834</td>\n",
              "      <td>0.006834</td>\n",
              "      <td>0.014123</td>\n",
              "      <td>0.484738</td>\n",
              "      <td>0.088383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRT</th>\n",
              "      <td>0.045010</td>\n",
              "      <td>0.082975</td>\n",
              "      <td>0.019569</td>\n",
              "      <td>0.009393</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.101370</td>\n",
              "      <td>0.250489</td>\n",
              "      <td>0.056751</td>\n",
              "      <td>0.017613</td>\n",
              "      <td>0.001174</td>\n",
              "      <td>0.401174</td>\n",
              "      <td>0.012133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VERB</th>\n",
              "      <td>0.034807</td>\n",
              "      <td>0.066390</td>\n",
              "      <td>0.092357</td>\n",
              "      <td>0.083886</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>0.133610</td>\n",
              "      <td>0.110589</td>\n",
              "      <td>0.022836</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.030663</td>\n",
              "      <td>0.167956</td>\n",
              "      <td>0.215930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X</th>\n",
              "      <td>0.160869</td>\n",
              "      <td>0.017682</td>\n",
              "      <td>0.142226</td>\n",
              "      <td>0.025754</td>\n",
              "      <td>0.010379</td>\n",
              "      <td>0.056890</td>\n",
              "      <td>0.061695</td>\n",
              "      <td>0.003075</td>\n",
              "      <td>0.054200</td>\n",
              "      <td>0.185086</td>\n",
              "      <td>0.206419</td>\n",
              "      <td>0.075726</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06e3aa7d-2ba0-4505-99bd-df4e36d981d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06e3aa7d-2ba0-4505-99bd-df4e36d981d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06e3aa7d-2ba0-4505-99bd-df4e36d981d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             .       ADJ       ADP       ADV      CONJ       DET      NOUN  \\\n",
              ".     0.092382  0.046137  0.092918  0.052575  0.060086  0.172210  0.218562   \n",
              "ADJ   0.066019  0.063301  0.080583  0.005243  0.016893  0.005243  0.696893   \n",
              "ADP   0.038724  0.107062  0.016958  0.014553  0.001012  0.320931  0.323589   \n",
              "ADV   0.139255  0.130721  0.119472  0.081458  0.006982  0.071373  0.032196   \n",
              "CONJ  0.035126  0.113611  0.055982  0.057080  0.000549  0.123491  0.349067   \n",
              "DET   0.017393  0.206411  0.009918  0.012074  0.000431  0.006037  0.635906   \n",
              "NOUN  0.240094  0.012584  0.176827  0.016895  0.042454  0.013106  0.262344   \n",
              "NUM   0.119243  0.035345  0.037487  0.003570  0.014281  0.003570  0.351660   \n",
              "PRON  0.041913  0.070615  0.022323  0.036902  0.005011  0.009567  0.212756   \n",
              "PRT   0.045010  0.082975  0.019569  0.009393  0.002348  0.101370  0.250489   \n",
              "VERB  0.034807  0.066390  0.092357  0.083886  0.005433  0.133610  0.110589   \n",
              "X     0.160869  0.017682  0.142226  0.025754  0.010379  0.056890  0.061695   \n",
              "\n",
              "           NUM      PRON       PRT      VERB         X  \n",
              ".     0.078219  0.068777  0.002790  0.089700  0.025644  \n",
              "ADJ   0.021748  0.000194  0.011456  0.011456  0.020971  \n",
              "ADP   0.063275  0.069603  0.001266  0.008479  0.034548  \n",
              "ADV   0.029868  0.012025  0.014740  0.339022  0.022886  \n",
              "CONJ  0.040615  0.060373  0.004391  0.150384  0.009330  \n",
              "DET   0.022855  0.003306  0.000287  0.040247  0.045134  \n",
              "NOUN  0.009144  0.004659  0.043935  0.149134  0.028825  \n",
              "NUM   0.184220  0.001428  0.026062  0.020707  0.202428  \n",
              "PRON  0.006834  0.006834  0.014123  0.484738  0.088383  \n",
              "PRT   0.056751  0.017613  0.001174  0.401174  0.012133  \n",
              "VERB  0.022836  0.035543  0.030663  0.167956  0.215930  \n",
              "X     0.003075  0.054200  0.185086  0.206419  0.075726  "
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# you can also convert the matrix to a pandas dataframe for better readability\n",
        "pd.DataFrame(A, index=tag_order.keys(),columns=tag_order.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "a321eeee",
      "metadata": {
        "id": "a321eeee"
      },
      "outputs": [],
      "source": [
        "def Viterbi(words, train_bag = train_tagged_words):\n",
        "    bk_track = np.zeros((len(words), len(tags)))\n",
        "    \n",
        "    # Probabilities at time T=1\n",
        "    curr_layer = np.array([\n",
        "        pi[idx]*word_given_tag(words[0], tag) for idx, tag in enumerate(tags_list)\n",
        "    ])\n",
        "    # curr_layer = (curr_layer)/(curr_layer).sum()\n",
        "    next_layer = np.zeros(len(tags))\n",
        "\n",
        "    # Loop over all the words\n",
        "    for idx, word in enumerate(words[1:], start=1):\n",
        "        # Looping over all states in next time step\n",
        "        for j in range(len(next_layer)):\n",
        "            temp = np.zeros(len(tags))\n",
        "            wgt = word_given_tag(word, tags_list[j]) if word in word_order else 1\n",
        "            # Looping over all current layer(time) states\n",
        "            # to determine values of a state in next layer\n",
        "            for k in range(len(curr_layer)):\n",
        "                # Probability of the new state, coming from a state from prev layer\n",
        "                # and the word(observation) given new state.\n",
        "                temp[k] = curr_layer[k] * t2_given_t1(k,j) * wgt\n",
        "            next_layer[j] = temp.max()\n",
        "            # Adding the state index coming from prev layer in back track matrix\n",
        "            bk_track[idx, j] = temp.argmax()\n",
        "        next_layer = next_layer/next_layer.sum()\n",
        "        curr_layer = next_layer.copy()\n",
        "    \n",
        "    path = []\n",
        "    pointer = curr_layer.argmax()\n",
        "    path.append((words[-1], tags_list[pointer]))\n",
        "    # Backtracking\n",
        "    for row, word in zip(np.flipud(bk_track), words[-2::-1]):\n",
        "        path.append( (word, tags_list[int(row[pointer])]) )\n",
        "        pointer = int(row[pointer])\n",
        "\n",
        "    return path[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z9MOLwax-Hh-",
      "metadata": {
        "id": "z9MOLwax-Hh-"
      },
      "source": [
        "## Random 10 sentences Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "5DIFj_e9GKJ_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DIFj_e9GKJ_",
        "outputId": "f94f35ae-e54e-49e6-dcbe-49f31b1c98e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken in seconds: 0.06524825096130371\n",
            "Viterbi Algorithm Accuracy: 92.82296650717703\n"
          ]
        }
      ],
      "source": [
        "# test the Viterbi algorithm on a few sample sentences of test dataset\n",
        "random.seed(1234)      # define a random seed to get same sentences when run multiple times\n",
        "np.random.seed(1234)\n",
        "\n",
        "# choose random 10 numbers\n",
        "rndom = [random.randint(1, len(test_set)) for x in range(10)]\n",
        " \n",
        "# list of 10 sentencess on which to test the model\n",
        "test_run = [test_set[i] for i in rndom]\n",
        " \n",
        "# list of tagged words\n",
        "test_run_base = [tup for sent in test_run for tup in sent]\n",
        " \n",
        "# list of untagged words\n",
        "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
        "\n",
        "# testing 10 sentences to check the accuracy\n",
        "start = time.time()\n",
        "tagged_seq = Viterbi(test_tagged_words)\n",
        "end = time.time()\n",
        "difference = end - start\n",
        " \n",
        "print(\"Time taken in seconds:\", difference)\n",
        " \n",
        "# accuracy should be good enough (> 90%) to be a satisfactory model\n",
        "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
        " \n",
        "accuracy = len(check) / len(tagged_seq)\n",
        "print('Viterbi Algorithm Accuracy:', accuracy * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G6TJ3raD-Zqu",
      "metadata": {
        "id": "G6TJ3raD-Zqu"
      },
      "source": [
        "## Test Accuracy for the entire Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "Tm5iGNa35G2G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm5iGNa35G2G",
        "outputId": "433bbd2c-624e-4803-c527-b12b0385e15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken in seconds: 3.193380355834961\n",
            "Viterbi Algorithm Accuracy: 90\n"
          ]
        }
      ],
      "source": [
        "# test the Viterbi algorithm on a few sample sentences of test dataset\n",
        "random.seed(1234)      # define a random seed to get same sentences when run multiple times\n",
        "np.random.seed(1234)\n",
        "\n",
        "# list of tagged words\n",
        "test_run_base = [tup for sent in test_set for tup in sent]\n",
        " \n",
        "# list of untagged words\n",
        "test_tagged_words = [tup[0] for sent in test_set for tup in sent]\n",
        "\n",
        "# testing 10 sentences to check the accuracy\n",
        "start = time.time()\n",
        "tagged_seq = Viterbi(test_tagged_words)\n",
        "end = time.time()\n",
        "difference = end - start\n",
        " \n",
        "print(\"Time taken in seconds:\", difference)\n",
        " \n",
        "# accuracy should be good enough (> 90%) to be a satisfactory model\n",
        "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
        " \n",
        "accuracy = len(check) / len(tagged_seq)\n",
        "print('Viterbi Algorithm Accuracy:', round(accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N5cBiRTJKCIj",
      "metadata": {
        "id": "N5cBiRTJKCIj"
      },
      "source": [
        "## Hot Cold State Problem\n",
        "\n",
        "Two Hidden States: **Hot**ðŸ¥µ and **Cold**ðŸ¥¶ \n",
        "\n",
        "Three Observations: **1**, **2**, and **3**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zvL9tZL7KTCW",
      "metadata": {
        "id": "zvL9tZL7KTCW"
      },
      "source": [
        "### Probability of Sequence 3-1-3 Occuring?\n",
        "\n",
        "Ans: **2.86%**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l3lj50iXKey7",
      "metadata": {
        "id": "l3lj50iXKey7"
      },
      "source": [
        "### Most Probable States Sequence given the Observation 3-1-3\n",
        "\n",
        "Ans: **Hot**-**Cold**-**Hold**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "8d519ce160da629244c5fa84cf02a853bdfbd2d6722f9390c675e535dc1f1a96"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
